{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83HaTR3j7tkp",
      "metadata": {
        "id": "83HaTR3j7tkp"
      },
      "source": [
        "# Laboratorio TECH: Chatbot RAG + S3 + Telegram\n",
        "By **Ing. Engler Gonzalez**\n",
        "\n",
        "### Novedades clave\n",
        "- **LLM seleccionable**: OpenAI *o* Gemini. Si no hay API key, cae en *modo fragmento mÃ¡s relevante*.\n",
        "- **Persistencia real** en S3: Ã­ndice FAISS guardado/cargado por equipo + reconstrucciÃ³n desde `docs/`.\n",
        "- **Telegram completo**: los clientes escriben al bot, el admin ve todos los chats, activa/desactiva **Auto-responder** y puede **intervenir** en cualquier momento.\n",
        "- **Transcripciones** por `chat_id` en `mini_chatbot_work/logs/`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ghh-ERwS7tk2",
      "metadata": {
        "id": "Ghh-ERwS7tk2"
      },
      "source": [
        "## 0) InstalaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "-tL-JJTo7tk3",
      "metadata": {
        "id": "-tL-JJTo7tk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308d3a5e-4ad4-4195-9e96-5086bedda1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "!pip -q install sentence-transformers faiss-cpu pypdf gradio boto3 openai==1.* tiktoken requests google-generativeai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AFEkEyMR7tk7",
      "metadata": {
        "id": "AFEkEyMR7tk7"
      },
      "source": [
        "## 1) ConfiguraciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "G4nOrSx47tk8",
      "metadata": {
        "id": "G4nOrSx47tk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5faf118-ad5a-43ad-a45a-c75b13397876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AWS_ACCESS_KEY_ID: AKIAQNBYZOKR56S2G57T\n",
            "AWS_SECRET_ACCESS_KEY (oculto): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ” AWS: us-east-2 talentotech2025 IA-Innovador/\n",
            "âœ… API Key(s) cargada(s) correctamente.\n",
            "âœ” LLM provider: openai | OpenAI model: gpt-4o-mini | Gemini model: gemini-1.5-flash\n"
          ]
        }
      ],
      "source": [
        "# A) Entrada interactiva\n",
        "import os\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- AWS / S3 ---\n",
        "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-2\")\n",
        "S3_BUCKET  = os.getenv(\"S3_BUCKET\", \"talentotech2025\")\n",
        "S3_PREFIX  = os.getenv(\"S3_PREFIX\", \"IA-Innovador/\")  # prefijo base del curso\n",
        "\n",
        "# --- LLM ---\n",
        "LLM_PROVIDER = (os.getenv(\"LLM_PROVIDER\") or \"openai\").lower()   # \"openai\" o \"gemini\"\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or userdata.get(\"OPENAI_API_KEY\")\n",
        "OPENAI_MODEL   = os.getenv(\"OPENAI_MODEL\")   or \"gpt-4o-mini\"\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or userdata.get(\"GOOGLE_API_KEY\")\n",
        "GEMINI_MODEL   = os.getenv(\"GEMINI_MODEL\")   or \"gemini-1.5-flash\"\n",
        "\n",
        "\n",
        "# Pide claves si faltan (opcional)\n",
        "AWS_ACCESS_KEY_ID     = os.getenv(\"AWS_ACCESS_KEY_ID\")     or input(\"AWS_ACCESS_KEY_ID: \").strip()\n",
        "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\") or getpass(\"AWS_SECRET_ACCESS_KEY (oculto): \").strip()\n",
        "AWS_SESSION_TOKEN     = os.getenv(\"AWS_SESSION_TOKEN\")     # opcional (solo STS/Academy)\n",
        "\n",
        "# Persistir en entorno\n",
        "env = {\n",
        "\"AWS_REGION\":AWS_REGION, \"S3_BUCKET\":S3_BUCKET, \"S3_PREFIX\":S3_PREFIX,\n",
        "\"AWS_ACCESS_KEY_ID\":AWS_ACCESS_KEY_ID, \"AWS_SECRET_ACCESS_KEY\":AWS_SECRET_ACCESS_KEY\n",
        "}\n",
        "if AWS_SESSION_TOKEN: env[\"AWS_SESSION_TOKEN\"]=AWS_SESSION_TOKEN\n",
        "for k,v in env.items(): os.environ[k]=v\n",
        "\n",
        "# LLM env (deja vacÃ­o si no tienes)\n",
        "os.environ[\"LLM_PROVIDER\"]=LLM_PROVIDER\n",
        "if OPENAI_API_KEY: os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY\n",
        "os.environ[\"OPENAI_MODEL\"]=OPENAI_MODEL\n",
        "if GOOGLE_API_KEY: os.environ[\"GOOGLE_API_KEY\"]=GOOGLE_API_KEY\n",
        "os.environ[\"GEMINI_MODEL\"]=GEMINI_MODEL\n",
        "\n",
        "print(\"âœ” AWS:\", AWS_REGION, S3_BUCKET, S3_PREFIX)\n",
        "\n",
        "if OPENAI_API_KEY or GOOGLE_API_KEY:\n",
        "    print(\"âœ… API Key(s) cargada(s) correctamente.\")\n",
        "    print(\"âœ” LLM provider:\", LLM_PROVIDER, \"| OpenAI model:\", OPENAI_MODEL, \"| Gemini model:\", GEMINI_MODEL)\n",
        "else:\n",
        "    print(\"âš ï¸ No se detectaron API Keys. El chat funcionarÃ¡ en modo 'fragmento mÃ¡s relevante'.\")\n",
        "    print(\"âœ” LLM provider: Fragmento mÃ¡s relevante\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BeH2Jtmx7tlA",
      "metadata": {
        "id": "BeH2Jtmx7tlA"
      },
      "source": [
        "## 2) S3 helpers (autoregiÃ³n, listar carpetas/archivos, sync, prefijo efectivo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1xLVWVFv7tlC",
      "metadata": {
        "id": "1xLVWVFv7tlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf435400-8f5c-45a9-8da6-5d754eec5d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ” Helpers S3 OK\n"
          ]
        }
      ],
      "source": [
        "import os, boto3, datetime, re, json, time, threading, requests, unicodedata\n",
        "from pathlib import Path\n",
        "from botocore.exceptions import ClientError\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "\n",
        "def norm_prefix(p: str) -> str:\n",
        "    if p is None:\n",
        "        return \"\"\n",
        "    p = p.strip().replace(\"\\\\\", \"/\")\n",
        "    p = p.lstrip(\"/\")\n",
        "    if p and not p.endswith(\"/\"):\n",
        "        p += \"/\"\n",
        "    return p\n",
        "\n",
        "\n",
        "def get_bucket_region(bucket: str) -> str:\n",
        "    s3g = boto3.client(\"s3\")\n",
        "    loc = s3g.get_bucket_location(Bucket=bucket).get(\"LocationConstraint\")\n",
        "    return \"us-east-1\" if loc in (None, \"EU\") else loc\n",
        "\n",
        "\n",
        "def s3_client_autoregion(bucket: str):\n",
        "    try:\n",
        "        region = get_bucket_region(bucket)\n",
        "    except Exception:\n",
        "        region = os.getenv(\"AWS_REGION\", \"us-east-2\")\n",
        "    return boto3.client(\"s3\", region_name=region)\n",
        "\n",
        "\n",
        "def s3_list_objects(bucket: str, prefix: str, delimiter: str = None):\n",
        "    s3 = s3_client_autoregion(bucket)\n",
        "    kwargs = {\"Bucket\": bucket, \"Prefix\": norm_prefix(prefix)}\n",
        "    if delimiter:\n",
        "        kwargs[\"Delimiter\"] = delimiter\n",
        "    keys, folders = [], []\n",
        "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
        "    for page in paginator.paginate(**kwargs):\n",
        "        for obj in page.get(\"Contents\", []) or []:\n",
        "            keys.append(obj[\"Key\"])\n",
        "        for cp in page.get(\"CommonPrefixes\", []) or []:\n",
        "            folders.append(cp[\"Prefix\"])\n",
        "    return keys, folders\n",
        "\n",
        "\n",
        "def s3_list_immediate_folders(bucket: str, base_prefix: str):\n",
        "    _, folders = s3_list_objects(bucket, norm_prefix(base_prefix), delimiter=\"/\")\n",
        "    return sorted({f.split(\"/\")[-2] for f in folders}) if folders else []\n",
        "\n",
        "\n",
        "def s3_sync_docs_to_local(bucket: str, prefix_docs: str, local_folder: str):\n",
        "    s3 = s3_client_autoregion(bucket)\n",
        "    prefix_docs = norm_prefix(prefix_docs)\n",
        "    Path(local_folder).mkdir(parents=True, exist_ok=True)\n",
        "    count = 0\n",
        "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
        "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix_docs):\n",
        "        for obj in page.get(\"Contents\", []) or []:\n",
        "            key = obj[\"Key\"]\n",
        "            if key.endswith(\"/\"):\n",
        "                continue\n",
        "            rel = key[len(prefix_docs):]\n",
        "            out = Path(local_folder) / rel\n",
        "            out.parent.mkdir(parents=True, exist_ok=True)\n",
        "            s3.download_file(bucket, key, str(out))\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "\n",
        "def s3_sync_local_docs_to_s3(bucket: str, prefix_docs: str, local_folder: str):\n",
        "    s3 = s3_client_autoregion(bucket)\n",
        "    prefix_docs = norm_prefix(prefix_docs)\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(local_folder):\n",
        "        for name in files:\n",
        "            full = Path(root) / name\n",
        "            rel = Path(full).relative_to(local_folder).as_posix()\n",
        "            key = prefix_docs + rel\n",
        "            s3.upload_file(str(full), bucket, key)\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "\n",
        "print(\"âœ” Helpers S3 OK\")\n",
        "\n",
        "\n",
        "SYS_PROMPT = (\n",
        "    \"Eres un Generador de Conocimiento Conversacional (CEREBRO) para un chatbot RAG orientado a clientes de Talento TECH. \"\n",
        "    \"Hablas en espaÃ±ol neutro con tono cercano, empÃ¡tico y proactivo. \"\n",
        "    \"Usa Ãºnicamente la informaciÃ³n del contexto documental proporcionado. \"\n",
        "    \"Si la respuesta no estÃ¡ en el contexto, explica quÃ© falta y solicita el dato necesario con amabilidad. \"\n",
        "    \"Resume en pasos claros cuando aplique y evita sonar como administrador interno.\"\n",
        ")\n",
        "\n",
        "\n",
        "def format_context(hits):\n",
        "    lines = []\n",
        "    for score, ch in hits:\n",
        "        snippet = (ch.text[:350] + \"â€¦\") if len(ch.text) > 350 else ch.text\n",
        "        lines.append(f\"[{ch.source_name} | score={score:.3f}] {snippet}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "\n",
        "\n",
        "def normalize_for_matching(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFKD\", text or \"\")\n",
        "    return \"\".join(ch for ch in text if not unicodedata.combining(ch)).lower()\n",
        "\n",
        "\n",
        "def extract_relevant_sentences(question: str, hits, limit: int = 3):\n",
        "    if not hits:\n",
        "        return []\n",
        "    question_norm = normalize_for_matching(question)\n",
        "    keywords = [tok for tok in re.findall(r\"\\w+\", question_norm) if len(tok) > 2]\n",
        "    sentences = []\n",
        "    seen = set()\n",
        "    for _score, chunk in hits:\n",
        "        text = getattr(chunk, \"text\", \"\") or \"\"\n",
        "        parts = re.split(r\"(?<=[.!?])\\s+|\\n+\", text)\n",
        "        for part in parts:\n",
        "            cleaned = part.strip()\n",
        "            if not cleaned:\n",
        "                continue\n",
        "            normalized = normalize_for_matching(cleaned)\n",
        "            if keywords and not any(tok in normalized for tok in keywords):\n",
        "                continue\n",
        "            if normalized in seen:\n",
        "                continue\n",
        "            seen.add(normalized)\n",
        "            sentences.append(cleaned)\n",
        "            if len(sentences) >= limit:\n",
        "                return sentences\n",
        "    if not sentences:\n",
        "        fallback = (getattr(hits[0][1], \"text\", \"\") or \"\").strip()\n",
        "        if fallback:\n",
        "            sentences.append(fallback[:400])\n",
        "    return sentences[:limit]\n",
        "\n",
        "def available_sources() -> List[str]:\n",
        "    names: List[str] = []\n",
        "    meta_path = globals().get(\"META_PATH\")\n",
        "    if meta_path:\n",
        "        try:\n",
        "            with open(meta_path, \"r\", encoding=\"utf-8\") as handle:\n",
        "                data = json.load(handle) or []\n",
        "            for entry in data:\n",
        "                if not isinstance(entry, dict):\n",
        "                    continue\n",
        "                name = entry.get(\"source_name\")\n",
        "                if name:\n",
        "                    names.append(str(name))\n",
        "        except Exception:\n",
        "            pass\n",
        "    chunks = globals().get(\"_chunks\")\n",
        "    if chunks:\n",
        "        for chunk in chunks:\n",
        "            name = getattr(chunk, \"source_name\", \"\")\n",
        "            if name:\n",
        "                names.append(str(name))\n",
        "    seen = set()\n",
        "    ordered: List[str] = []\n",
        "    for name in names:\n",
        "        if name not in seen:\n",
        "            seen.add(name)\n",
        "            ordered.append(name)\n",
        "    return ordered\n",
        "\n",
        "\n",
        "def build_fallback_message(question: str, hits=None) -> str:\n",
        "    hits = list(hits or [])\n",
        "    helper = globals().get(\"extract_relevant_sentences\")\n",
        "    snippets: List[str] = []\n",
        "    if callable(helper):\n",
        "        try:\n",
        "            snippets = helper(question, hits, limit=3)\n",
        "        except Exception:\n",
        "            snippets = []\n",
        "    if snippets:\n",
        "        bullets = \"\\n\".join(f\"â€¢ {s}\" for s in snippets)\n",
        "        return (\n",
        "            \"Esto es lo mÃ¡s cercano que encontrÃ© en tus archivos:\\n\"\n",
        "            + bullets\n",
        "            + \"\\n\\nSi necesitas que profundice en algo, dime quÃ© detalle buscas.\"\n",
        "        )\n",
        "    sources = available_sources()\n",
        "    if sources:\n",
        "        return (\n",
        "            \"AÃºn no veo datos directos sobre eso. Revisa o comparte informaciÃ³n en estos archivos disponibles: \"\n",
        "            + \", \".join(sources)\n",
        "            + \". Si tienes mÃ¡s contexto, cuÃ©ntamelo y sigo buscando.\"\n",
        "        )\n",
        "    return (\n",
        "        \"AÃºn no tengo informaciÃ³n sobre ese tema en los archivos cargados. \"\n",
        "        \"Comparte mÃ¡s detalles o sube un documento relacionado y con gusto investigo.\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "class LLMClient:\n",
        "    DEFAULT_PROVIDER = \"openai\"\n",
        "    FALLBACK_PROVIDER = \"fragmento\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._oai = None\n",
        "        self._gem = None\n",
        "        self.provider = self.DEFAULT_PROVIDER\n",
        "        self.temperature = 0.2\n",
        "        self.max_tokens = 400\n",
        "        self.openai_model = \"gpt-4o-mini\"\n",
        "        self.gemini_model = \"gemini-1.5-flash\"\n",
        "        self.effective_provider = self.DEFAULT_PROVIDER\n",
        "        self._cached_provider = None\n",
        "        self._cached_openai_model = None\n",
        "        self._cached_gemini_model = None\n",
        "        self.configure(persist=False)\n",
        "\n",
        "    def configure(self, provider: Optional[str] = None, temperature: Optional[float] = None,\n",
        "                  max_tokens: Optional[int] = None, openai_model: Optional[str] = None,\n",
        "                  gemini_model: Optional[str] = None, persist: bool = True) -> Dict[str, Any]:\n",
        "        if provider is None:\n",
        "            provider = os.getenv(\"LLM_PROVIDER\", self.provider)\n",
        "        provider = (provider or \"\").strip().lower()\n",
        "        if provider not in {\"openai\", \"gemini\"}:\n",
        "            provider = self.FALLBACK_PROVIDER\n",
        "        self.provider = provider\n",
        "\n",
        "        if temperature is None:\n",
        "            temperature = os.getenv(\"LLM_TEMPERATURE\", self.temperature)\n",
        "        self.temperature = float(temperature)\n",
        "\n",
        "        if max_tokens is None:\n",
        "            max_tokens = os.getenv(\"LLM_MAX_TOKENS\", self.max_tokens)\n",
        "        self.max_tokens = int(max_tokens)\n",
        "\n",
        "        if openai_model is None:\n",
        "            openai_model = os.getenv(\"OPENAI_MODEL\", self.openai_model)\n",
        "        self.openai_model = (openai_model or \"gpt-4o-mini\").strip()\n",
        "\n",
        "        if gemini_model is None:\n",
        "            gemini_model = os.getenv(\"GEMINI_MODEL\", self.gemini_model)\n",
        "        self.gemini_model = (gemini_model or \"gemini-1.5-flash\").strip()\n",
        "\n",
        "        if persist:\n",
        "            os.environ[\"LLM_PROVIDER\"] = self.provider\n",
        "            os.environ[\"LLM_TEMPERATURE\"] = str(self.temperature)\n",
        "            os.environ[\"LLM_MAX_TOKENS\"] = str(self.max_tokens)\n",
        "            os.environ[\"OPENAI_MODEL\"] = self.openai_model\n",
        "            os.environ[\"GEMINI_MODEL\"] = self.gemini_model\n",
        "\n",
        "        if self.provider != self._cached_provider:\n",
        "            self._oai = None\n",
        "            self._gem = None\n",
        "        if self.openai_model != self._cached_openai_model:\n",
        "            self._oai = None\n",
        "        if self.gemini_model != self._cached_gemini_model:\n",
        "            self._gem = None\n",
        "\n",
        "        self._cached_provider = self.provider\n",
        "        self._cached_openai_model = self.openai_model\n",
        "        self._cached_gemini_model = self.gemini_model\n",
        "\n",
        "        return self.status()\n",
        "\n",
        "    def status(self) -> Dict[str, Any]:\n",
        "        has_openai = bool(os.getenv(\"OPENAI_API_KEY\"))\n",
        "        has_gemini = bool(os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\"))\n",
        "        available: List[str] = []\n",
        "        if has_openai:\n",
        "            available.append(\"openai\")\n",
        "        if has_gemini:\n",
        "            available.append(\"gemini\")\n",
        "        if not available:\n",
        "            available = [self.FALLBACK_PROVIDER]\n",
        "        effective = self.provider\n",
        "        if effective not in {\"openai\", \"gemini\"}:\n",
        "            effective = self.FALLBACK_PROVIDER\n",
        "        elif effective not in available:\n",
        "            effective = self.FALLBACK_PROVIDER\n",
        "        self.effective_provider = effective\n",
        "        return {\n",
        "            \"provider\": effective,\n",
        "            \"configured_provider\": self.provider,\n",
        "            \"available\": available,\n",
        "            \"temperature\": self.temperature,\n",
        "            \"max_tokens\": self.max_tokens,\n",
        "            \"openai_model\": self.openai_model,\n",
        "            \"gemini_model\": self.gemini_model,\n",
        "            \"has_openai_key\": has_openai,\n",
        "            \"has_gemini_key\": has_gemini,\n",
        "        }\n",
        "\n",
        "    def _ensure_openai(self):\n",
        "        key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if not key:\n",
        "            return None\n",
        "        if self._oai is None:\n",
        "            from openai import OpenAI\n",
        "            self._oai = OpenAI(api_key=key)\n",
        "        return self._oai\n",
        "\n",
        "    def _ensure_gemini(self):\n",
        "        key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not key:\n",
        "            return None\n",
        "        if self._gem is None:\n",
        "            import google.generativeai as genai\n",
        "            genai.configure(api_key=key)\n",
        "            self._gem = genai.GenerativeModel(self.gemini_model)\n",
        "        return self._gem\n",
        "\n",
        "    def _fallback_answer(self, question: str, hits) -> str:\n",
        "        return build_fallback_message(question, hits)\n",
        "\n",
        "    def generate(self, question: str, hits: Optional[List] = None,\n",
        "                 temperature: Optional[float] = None, max_tokens: Optional[int] = None,\n",
        "                 chat_history: Optional[List[Dict[str, str]]] = None) -> str:\n",
        "        hits = hits or []\n",
        "        context = format_context(hits) if hits else \"(sin fragmentos relevantes)\"\n",
        "        temp = float(self.temperature if temperature is None else temperature)\n",
        "        mtok = int(self.max_tokens if max_tokens is None else max_tokens)\n",
        "        provider = self.status()[\"provider\"]\n",
        "\n",
        "        history_messages = []\n",
        "        for turn in (chat_history or [])[-8:]:\n",
        "            role = \"assistant\" if turn.get(\"role\") == \"assistant\" else \"user\"\n",
        "            content = (turn.get(\"content\") or \"\").strip()\n",
        "            if content:\n",
        "                history_messages.append((role, content))\n",
        "\n",
        "        if provider == \"openai\":\n",
        "            cli = self._ensure_openai()\n",
        "            if cli:\n",
        "                try:\n",
        "                    msgs = [{\"role\": \"system\", \"content\": SYS_PROMPT}]\n",
        "                    for role, content in history_messages:\n",
        "                        msgs.append({\"role\": role, \"content\": content})\n",
        "                    user_sections = [f\"Pregunta actual del usuario: {question}\"]\n",
        "                    user_sections.append(f\"Contexto documental relevante:\\n{context}\")\n",
        "                    user_sections.append(\"Responde de forma conversacional y orientada al cliente usando solo ese contexto.\")\n",
        "                    msgs.append({\"role\": \"user\", \"content\": \"\\n\\n\".join(user_sections)})\n",
        "                    resp = cli.chat.completions.create(\n",
        "                        model=self.openai_model,\n",
        "                        messages=msgs,\n",
        "                        temperature=temp,\n",
        "                        max_tokens=mtok,\n",
        "                    )\n",
        "                    return (resp.choices[0].message.content or \"\").strip()\n",
        "                except Exception as exc:\n",
        "                    return f\"âš ï¸ Error OpenAI: {exc}\"\n",
        "        elif provider == \"gemini\":\n",
        "            model = self._ensure_gemini()\n",
        "            if model:\n",
        "                try:\n",
        "                    history_lines = \"\\n\".join(\n",
        "                        (\"Usuario: \" + content) if role == \"user\" else (\"Asistente: \" + content)\n",
        "                        for role, content in history_messages\n",
        "                    )\n",
        "                    prompt_parts = [SYS_PROMPT]\n",
        "                    if history_lines:\n",
        "                        prompt_parts.append(\"ConversaciÃ³n previa:\\n\" + history_lines)\n",
        "                    prompt_parts.append(f\"Pregunta actual del usuario: {question}\")\n",
        "                    prompt_parts.append(f\"Contexto documental relevante:\\n{context}\")\n",
        "                    prompt_parts.append(\"Responde de forma conversacional y orientada al cliente usando solo ese contexto.\")\n",
        "                    prompt = \"\\n\\n\".join(prompt_parts)\n",
        "                    out = model.generate_content(prompt)\n",
        "                    return (getattr(out, \"text\", \"\") or \"\").strip()\n",
        "                except Exception as exc:\n",
        "                    return f\"âš ï¸ Error Gemini: {exc}\"\n",
        "        return self._fallback_answer(question, hits)\n",
        "\n",
        "\n",
        "LLM = LLMClient()\n",
        "\n",
        "def collect_sources(hits):\n",
        "    return sorted({getattr(h[1], \"source_name\", \"\") for h in hits if getattr(h[1], \"source_name\", \"\")})\n",
        "\n",
        "\n",
        "def rag_answer(question: str, top_k: int = 4, temperature: Optional[float] = None,\n",
        "               max_tokens: Optional[int] = None, with_sources: bool = True, hits: Optional[List] = None,\n",
        "               chat_history: Optional[List[Dict[str, str]]] = None):\n",
        "    if hits is not None:\n",
        "        use_hits = list(hits)\n",
        "        if top_k is not None and int(top_k) > 0:\n",
        "            use_hits = use_hits[:int(top_k)]\n",
        "    else:\n",
        "        loader = globals().get(\"ensure_index_loaded\")\n",
        "        if callable(loader):\n",
        "            try:\n",
        "                loader()\n",
        "            except Exception:\n",
        "                pass\n",
        "        retrieval_question = question\n",
        "        if chat_history:\n",
        "            recent_user_turns = [t.get(\"content\") for t in chat_history if t.get(\"role\") == \"user\"]\n",
        "            if recent_user_turns:\n",
        "                retrieval_question = \" \".join(recent_user_turns[-2:] + [question])\n",
        "        use_hits = retrieve(retrieval_question, top_k=top_k) if \"retrieve\" in globals() else []\n",
        "    if not use_hits:\n",
        "        return build_fallback_message(question, []), []\n",
        "    answer = LLM.generate(\n",
        "        question,\n",
        "        hits=use_hits,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        chat_history=chat_history,\n",
        "    )\n",
        "    if not (answer or \"\").strip():\n",
        "        answer = build_fallback_message(question, use_hits)\n",
        "    sources = collect_sources(use_hits)\n",
        "    if with_sources and sources and \"Fuentes\" not in answer:\n",
        "        answer = answer.rstrip() + \"\\n\\nFuentes: \" + \", \".join(sources)\n",
        "    return answer, sources"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JFPXZY_T7tlE",
      "metadata": {
        "id": "JFPXZY_T7tlE"
      },
      "source": [
        "## 3) NÃºcleo RAG (loaders â†’ chunking â†’ FAISS â†’ retrieval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "tMhbAOdH7tlG",
      "metadata": {
        "id": "tMhbAOdH7tlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6cf7c9-fc8a-4c9f-ba76-3d148eab4370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ” NÃºcleo RAG OK\n"
          ]
        }
      ],
      "source": [
        "import os, json, uuid, shutil\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import pandas as pd\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np, faiss, time\n",
        "\n",
        "BASE_DIR = Path.cwd() / \"mini_chatbot_work\"\n",
        "DOCS_DIR = BASE_DIR / \"docs_raw\"\n",
        "INDEX_DIR = BASE_DIR / \"faiss_index\"\n",
        "LOGS_DIR = BASE_DIR / \"logs\"\n",
        "META_PATH = BASE_DIR / \"docs_metadata.json\"\n",
        "for path in [BASE_DIR, DOCS_DIR, INDEX_DIR, LOGS_DIR]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def load_txt(path: Path) -> str:\n",
        "    try:\n",
        "        return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    except Exception as exc:\n",
        "        return f\"[ERROR TXT] {exc}\"\n",
        "\n",
        "\n",
        "def load_pdf(path: Path) -> str:\n",
        "    try:\n",
        "        reader = PdfReader(str(path))\n",
        "        return \"\\n\".join((page.extract_text() or \"\") for page in reader.pages)\n",
        "    except Exception as exc:\n",
        "        return f\"[ERROR PDF] {exc}\"\n",
        "\n",
        "\n",
        "def load_csv(path: Path, n: int = 1500) -> str:\n",
        "    try:\n",
        "        df = pd.read_csv(path, nrows=n)\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(path, nrows=n, encoding=\"latin-1\")\n",
        "    preview = df.head(20).to_markdown(index=False)\n",
        "    return \"\\n\".join([\n",
        "        f\"# CSV: {path.name}\",\n",
        "        f\"Columnas: {list(df.columns)}\",\n",
        "        \"Muestra:\\n\" + preview,\n",
        "    ])\n",
        "\n",
        "\n",
        "def load_any(path: Path) -> str:\n",
        "    ext = path.suffix.lower()\n",
        "    if ext in {\".txt\", \".md\"}:\n",
        "        return load_txt(path)\n",
        "    if ext == \".pdf\":\n",
        "        return load_pdf(path)\n",
        "    if ext == \".csv\":\n",
        "        return load_csv(path)\n",
        "    return f\"[BINARIO] {path.name} (no indexado)\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ChunkedDoc:\n",
        "    doc_id: str\n",
        "    source_name: str\n",
        "    chunk_id: int\n",
        "    text: str\n",
        "\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 800, overlap: int = 150) -> List[str]:\n",
        "    tokens = text.split()\n",
        "    if not tokens:\n",
        "        return []\n",
        "    pieces: List[str] = []\n",
        "    step = max(1, chunk_size - overlap)\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        pieces.append(\" \".join(tokens[i : i + chunk_size]))\n",
        "        i += step\n",
        "    return pieces\n",
        "\n",
        "\n",
        "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "_emb = None\n",
        "_index = None\n",
        "_chunks: List[ChunkedDoc] = []\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    global _emb\n",
        "    if _emb is None:\n",
        "        _emb = SentenceTransformer(EMB_MODEL_NAME)\n",
        "    return _emb\n",
        "\n",
        "\n",
        "def build_index_from_local(paths: List[Path], chunk_size: int = 800, overlap: int = 150):\n",
        "    global _index, _chunks\n",
        "    metas = []\n",
        "    chunks: List[ChunkedDoc] = []\n",
        "    for path in paths:\n",
        "        path = Path(path)\n",
        "        if not path.is_file():\n",
        "            continue\n",
        "        raw = load_any(path)\n",
        "        doc_id = str(uuid.uuid4())\n",
        "        metas.append({\"doc_id\": doc_id, \"source_name\": path.name, \"path\": str(path)})\n",
        "        for idx, chunk in enumerate(chunk_text(raw, chunk_size, overlap)):\n",
        "            if not chunk.strip():\n",
        "                continue\n",
        "            chunks.append(ChunkedDoc(doc_id=doc_id, source_name=path.name, chunk_id=idx, text=chunk))\n",
        "    if not chunks:\n",
        "        _index = None\n",
        "        _chunks = []\n",
        "        return 0, len(metas)\n",
        "\n",
        "    embeddings = get_model().encode(\n",
        "        [chunk.text for chunk in chunks],\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "    ).astype(np.float32)\n",
        "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "    _index = index\n",
        "    _chunks = chunks\n",
        "\n",
        "    INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    faiss.write_index(_index, str(INDEX_DIR / \"faiss.index\"))\n",
        "    json.dump([chunk.__dict__ for chunk in chunks], open(INDEX_DIR / \"chunks.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False)\n",
        "    json.dump(metas, open(META_PATH, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
        "    return len(chunks), len(metas)\n",
        "\n",
        "\n",
        "def load_index_local() -> bool:\n",
        "    global _index, _chunks\n",
        "    fidx = INDEX_DIR / \"faiss.index\"\n",
        "    fch = INDEX_DIR / \"chunks.json\"\n",
        "    if not (fidx.exists() and fch.exists()):\n",
        "        return False\n",
        "    _index = faiss.read_index(str(fidx))\n",
        "    data = json.load(open(fch, \"r\", encoding=\"utf-8\"))\n",
        "    _chunks = [ChunkedDoc(**entry) for entry in data]\n",
        "    return True\n",
        "\n",
        "\n",
        "def ensure_index_loaded() -> bool:\n",
        "    global _index, _chunks\n",
        "    if _index is not None and _chunks:\n",
        "        return True\n",
        "    try:\n",
        "        return load_index_local()\n",
        "    except Exception:\n",
        "        _index = None\n",
        "        _chunks = []\n",
        "        return False\n",
        "\n",
        "\n",
        "def retrieve(question: str, top_k: int = 4) -> List[Tuple[float, ChunkedDoc]]:\n",
        "    if not ensure_index_loaded():\n",
        "        return []\n",
        "    query = get_model().encode([question], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
        "    distances, indices = _index.search(query, top_k)\n",
        "    results: List[Tuple[float, ChunkedDoc]] = []\n",
        "    for score, idx in zip(distances[0], indices[0]):\n",
        "        if idx < 0:\n",
        "            continue\n",
        "        results.append((float(score), _chunks[idx]))\n",
        "    return results\n",
        "\n",
        "\n",
        "def log_event(chat_id: str, role: str, text: str):\n",
        "    path = LOGS_DIR / f\"{chat_id}.jsonl\"\n",
        "    record = {\"t\": time.time(), \"role\": role, \"text\": text}\n",
        "    with open(path, \"a\", encoding=\"utf-8\") as handle:\n",
        "        handle.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "print(\"âœ” NÃºcleo RAG OK\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144S4eh77tlK",
      "metadata": {
        "id": "144S4eh77tlK"
      },
      "source": [
        "### 4) Cargar/Guardar Ã­ndice en S3 + reconstrucciÃ³n desde docs en S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "I-8KWQb87tlL",
      "metadata": {
        "id": "I-8KWQb87tlL"
      },
      "outputs": [],
      "source": [
        "def effective_team_prefix(base_prefix: str, team_folder: str):\n",
        "    return norm_prefix(base_prefix) + norm_prefix(team_folder)\n",
        "\n",
        "\n",
        "def s3_upload_index(bucket: str, base_prefix: str, team_folder: str):\n",
        "    idx_prefix_root = effective_team_prefix(base_prefix, team_folder)\n",
        "    idx_prefix_sub = idx_prefix_root + \"index/\"\n",
        "    if not (INDEX_DIR / \"faiss.index\").exists() or not (INDEX_DIR / \"chunks.json\").exists():\n",
        "        return \"âŒ No hay Ã­ndice local (faiss.index / chunks.json). Construye primero.\"\n",
        "    s3 = s3_client_autoregion(bucket)\n",
        "    for target_prefix in [idx_prefix_root, idx_prefix_sub]:\n",
        "        for name in [\"faiss.index\", \"chunks.json\"]:\n",
        "            s3.upload_file(str(INDEX_DIR / name), bucket, f\"{target_prefix}{name}\")\n",
        "        if META_PATH.exists():\n",
        "            s3.upload_file(str(META_PATH), bucket, f\"{target_prefix}docs_metadata.json\")\n",
        "    return f\"â˜ï¸ Subido a: s3://{bucket}/{idx_prefix_root}  y  s3://{bucket}/{idx_prefix_sub}\"\n",
        "\n",
        "\n",
        "def _download_index_from_prefix(bucket: str, prefix: str):\n",
        "    s3 = s3_client_autoregion(bucket)\n",
        "    found = set()\n",
        "    wanted = [\"faiss.index\", \"chunks.json\", \"docs_metadata.json\"]\n",
        "    keys, _ = s3_list_objects(bucket, prefix)\n",
        "    for key in keys:\n",
        "        base = key.split(\"/\")[-1]\n",
        "        if base in wanted:\n",
        "            out = INDEX_DIR / base if base != \"docs_metadata.json\" else META_PATH\n",
        "            out.parent.mkdir(parents=True, exist_ok=True)\n",
        "            s3.download_file(bucket, key, str(out))\n",
        "            found.add(base)\n",
        "    return found\n",
        "\n",
        "\n",
        "def s3_download_index(bucket: str, base_prefix: str, team_folder: str):\n",
        "    idx_prefix_root = effective_team_prefix(base_prefix, team_folder)\n",
        "    idx_prefix_sub = idx_prefix_root + \"index/\"\n",
        "    found = _download_index_from_prefix(bucket, idx_prefix_root)\n",
        "    if not {\"faiss.index\", \"chunks.json\"}.issubset(found):\n",
        "        found = _download_index_from_prefix(bucket, idx_prefix_sub)\n",
        "    if {\"faiss.index\", \"chunks.json\"}.issubset(found):\n",
        "        ok = load_index_local()\n",
        "        return \"ðŸ“¥ Ãndice cargado.\" if ok else \"âŒ Descargado pero fallÃ³ carga local.\"\n",
        "    return f\"âŒ No encontrÃ© Ã­ndice en {idx_prefix_root} ni {idx_prefix_sub}.\"\n",
        "\n",
        "\n",
        "def s3_rebuild_from_docs(bucket: str, base_prefix: str, team_folder: str, chunk_size=800, overlap=150):\n",
        "    docs_prefix = effective_team_prefix(base_prefix, team_folder) + \"docs/\"\n",
        "    count = s3_sync_docs_to_local(bucket, docs_prefix, str(DOCS_DIR))\n",
        "    if count == 0:\n",
        "        return \"âŒ No hay documentos en S3 (carpeta 'docs/'). Sube alguno primero.\"\n",
        "    n_chunks, n_docs = build_index_from_local(list(Path(DOCS_DIR).glob('*')), chunk_size, overlap)\n",
        "    return f\"âœ… Reconstruido desde S3: {n_docs} docs â†’ {n_chunks} chunks.\"\n",
        "\n",
        "\n",
        "def s3_upload_local_docs(bucket: str, base_prefix: str, team_folder: str):\n",
        "    docs_prefix = effective_team_prefix(base_prefix, team_folder) + \"docs/\"\n",
        "    count = s3_sync_local_docs_to_s3(bucket, docs_prefix, str(DOCS_DIR))\n",
        "    return f\"â˜ï¸ Subidos {count} archivo(s) a s3://{bucket}/{docs_prefix}\"\n",
        "\n",
        "\n",
        "def s3_download_docs(bucket: str, base_prefix: str, team_folder: str):\n",
        "    docs_prefix = effective_team_prefix(base_prefix, team_folder) + \"docs/\"\n",
        "    count = s3_sync_docs_to_local(bucket, docs_prefix, str(DOCS_DIR))\n",
        "    return f\"ðŸ“¥ Descargados {count} archivo(s) a {DOCS_DIR}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u9tg4sKB7tlO",
      "metadata": {
        "id": "u9tg4sKB7tlO"
      },
      "source": [
        "## 5) UI Gradio â€” S3/equipo, Docs, Ãndice, Persistencia, Preguntas, Telegram (Admin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Jn48XVDL7tlM",
      "metadata": {
        "id": "Jn48XVDL7tlM"
      },
      "outputs": [],
      "source": [
        "import os, re, json, time, threading, requests\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Optional, List\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _tt_answer_logic(question: str, for_telegram: bool = False,\n",
        "                       chat_history: Optional[List[Dict[str, str]]] = None) -> str:\n",
        "    question = (question or \"\").strip()\n",
        "    if not question:\n",
        "        return \"EnvÃ­ame un texto y responderÃ© con lo que haya en tus archivos.\"\n",
        "\n",
        "    chat_history = [\n",
        "        {\"role\": item.get(\"role\", \"user\"), \"content\": (item.get(\"content\") or \"\").strip()}\n",
        "        for item in (chat_history or [])\n",
        "        if (item.get(\"content\") or \"\").strip()\n",
        "    ]\n",
        "\n",
        "    fallback_error = None\n",
        "\n",
        "    retrieval_query = question\n",
        "    recent_user_turns = [t[\"content\"] for t in chat_history if t.get(\"role\") == \"user\"]\n",
        "    if recent_user_turns:\n",
        "        retrieval_query = \" \".join(recent_user_turns[-2:] + [question])\n",
        "\n",
        "    hits = []\n",
        "    top_hits = []\n",
        "    if 'retrieve' in globals():\n",
        "        try:\n",
        "            hits = retrieve(retrieval_query, top_k=6)\n",
        "            if hits:\n",
        "                limit = max(1, int(min(4, len(hits))))\n",
        "                top_hits = hits[:limit]\n",
        "        except Exception:\n",
        "            hits = []\n",
        "            top_hits = []\n",
        "\n",
        "    if 'responder' in globals():\n",
        "        try:\n",
        "            reply = responder(question)\n",
        "            if for_telegram:\n",
        "                reply = re.sub(r\"\\n\\s*Fuentes\\s*:.*$\", \"\", reply, flags=re.S)\n",
        "            return reply\n",
        "        except Exception as exc:\n",
        "            fallback_error = f\"âš ï¸ Error en responder(): {exc}\"\n",
        "\n",
        "    try:\n",
        "        temp_env = os.getenv(\"LLM_TEMPERATURE\")\n",
        "        temperature = float(temp_env) if temp_env is not None else None\n",
        "    except Exception:\n",
        "        temperature = None\n",
        "    try:\n",
        "        max_tok_env = os.getenv(\"LLM_MAX_TOKENS\")\n",
        "        max_tokens = int(max_tok_env) if max_tok_env is not None else None\n",
        "    except Exception:\n",
        "        max_tokens = None\n",
        "\n",
        "    if 'rag_answer' in globals():\n",
        "        try:\n",
        "            answer, _ = rag_answer(\n",
        "                question,\n",
        "                top_k=len(top_hits) or 4,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "                with_sources=not for_telegram,\n",
        "                hits=top_hits if top_hits else None,\n",
        "                chat_history=chat_history,\n",
        "            )\n",
        "            return answer\n",
        "        except Exception as exc:\n",
        "            fallback_error = f\"âš ï¸ Error en rag_answer(): {exc}\"\n",
        "\n",
        "    chosen_hits = top_hits if top_hits else hits\n",
        "    answer = LLM.generate(\n",
        "        question,\n",
        "        hits=chosen_hits,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        chat_history=chat_history,\n",
        "    )\n",
        "    if not for_telegram and chosen_hits:\n",
        "        sources = collect_sources(chosen_hits)\n",
        "        if sources and \"Fuentes\" not in answer:\n",
        "            answer = answer.rstrip() + \"\\n\\nFuentes: \" + \", \".join(sources)\n",
        "    if fallback_error and \"âš ï¸\" not in answer:\n",
        "        answer = answer.rstrip() + \"\\n\\n\" + fallback_error\n",
        "    return answer or \"No encuentro esa informaciÃ³n en mis archivos.\"\n",
        "\n",
        "\n",
        "class TTGram:\n",
        "    HOLD_AFTER_ADMIN = 60\n",
        "    HOLD_AFTER_ALERT = 120\n",
        "    AUTO_CLOSE_AFTER = 600\n",
        "    HISTORY_LIMIT = 12\n",
        "    ALERT_REGEX = re.compile(r\"\\b(asesor|ayuda)\\b\", re.I)\n",
        "\n",
        "    def __init__(self):\n",
        "        base_token = (\n",
        "            os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
        "            or (userdata.get(\"TELEGRAM_BOT_TOKEN\") if 'userdata' in globals() else None)\n",
        "            or \"\"\n",
        "        )\n",
        "        self.token = base_token.strip()\n",
        "        self.offset = None\n",
        "        self.global_auto = True\n",
        "        self.stop = threading.Event()\n",
        "        self.thread = None\n",
        "        self.known: Dict[str, Dict[str, Any]] = {}\n",
        "        self.admin_chat_id = (os.getenv(\"ADMIN_CHAT_ID\") or \"\").strip()\n",
        "        self.logs = LOGS_DIR if 'LOGS_DIR' in globals() else Path.cwd() / \"mini_chatbot_work\" / \"logs\"\n",
        "        self.logs.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def shutdown(self):\n",
        "        \"\"\"Detiene hilos/polling activos para evitar duplicados al re-ejecutar la celda.\"\"\"\n",
        "        self.stop.set()\n",
        "        thread = getattr(self, \"thread\", None)\n",
        "        if thread and thread.is_alive():\n",
        "            try:\n",
        "                thread.join(timeout=2)\n",
        "            except Exception:\n",
        "                pass\n",
        "        self.thread = None\n",
        "\n",
        "    # --------------- core helpers ---------------\n",
        "    def _known_meta(self, chat_id: str) -> Dict[str, Any]:\n",
        "        return self.known.setdefault(chat_id, {\n",
        "            \"title\": \"\",\n",
        "            \"last_text\": \"\",\n",
        "            \"auto\": None,\n",
        "            \"hold_until\": 0.0,\n",
        "            \"last_bot_reply\": \"\",\n",
        "            \"last_bot_ts\": 0.0,\n",
        "            \"history\": [],\n",
        "        })\n",
        "\n",
        "    def _update_known(self, chat_id: str, title: str = \"\") -> Dict[str, Any]:\n",
        "        meta = self._known_meta(chat_id)\n",
        "        if title and not meta.get(\"title\"):\n",
        "            meta[\"title\"] = title\n",
        "        return meta\n",
        "\n",
        "    def _append_history(self, meta: Dict[str, Any], role: str, text: str) -> List[Dict[str, str]]:\n",
        "        history = meta.setdefault(\"history\", [])\n",
        "        clean = (text or \"\").strip()\n",
        "        if not clean:\n",
        "            return history\n",
        "        history.append({\"role\": role, \"content\": clean})\n",
        "        if len(history) > self.HISTORY_LIMIT:\n",
        "            del history[:-self.HISTORY_LIMIT]\n",
        "        return history\n",
        "\n",
        "\n",
        "    def _log(self, chat_id: str, role: str, text: str):\n",
        "        record = {\"t\": time.time(), \"role\": role, \"text\": text}\n",
        "        try:\n",
        "            with open(self.logs / f\"{chat_id}.jsonl\", \"a\", encoding=\"utf-8\") as handle:\n",
        "                handle.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def _send(self, chat_id: str, text: str):\n",
        "        if not self.token:\n",
        "            raise RuntimeError(\"No TELEGRAM_BOT_TOKEN configurado.\")\n",
        "        response = requests.post(\n",
        "            f\"https://api.telegram.org/bot{self.token}/sendMessage\",\n",
        "            json={\"chat_id\": chat_id, \"text\": text},\n",
        "            timeout=15,\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "    def _send_silent(self, chat_id: str, text: str, role: Optional[str] = None, dedupe: bool = False) -> bool:\n",
        "        meta = None\n",
        "        if dedupe or role == \"bot\":\n",
        "            meta = self._known_meta(chat_id)\n",
        "            if dedupe and role == \"bot\":\n",
        "                last = (meta.get(\"last_bot_reply\") or \"\").strip().lower()\n",
        "                if last and last == text.strip().lower():\n",
        "                    return False\n",
        "        try:\n",
        "            self._send(chat_id, text)\n",
        "            if role == \"bot\":\n",
        "                if meta is None:\n",
        "                    meta = self._known_meta(chat_id)\n",
        "                meta[\"last_bot_reply\"] = text.strip()\n",
        "                meta[\"last_bot_ts\"] = time.time()\n",
        "            elif role == \"admin\":\n",
        "                if meta is None:\n",
        "                    meta = self._known_meta(chat_id)\n",
        "                meta[\"last_admin_reply\"] = text.strip()\n",
        "            return True\n",
        "        except Exception as exc:\n",
        "            self._log(chat_id, \"error\", f\"send fail: {exc}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _mirror_to_admin(self, message: str, origin_chat: Optional[str] = None):\n",
        "        admin = getattr(self, \"admin_chat_id\", \"\")\n",
        "        if not admin or admin == origin_chat:\n",
        "            return\n",
        "        self._send_silent(admin, message)\n",
        "\n",
        "    def _set_hold(self, chat_id: str, seconds: float):\n",
        "        meta = self._known_meta(chat_id)\n",
        "        meta[\"hold_until\"] = time.time() + float(seconds)\n",
        "\n",
        "    def _set_auto_chat(self, chat_id: str, flag: bool):\n",
        "        meta = self._known_meta(chat_id)\n",
        "        meta[\"auto\"] = bool(flag)\n",
        "        meta[\"hold_until\"] = 0.0\n",
        "        meta[\"awaiting_admin\"] = False\n",
        "        meta[\"closed_notified\"] = False\n",
        "\n",
        "    def _should_auto(self, chat_id: str) -> bool:\n",
        "        meta = self._known_meta(chat_id)\n",
        "        if meta.get(\"awaiting_admin\"):\n",
        "            return False\n",
        "        hold_until = meta.get(\"hold_until\", 0)\n",
        "        if hold_until and time.time() < hold_until:\n",
        "            return False\n",
        "        auto = meta.get(\"auto\")\n",
        "        if auto is None:\n",
        "            auto = self.global_auto\n",
        "        return bool(auto)\n",
        "\n",
        "    def _delete_webhook(self):\n",
        "        if not self.token:\n",
        "            return\n",
        "        try:\n",
        "            requests.get(f\"https://api.telegram.org/bot{self.token}/deleteWebhook\", timeout=10)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def _me(self):\n",
        "        if not self.token:\n",
        "            return \"No TELEGRAM_BOT_TOKEN configurado.\"\n",
        "        try:\n",
        "            resp = requests.get(f\"https://api.telegram.org/bot{self.token}/getMe\", timeout=10)\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json().get(\"result\", {})\n",
        "            username = data.get(\"username\", \"(sin username)\")\n",
        "            return \"Bot: @\" + username\n",
        "        except Exception as exc:\n",
        "            return f\"Token/Bot error: {exc}\"\n",
        "\n",
        "    # --------------- admin commands ---------------\n",
        "    def _handle_admin_command(self, text: str) -> bool:\n",
        "        cid = getattr(self, \"admin_chat_id\", \"\")\n",
        "        if not cid:\n",
        "            return False\n",
        "        cmd = text.strip()\n",
        "        if cmd == \"/admin\":\n",
        "            self._send_silent(cid, \"Ya eres canal admin âœ…\", role=\"bot\")\n",
        "            return True\n",
        "        if cmd.startswith(\"/auto_on\"):\n",
        "            parts = cmd.split()\n",
        "            if len(parts) >= 2:\n",
        "                self._set_auto_chat(parts[1], True)\n",
        "                self._send_silent(cid, f\"Auto ON para {parts[1]}\", role=\"bot\")\n",
        "            return True\n",
        "        if cmd.startswith(\"/auto_off\"):\n",
        "            parts = cmd.split()\n",
        "            if len(parts) >= 2:\n",
        "                self._set_auto_chat(parts[1], False)\n",
        "                self._send_silent(cid, f\"Auto OFF para {parts[1]}\", role=\"bot\")\n",
        "            return True\n",
        "        if cmd.startswith(\"/say\"):\n",
        "            parts = cmd.split(maxsplit=2)\n",
        "            if len(parts) >= 3:\n",
        "                target, message = parts[1], parts[2]\n",
        "                if self._send_silent(target, message, role=\"admin\"):\n",
        "                    self._log(target, \"admin\", message)\n",
        "                self._set_hold(target, self.HOLD_AFTER_ADMIN)\n",
        "                meta = self._known_meta(target)\n",
        "                meta[\"awaiting_admin\"] = False\n",
        "                meta[\"closed_notified\"] = False\n",
        "                self._mirror_to_admin(f\"[ADMINâ†’{target}] {message}\", origin_chat=cid)\n",
        "            return True\n",
        "        if cmd.startswith(\"/close\"):\n",
        "            parts = cmd.split(maxsplit=1)\n",
        "            if len(parts) >= 2:\n",
        "                self.close_chat(parts[1], notify_user=True)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # --------------- public API ---------------\n",
        "    def set_token(self, token: str):\n",
        "        token = (token or self.token or os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
        "                 or (userdata.get(\"TELEGRAM_BOT_TOKEN\") if 'userdata' in globals() else \"\")).strip()\n",
        "        if token.upper().startswith(\"BOT:\"):\n",
        "            token = token.split(\":\", 1)[1].strip()\n",
        "        self.token = token\n",
        "        if token:\n",
        "            os.environ[\"TELEGRAM_BOT_TOKEN\"] = token\n",
        "            if 'userdata' in globals():\n",
        "                try:\n",
        "                    userdata[\"TELEGRAM_BOT_TOKEN\"] = token\n",
        "                except Exception:\n",
        "                    pass\n",
        "        self._delete_webhook()\n",
        "        return self._me()\n",
        "\n",
        "    def toggle_poll(self, flag: bool):\n",
        "        if flag:\n",
        "            if self.thread and self.thread.is_alive():\n",
        "                return \"Auto-escuchar: ya activo.\"\n",
        "            self.stop.clear()\n",
        "            self._delete_webhook()\n",
        "            self.thread = threading.Thread(target=self._loop, daemon=True)\n",
        "            self.thread.start()\n",
        "            return \"Auto-escuchar: ON\"\n",
        "        self.stop.set()\n",
        "        thread = getattr(self, \"thread\", None)\n",
        "        if thread and thread.is_alive():\n",
        "            try:\n",
        "                thread.join(timeout=2)\n",
        "            except Exception:\n",
        "                pass\n",
        "        self.thread = None\n",
        "        return \"Auto-escuchar: OFF\"\n",
        "\n",
        "    def poll_once(self):\n",
        "        if not self.token:\n",
        "            return \"Sin TELEGRAM_BOT_TOKEN\"\n",
        "        try:\n",
        "            params = {\"timeout\": 10}\n",
        "            if self.offset is not None:\n",
        "                params[\"offset\"] = self.offset\n",
        "            resp = requests.get(\n",
        "                f\"https://api.telegram.org/bot{self.token}/getUpdates\",\n",
        "                params=params,\n",
        "                timeout=15,\n",
        "            )\n",
        "            if resp.status_code == 409:\n",
        "                self._delete_webhook()\n",
        "                self.offset = None\n",
        "                try:\n",
        "                    detail = resp.json().get(\"description\", \"\")\n",
        "                except Exception:\n",
        "                    detail = resp.text\n",
        "                return (\n",
        "                    \"âš ï¸ Telegram devolviÃ³ 409 (posible webhook activo). \"\n",
        "                    \"Se eliminÃ³ el webhook; vuelve a pulsar 'Leer ahora'. \"\n",
        "                    f\"Detalle: {detail.strip()}\"\n",
        "                )\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json()\n",
        "            for item in data.get(\"result\", []):\n",
        "                self.offset = item[\"update_id\"] + 1\n",
        "                self.handle_update(item)\n",
        "            return f\"OK, updates: {len(data.get('result', []))}\"\n",
        "        except Exception as exc:\n",
        "            return f\"âŒ Poll error: {exc}\"\n",
        "\n",
        "    def poll_now(self):\n",
        "        return self.poll_once()\n",
        "\n",
        "    def _loop(self):\n",
        "        while not self.stop.is_set():\n",
        "            self.poll_once()\n",
        "            self._check_auto_close()\n",
        "            for _ in range(10):\n",
        "                if self.stop.is_set():\n",
        "                    break\n",
        "                time.sleep(0.3)\n",
        "\n",
        "    def _check_auto_close(self):\n",
        "        now = time.time()\n",
        "        for cid, meta in list(self.known.items()):\n",
        "            alert_ts = meta.get(\"alert_ts\")\n",
        "            awaiting = meta.get(\"awaiting_admin\", False)\n",
        "            closed = meta.get(\"closed_notified\", False)\n",
        "            if awaiting and alert_ts and not closed and now - alert_ts > self.AUTO_CLOSE_AFTER:\n",
        "                message = (\n",
        "                    \"Cierro esta conversaciÃ³n por inactividad. Si necesitas mÃ¡s ayuda, \"\n",
        "                    \"escrÃ­beme de nuevo y retomarÃ© la consulta.\"\n",
        "                )\n",
        "                if self._send_silent(cid, message, role=\"bot\"):\n",
        "                    self._log(cid, \"bot\", message)\n",
        "                meta[\"awaiting_admin\"] = False\n",
        "                meta[\"closed_notified\"] = True\n",
        "                meta[\"hold_until\"] = 0.0\n",
        "                meta[\"auto\"] = self.global_auto\n",
        "                if self.admin_chat_id:\n",
        "                    self._send_silent(self.admin_chat_id, f\"â„¹ï¸ ConversaciÃ³n {cid} cerrada por inactividad.\")\n",
        "\n",
        "    def list_chats(self):\n",
        "        if not self.known:\n",
        "            return \"(sin chats aÃºn â€” envÃ­a /start al bot y pulsa 'Leer ahora')\"\n",
        "        now = time.time()\n",
        "        lines = []\n",
        "        for cid, meta in self.known.items():\n",
        "            hold_remaining = max(0, int(meta.get(\"hold_until\", 0) - now))\n",
        "            auto = meta.get(\"auto\")\n",
        "            if auto is None:\n",
        "                auto = self.global_auto\n",
        "            awaiting = meta.get(\"awaiting_admin\") and not meta.get(\"closed_notified\")\n",
        "            state = \"esperando asesor\" if awaiting else \"activo\"\n",
        "            alert_age = \"\"\n",
        "            if awaiting:\n",
        "                age = max(0, int(now - meta.get(\"alert_ts\", now)))\n",
        "                alert_age = f\" (hace {age}s)\"\n",
        "            prefix = \"âš ï¸ \" if awaiting else \"\"\n",
        "            lines.append(\n",
        "                f\"{prefix}{cid} | {meta.get('title','')} | auto={'ON' if auto else 'OFF'} | hold={hold_remaining}s | {state}{alert_age} | Ãºltimo: {meta.get('last_text','')[:60]}\"\n",
        "            )\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def admin_send(self, chat_id: str, text: str):\n",
        "        chat_id = (chat_id or \"\").strip()\n",
        "        if not chat_id:\n",
        "            return \"Selecciona un chat.\"\n",
        "        message = text or \"(mensaje vacÃ­o)\"\n",
        "        try:\n",
        "            self._send(chat_id, message)\n",
        "            self._log(chat_id, \"admin\", message)\n",
        "            meta = self._known_meta(chat_id)\n",
        "            meta[\"last_text\"] = message\n",
        "            meta[\"awaiting_admin\"] = False\n",
        "            meta[\"closed_notified\"] = False\n",
        "            self._set_hold(chat_id, self.HOLD_AFTER_ADMIN)\n",
        "            self._mirror_to_admin(f\"[ADMINâ†’{chat_id}] {message}\", origin_chat=self.admin_chat_id)\n",
        "            return \"âœ… Mensaje enviado.\"\n",
        "        except Exception as exc:\n",
        "            return f\"âŒ Error: {exc}\"\n",
        "\n",
        "    def auto_toggle(self, flag: bool):\n",
        "        self.global_auto = bool(flag)\n",
        "        return f\"Auto-responder (global): {'ON' if self.global_auto else 'OFF'}\"\n",
        "\n",
        "    def view_chat(self, chat_id: str, limit: int = 40):\n",
        "        chat_id = (chat_id or \"\").strip()\n",
        "        if not chat_id:\n",
        "            return \"Indica un chat_id.\"\n",
        "        path = self.logs / f\"{chat_id}.jsonl\"\n",
        "        if not path.exists():\n",
        "            return \"No hay historial para ese chat.\"\n",
        "        try:\n",
        "            raw = path.read_text(encoding=\"utf-8\")\n",
        "        except Exception as exc:\n",
        "            return f\"No se pudo leer el historial: {exc}\"\n",
        "        entries = [json.loads(line) for line in raw.splitlines() if line.strip()]\n",
        "        entries = entries[-limit:]\n",
        "        formatted = []\n",
        "        for entry in entries:\n",
        "            ts = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(entry.get(\"t\", time.time())))\n",
        "            role = entry.get(\"role\", \"\").upper()\n",
        "            text = entry.get(\"text\", \"\")\n",
        "            formatted.append(f\"{ts} | {role}: {text}\")\n",
        "        return \"\\n\".join(formatted)\n",
        "\n",
        "    def release_hold(self, chat_id: str):\n",
        "        chat_id = (chat_id or \"\").strip()\n",
        "        if not chat_id:\n",
        "            return \"Indica un chat_id.\"\n",
        "        meta = self._known_meta(chat_id)\n",
        "        meta[\"hold_until\"] = 0.0\n",
        "        meta[\"awaiting_admin\"] = False\n",
        "        meta[\"closed_notified\"] = False\n",
        "        if meta.get(\"auto\") is None:\n",
        "            meta[\"auto\"] = self.global_auto\n",
        "        return f\"Hold liberado y auto={'ON' if meta.get('auto') else 'OFF'}\"\n",
        "\n",
        "    def close_chat(self, chat_id: str, notify_user: bool = True):\n",
        "        chat_id = (chat_id or \"\").strip()\n",
        "        if not chat_id:\n",
        "            return \"Indica un chat_id.\"\n",
        "        meta = self._known_meta(chat_id)\n",
        "        closing = \"Cierro esta conversaciÃ³n. Si necesitas mÃ¡s ayuda, escrÃ­beme nuevamente.\" if notify_user else \"Soporte cerrado.\"\n",
        "        if notify_user:\n",
        "            self._send_silent(chat_id, closing, role=\"bot\")\n",
        "            self._log(chat_id, \"bot\", closing)\n",
        "        meta[\"awaiting_admin\"] = False\n",
        "        meta[\"hold_until\"] = time.time() + 2\n",
        "        meta[\"auto\"] = self.global_auto\n",
        "        meta[\"closed_notified\"] = True\n",
        "        if self.admin_chat_id:\n",
        "            self._send_silent(self.admin_chat_id, f\"â„¹ï¸ ConversaciÃ³n {chat_id} marcada como cerrada.\")\n",
        "        return \"âœ… ConversaciÃ³n cerrada.\"\n",
        "\n",
        "    def handle_update(self, item):\n",
        "        message = item.get(\"message\") or {}\n",
        "        chat = message.get(\"chat\") or {}\n",
        "        chat_id = str(chat.get(\"id\")) if chat.get(\"id\") is not None else \"\"\n",
        "        text = (message.get(\"text\") or \"\").strip()\n",
        "        title = chat.get(\"username\") or chat.get(\"title\") or \"\"\n",
        "        if not chat_id or not text:\n",
        "            return\n",
        "\n",
        "        sender = message.get(\"from\") or {}\n",
        "        if sender.get(\"is_bot\"):\n",
        "            return\n",
        "\n",
        "        meta = self._update_known(chat_id, title)\n",
        "        meta[\"last_text\"] = text\n",
        "        if message.get(\"date\"):\n",
        "            meta[\"last_user_ts\"] = message[\"date\"]\n",
        "        else:\n",
        "            meta[\"last_user_ts\"] = time.time()\n",
        "\n",
        "        self._log(chat_id, \"user\", text)\n",
        "        admin_chat = getattr(self, \"admin_chat_id\", \"\")\n",
        "        is_admin_chat = chat_id == admin_chat\n",
        "        if not is_admin_chat:\n",
        "            self._mirror_to_admin(f\"[USERâ†’{chat_id}] {text}\", origin_chat=chat_id)\n",
        "\n",
        "        if is_admin_chat:\n",
        "            if self._handle_admin_command(text):\n",
        "                return\n",
        "\n",
        "        if text == \"/admin\" and not admin_chat:\n",
        "            self.admin_chat_id = chat_id\n",
        "            os.environ[\"ADMIN_CHAT_ID\"] = chat_id\n",
        "            if 'userdata' in globals():\n",
        "                try:\n",
        "                    userdata[\"ADMIN_CHAT_ID\"] = chat_id\n",
        "                except Exception:\n",
        "                    pass\n",
        "            self._send_silent(chat_id, \"Este chat queda configurado como canal admin âœ…\", role=\"bot\")\n",
        "            return\n",
        "\n",
        "        history_ref = None\n",
        "        history_for_model: List[Dict[str, str]] = []\n",
        "        if not is_admin_chat:\n",
        "            history_ref = self._append_history(meta, \"user\", text)\n",
        "            history_for_model = history_ref[:-1]\n",
        "\n",
        "        if self.ALERT_REGEX.search(text):\n",
        "            handoff = (\n",
        "                \"He activado soporte humano. Un asesor se unirÃ¡ en breve. \"\n",
        "                \"TambiÃ©n puedes seguir escribiendo y lo revisarÃ©.\"\n",
        "            )\n",
        "            if self._send_silent(chat_id, handoff, role=\"bot\"):\n",
        "                self._log(chat_id, \"bot\", handoff)\n",
        "            meta[\"awaiting_admin\"] = True\n",
        "            meta[\"alert_ts\"] = time.time()\n",
        "            meta[\"auto\"] = False\n",
        "            meta[\"closed_notified\"] = False\n",
        "            self._set_hold(chat_id, self.HOLD_AFTER_ALERT)\n",
        "            if admin_chat:\n",
        "                self._send_silent(\n",
        "                    admin_chat,\n",
        "                    f\"âš ï¸ ALERTA: {chat_id} pidiÃ³ ayuda (â€˜{text}â€™). Usa /say {chat_id} <mensaje> o /auto_on {chat_id} para reanudar el bot.\",\n",
        "                    role=\"bot\",\n",
        "                )\n",
        "\n",
        "        if text.lower() == \"/start\":\n",
        "            welcome = (\n",
        "                \"Hola, soy tu asistente de Talento TECH. EscrÃ­beme tu duda y consultarÃ© los archivos de tu equipo.\"\n",
        "            )\n",
        "            if self._send_silent(chat_id, welcome, role=\"bot\"):\n",
        "                self._log(chat_id, \"bot\", welcome)\n",
        "            return\n",
        "\n",
        "        if self._should_auto(chat_id):\n",
        "            reply = _tt_answer_logic(text, for_telegram=True, chat_history=history_for_model)\n",
        "            if self._send_silent(chat_id, reply, role=\"bot\", dedupe=True):\n",
        "                self._log(chat_id, \"bot\", reply)\n",
        "                if history_ref is not None:\n",
        "                    self._append_history(meta, \"assistant\", reply)\n",
        "            self._mirror_to_admin(f\"[BOTâ†’{chat_id}] {reply}\", origin_chat=chat_id)\n",
        "\n",
        "    def status(self):\n",
        "        data = {\n",
        "            \"token_configured\": bool(self.token),\n",
        "            \"admin_chat\": self.admin_chat_id or \"(sin configurar)\",\n",
        "            \"global_auto\": self.global_auto,\n",
        "            \"known_chats\": len(self.known),\n",
        "            \"awaiting_admin\": [\n",
        "                cid\n",
        "                for cid, meta in self.known.items()\n",
        "                if meta.get(\"awaiting_admin\") and not meta.get(\"closed_notified\")\n",
        "            ],\n",
        "        }\n",
        "        return json.dumps(data, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def list_alerts(self):\n",
        "        waiting = []\n",
        "        now = time.time()\n",
        "        for cid, meta in self.known.items():\n",
        "            if not meta.get(\"awaiting_admin\") or meta.get(\"closed_notified\"):\n",
        "                continue\n",
        "            age = max(0, int(now - meta.get(\"alert_ts\", now)))\n",
        "            title = meta.get(\"title\", \"\")\n",
        "            last = meta.get(\"last_text\", \"\")\n",
        "            waiting.append(\n",
        "                f\"âš ï¸ {cid} | {title} | espera desde hace {age}s | Ãºltimo: {last[:80]}\"\n",
        "            )\n",
        "        if not waiting:\n",
        "            return \"(sin chats solicitando asesor)\"\n",
        "        return \"\\n\".join(waiting)\n",
        "\n",
        "\n",
        "if \"_TTG\" in globals():\n",
        "    try:\n",
        "        _TTG.shutdown()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "_TTG = TTGram()\n",
        "\n",
        "\n",
        "def ui_set_token(token: str):\n",
        "    return _TTG.set_token(token)\n",
        "\n",
        "\n",
        "def ui_poll_toggle(flag: bool):\n",
        "    return _TTG.toggle_poll(bool(flag))\n",
        "\n",
        "\n",
        "def ui_poll_once():\n",
        "    return _TTG.poll_now()\n",
        "\n",
        "\n",
        "def ui_list_chats():\n",
        "    return _TTG.list_chats()\n",
        "\n",
        "\n",
        "def ui_admin_send(chat_id: str, text: str):\n",
        "    return _TTG.admin_send(chat_id, text)\n",
        "\n",
        "\n",
        "def ui_auto_toggle(flag: bool):\n",
        "    return _TTG.auto_toggle(bool(flag))\n",
        "\n",
        "\n",
        "def ui_view_chat(chat_id: str):\n",
        "    return _TTG.view_chat(chat_id)\n",
        "\n",
        "\n",
        "def ui_close_chat(chat_id: str):\n",
        "    return _TTG.close_chat(chat_id, notify_user=True)\n",
        "\n",
        "\n",
        "def ui_release_hold(chat_id: str):\n",
        "    return _TTG.release_hold(chat_id)\n",
        "\n",
        "\n",
        "def ui_list_alerts():\n",
        "    return _TTG.list_alerts()\n",
        "\n",
        "\n",
        "def ui_bot_status():\n",
        "    return _TTG.status()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "nSb_o1qQjkvj",
      "metadata": {
        "id": "nSb_o1qQjkvj"
      },
      "outputs": [],
      "source": [
        "STATE = {\n",
        "    \"bucket\": os.getenv(\"S3_BUCKET\", \"\"),\n",
        "    \"base_prefix\": norm_prefix(os.getenv(\"S3_PREFIX\", \"\")),\n",
        "    \"team_folder\": \"\",\n",
        "}\n",
        "\n",
        "\n",
        "def apply_route(bucket, base_prefix, team_folder):\n",
        "    STATE[\"bucket\"] = (bucket or '').strip() or STATE[\"bucket\"]\n",
        "    STATE[\"base_prefix\"] = norm_prefix(base_prefix or STATE[\"base_prefix\"]) \\\n",
        "        or norm_prefix(os.getenv(\"S3_PREFIX\", \"\"))\n",
        "    STATE[\"team_folder\"] = (team_folder or '').strip()\n",
        "\n",
        "    eff_root = effective_team_prefix(STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
        "    bucket_name = STATE[\"bucket\"]\n",
        "    if bucket_name:\n",
        "        try:\n",
        "            s3 = s3_client_autoregion(bucket_name)\n",
        "            for sub in (\"docs/\", \"index/\"):\n",
        "                key = eff_root + sub\n",
        "                s3.put_object(Bucket=bucket_name, Key=key, Body=b'')\n",
        "        except Exception:\n",
        "            pass\n",
        "    return f\"âœ” Ruta: s3://{bucket_name}/{eff_root}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kH3shO4KYZqH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "kH3shO4KYZqH",
        "outputId": "165d70a9-d1d1-4e5d-f899-84154b81c00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b5575f30c3df959e3e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b5575f30c3df959e3e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import shutil # Import shutil for copying files\n",
        "\n",
        "\n",
        "def format_llm_status(status: Dict[str, Any]) -> str:\n",
        "    available = \", \".join(status.get(\"available\", []))\n",
        "    lines = [\n",
        "        f\"Proveedor activo: {status.get('provider')}\",\n",
        "        f\"Proveedor configurado: {status.get('configured_provider')}\",\n",
        "        f\"Disponibles: {available or 'fragmento'}\",\n",
        "        f\"Temperatura: {status.get('temperature')}\",\n",
        "        f\"MÃ¡x. tokens: {status.get('max_tokens')}\",\n",
        "        f\"Modelo OpenAI: {status.get('openai_model')} ({'llave âœ…' if status.get('has_openai_key') else 'sin llave'})\",\n",
        "        f\"Modelo Gemini: {status.get('gemini_model')} ({'llave âœ…' if status.get('has_gemini_key') else 'sin llave'})\",\n",
        "    ]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def ui_apply_s3(bucket, base_prefix, team_folder):\n",
        "    return apply_route(bucket, base_prefix, team_folder)\n",
        "\n",
        "\n",
        "def ui_list_teams(bucket, base_prefix):\n",
        "    bucket = (bucket or '').strip() or os.getenv(\"S3_BUCKET\")\n",
        "    base_prefix = norm_prefix(base_prefix or os.getenv(\"S3_PREFIX\") or \"\")\n",
        "    teams = s3_list_immediate_folders(bucket, base_prefix)\n",
        "    return \"\\n\".join(teams) if teams else \"(sin subcarpetas)\"\n",
        "\n",
        "\n",
        "def ui_upload(files):\n",
        "    saved = []\n",
        "    DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    # Gradio's gr.File(type=\"filepath\", file_count=\"multiple\") provides a list of file paths\n",
        "    for file_path in files or []:\n",
        "        if not isinstance(file_path, str):\n",
        "            continue # Skip if not a string path\n",
        "        try:\n",
        "            name = Path(file_path).name\n",
        "            destination_path = DOCS_DIR / name\n",
        "            # Explicitly open and read the source file, then write to the destination\n",
        "            with open(file_path, 'rb') as f_in, open(destination_path, 'wb') as f_out:\n",
        "                f_out.write(f_in.read())\n",
        "            saved.append(name)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving file {file_path}: {e}\") # Log error for debugging\n",
        "    if not saved:\n",
        "        return \"No se cargaron archivos.\"\n",
        "    return f\"Guardados local: {saved}\"\n",
        "\n",
        "\n",
        "def ui_sync_local_to_s3():\n",
        "    return s3_upload_local_docs(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
        "\n",
        "\n",
        "def ui_sync_s3_to_local():\n",
        "    return s3_download_docs(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
        "\n",
        "\n",
        "def ui_build_index(chunk_size, overlap):\n",
        "    paths = list(Path(DOCS_DIR).glob(\"*\"))\n",
        "    if not paths:\n",
        "        return \"Sube o sincroniza documentos primero.\"\n",
        "    n_chunks, n_docs = build_index_from_local(paths, int(chunk_size), int(overlap))\n",
        "    return f\"âœ… Ãndice local: {n_docs} docs â†’ {n_chunks} chunks.\"\n",
        "\n",
        "\n",
        "def ui_rebuild_from_s3(chunk_size, overlap):\n",
        "    return s3_rebuild_from_s3(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"], int(chunk_size), int(overlap))\n",
        "\n",
        "\n",
        "def ui_save_index():\n",
        "    return s3_upload_index(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
        "\n",
        "\n",
        "def ui_load_index():\n",
        "    return s3_download_index(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
        "\n",
        "\n",
        "def ui_ask(question, top_k, temp, max_tokens, history=None):\n",
        "    question = (question or \"\").strip()\n",
        "    history = list(history or [])\n",
        "    if not question:\n",
        "        return \"Escribe una pregunta.\", \"\", history\n",
        "    if not load_index_local():\n",
        "        return \"Primero carga/crea un Ã­ndice.\", \"\", history\n",
        "    history.append({\"role\": \"user\", \"content\": question})\n",
        "    trimmed_history = history[:-1]\n",
        "    answer, sources = rag_answer(\n",
        "        question,\n",
        "        top_k=int(top_k),\n",
        "        temperature=float(temp),\n",
        "        max_tokens=int(max_tokens),\n",
        "        with_sources=False,\n",
        "        chat_history=trimmed_history,\n",
        "    )\n",
        "    clean_answer = (answer or \"\").strip() or \"No encuentro esa informaciÃ³n en mis archivos.\"\n",
        "    history.append({\"role\": \"assistant\", \"content\": clean_answer})\n",
        "    if len(history) > 12:\n",
        "        history = history[-12:]\n",
        "    return clean_answer, \"\\n\".join(sorted(set(sources))), history\n",
        "\n",
        "\n",
        "def ui_llm_status():\n",
        "    return format_llm_status(LLM.status())\n",
        "\n",
        "\n",
        "def ui_llm_config(provider, openai_model, gemini_model, temperature, max_tokens):\n",
        "    status = LLM.configure(\n",
        "        provider=provider,\n",
        "        openai_model=openai_model,\n",
        "        gemini_model=gemini_model,\n",
        "        temperature=float(temperature),\n",
        "        max_tokens=int(max_tokens),\n",
        "        persist=True,\n",
        "    )\n",
        "    return \"âœ… ConfiguraciÃ³n guardada.\", format_llm_status(status)\n",
        "\n",
        "\n",
        "def ui_available_sources():\n",
        "    items = available_sources()\n",
        "    if not items:\n",
        "        return \"No hay documentos indexados todavÃ­a. Usa las pestaÃ±as de Docs y Ãndice para cargarlos y construir el Ã­ndice.\"\n",
        "    return \"\\n\".join(f\"- {name}\" for name in items)\n",
        "\n",
        "\n",
        "def ui_view_chat_history(chat_id):\n",
        "    return ui_view_chat(chat_id)\n",
        "\n",
        "\n",
        "def ui_close_chat_from_ui(chat_id):\n",
        "    return ui_close_chat(chat_id)\n",
        "\n",
        "\n",
        "def ui_release_hold_from_ui(chat_id):\n",
        "    return ui_release_hold(chat_id)\n",
        "\n",
        "\n",
        "def ui_bot_status_box():\n",
        "    return ui_bot_status()\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Chatbot RAG + S3 + Telegram (Admin & equipos)\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"### ðŸ§  Flujo: Docs â†’ Ãndice â†’ Preguntas â†’ Telegram (Admin)\")\n",
        "\n",
        "    with gr.Tab(\"0) S3 (Equipo)\"):\n",
        "        with gr.Row():\n",
        "            bucket = gr.Textbox(value=os.getenv(\"S3_BUCKET\"), label=\"S3_BUCKET\")\n",
        "            base_prefix = gr.Textbox(value=os.getenv(\"S3_PREFIX\"), label=\"Prefijo base (p.ej. IA-Innovador/)\")\n",
        "            team_folder = gr.Textbox(label=\"Carpeta del equipo (p.ej. Daniel)\")\n",
        "        s3_state = gr.Textbox(label=\"Estado de la ruta activa\", lines=3)\n",
        "        teams_out = gr.Textbox(label=\"Carpetas encontradas\", lines=6)\n",
        "        with gr.Row():\n",
        "            btn_apply = gr.Button(\"Aplicar ruta\")\n",
        "            btn_list = gr.Button(\"Listar carpetas existentes\")\n",
        "        btn_apply.click(ui_apply_s3, inputs=[bucket, base_prefix, team_folder], outputs=s3_state)\n",
        "        btn_list.click(ui_list_teams, inputs=[bucket, base_prefix], outputs=teams_out)\n",
        "\n",
        "    with gr.Tab(\"1) Docs\"):\n",
        "        files = gr.File(label=\"Sube PDF/TXT/CSV\", file_count=\"multiple\", type=\"filepath\")\n",
        "        out_docs = gr.Textbox(label=\"Estado\", lines=3)\n",
        "        with gr.Row():\n",
        "            btn_upload = gr.Button(\"Subir a local\")\n",
        "            btn_s3_to_local = gr.Button(\"S3 â†’ Local\")\n",
        "            btn_local_to_s3 = gr.Button(\"Local â†’ S3\")\n",
        "        btn_upload.click(ui_upload, inputs=files, outputs=out_docs)\n",
        "        btn_s3_to_local.click(ui_sync_s3_to_local, outputs=out_docs)\n",
        "        btn_local_to_s3.click(ui_sync_local_to_s3, outputs=out_docs)\n",
        "        with gr.Accordion(\"Prompt base sugerido para generar el JSONL\", open=False):\n",
        "            gr.Markdown(\n",
        "                \"\"\"```text\n",
        "ActÃºa como un Generador de Conocimiento Conversacional (CEREBRO) para un chatbot RAG.\n",
        "Idioma: {espaÃ±ol_neutro | otro}\n",
        "Dominio/tema principal: {describe el tema o uso del bot}\n",
        "Nombre del bot (solo para tono): {NomadaAI | MiBot | ...}\n",
        "\n",
        "Tarea:\n",
        "1) Lee TODOS los archivos adjuntos (PDF, Word, CSV/Excel, imÃ¡genes con tablas/texto) y extrae contenido.\n",
        "2) Construye un corpus conversacional en formato JSONL (UNA lÃ­nea por objeto) optimizado para recuperaciÃ³n semÃ¡ntica:\n",
        "   - Entradas de varios niveles: documento, secciÃ³n/pÃ¡gina, pÃ¡rrafo/Ã­tem, fila de tabla/registro.\n",
        "   - Cada entrada debe poder responder mensajes libres (no FAQ rÃ­gido).\n",
        "\n",
        "Esquema de CADA lÃ­nea JSONL:\n",
        "{\n",
        "  \"id\": \"slug-unico\",\n",
        "  \"granularity\": \"doc|section|paragraph|record|definition|howto\",\n",
        "  \"topic\": \"tema breve (mÃ¡x. 6 palabras)\",\n",
        "  \"user_variants\": [\n",
        "    \"3â€“7 formas naturales de pedir esto en lenguaje libre\"\n",
        "  ],\n",
        "  \"assistant_reply\": \"respuesta fluida (80â€“160 palabras) basada SOLO en el contenido adjunto. Sin alucinar.\",\n",
        "  \"facts\": [\"3â€“8 hechos puntuales y verificables (con unidades/fechas)\"],\n",
        "  \"keywords\": [\"5â€“12 tÃ©rminos y sinÃ³nimos relevantes\"],\n",
        "  \"entities\": [\"personas|lugares|organizaciones|variables|columnas si aplica\"],\n",
        "  \"source\": {\n",
        "    \"file\": \"nombre_del_archivo.ext\",\n",
        "    \"locator\": \"pÃ¡g. X | secciÃ³n Y | hoja:Z fila:W | figura:N\",\n",
        "    \"quote\": \"cita corta opcional (<=25 palabras) que respalda el dato\"\n",
        "  },\n",
        "  \"units\": \"estÃ¡ndar/unificado si habÃ­a mezclas (%, aÃ±os, muertes/1000, km, etc.)\",\n",
        "  \"valid_for\": \"fechas o versiÃ³n si aplica\",\n",
        "  \"confidence\": 0.0-1.0\n",
        "}\n",
        "\n",
        "Reglas de extracciÃ³n y redacciÃ³n:\n",
        "- No inventes datos; si un dato no estÃ¡ en las fuentes, indica en assistant_reply quÃ© falta y pide con amabilidad lo necesario.\n",
        "- Normaliza nÃºmeros y unidades (2 decimales cuando corresponda). Explica siglas la primera vez.\n",
        "- Para tablas/CSV: crea entradas \"record\" por fila importante con user_variants que un usuario real escribirÃ­a (\"muÃ©strame la expectativa de vida de {paÃ­s} en {aÃ±o}\", etc.).\n",
        "- Para PDFs/Word: crea entradas \"doc\" (resumen ejecutivo), \"section\" y \"paragraph\" con tÃ­tulos claros.\n",
        "- Divide en fragmentos de 80â€“220 palabras mÃ¡x. (Ã³ 600â€“1000 caracteres) para favorecer embeddings. Evita pÃ¡rrafos gigantes.\n",
        "- Incluye **fuente y locator SIEMPRE** (archivo y pÃ¡gina/hoja/fila). \n",
        "- Deduplica contenido parecido; si son equivalentes, conserva la versiÃ³n mÃ¡s clara y marca las demÃ¡s con \"confidence\" menor.\n",
        "- MantÃ©n tono didÃ¡ctico, empÃ¡tico y directo; no menciones â€œsoy una IAâ€.\n",
        "- Si detectas PII sensible, anonimiza (ej.: â€œJuan P.â€) a menos que el archivo indique uso explÃ­cito.\n",
        "\n",
        "Cobertura mÃ­nima (si aplica al dominio):\n",
        "1) PresentaciÃ³n/alcance del bot (quÃ© hace y no hace)\n",
        "2) CÃ³mo consultar/quÃ© aportar (fechas, archivo, contexto)\n",
        "3) ResÃºmenes ejecutivos por documento\n",
        "4) Definiciones y glosario\n",
        "5) Procedimientos/How-to paso a paso\n",
        "6) Datos tabulares claves como â€œrecordâ€\n",
        "7) PolÃ­ticas/limitaciones/privacidad\n",
        "8) Fallbacks de desambiguaciÃ³n y escalamiento (â€œasesorâ€, â€œayudaâ€)\n",
        "\n",
        "ValidaciÃ³n (al final, agrega 1 ÃšLTIMA lÃ­nea especial con id=\"__validation__\"):\n",
        "{\n",
        "  \"id\": \"__validation__\",\n",
        "  \"stats\": {\n",
        "    \"files_leidos\": N,\n",
        "    \"entradas_total\": N,\n",
        "    \"por_granularity\": {\"doc\":n,\"section\":n,\"paragraph\":n,\"record\":n,\"definition\":n,\"howto\":n},\n",
        "    \"entradas_sin_source\": n\n",
        "  },\n",
        "  \"issues\": [\"lista breve de posibles problemas (pÃ¡ginas sin texto, columnas ambiguas, unidades mezcladas, etc.)\"],\n",
        "  \"sugerencias\": [\"mejoras para prÃ³xima iteraciÃ³n (mÃ¡s metadatos, aclarar siglas, etc.)\"]\n",
        "}\n",
        "\n",
        "Salida:\n",
        "- ENTREGAR EXCLUSIVAMENTE lÃ­neas JSONL vÃ¡lidas (sin comentarios ni encabezados).\n",
        "- Genera entre {80 y 200} entradas totales segÃºn el tamaÃ±o de los archivos.\n",
        "```\"\"\"\n",
        "            )\n",
        "\n",
        "    with gr.Tab(\"2) Ãndice\"):\n",
        "        out_index = gr.Textbox(label=\"Estado\", lines=4)\n",
        "        with gr.Row():\n",
        "            chunk_size = gr.Number(value=800, precision=0, label=\"chunk_size\")\n",
        "            overlap = gr.Number(value=150, precision=0, label=\"overlap\")\n",
        "        with gr.Row():\n",
        "            btn_build = gr.Button(\"Construir Ã­ndice desde local\")\n",
        "            btn_rebuild = gr.Button(\"Reconstruir Ã­ndice desde S3\")\n",
        "        btn_build.click(ui_build_index, inputs=[chunk_size, overlap], outputs=out_index)\n",
        "        btn_rebuild.click(ui_rebuild_from_s3, inputs=[chunk_size, overlap], outputs=out_index)\n",
        "\n",
        "    with gr.Tab(\"3) Persistencia\"):\n",
        "        out_persist = gr.Textbox(label=\"Estado\", lines=4)\n",
        "        with gr.Row():\n",
        "            btn_save = gr.Button(\"â¬†ï¸ Guardar Ã­ndice en S3\")\n",
        "            btn_load = gr.Button(\"â¬‡ï¸ Cargar Ã­ndice desde S3\")\n",
        "        btn_save.click(ui_save_index, outputs=out_persist)\n",
        "        btn_load.click(ui_load_index, outputs=out_persist)\n",
        "        gr.Markdown(\"Se intenta en `.../<equipo>/` y en `.../<equipo>/index/`.\")\n",
        "\n",
        "    with gr.Tab(\"4) LLM y parÃ¡metros\"):\n",
        "        llm_status_box = gr.Textbox(value=ui_llm_status(), label=\"Estado actual\", lines=7)\n",
        "        with gr.Row():\n",
        "            provider = gr.Dropdown(\n",
        "                choices=[\"openai\", \"gemini\", \"fragmento\"],\n",
        "                value=LLM.status().get(\"configured_provider\", \"openai\"),\n",
        "                label=\"Proveedor\",\n",
        "            )\n",
        "            temperature = gr.Slider(value=LLM.temperature, minimum=0.0, maximum=1.5, step=0.05, label=\"Temperatura\")\n",
        "            max_tokens = gr.Slider(value=LLM.max_tokens, minimum=50, maximum=1200, step=50, label=\"MÃ¡x. tokens\")\n",
        "        with gr.Row():\n",
        "            openai_model = gr.Textbox(value=LLM.openai_model, label=\"Modelo OpenAI (si aplica)\")\n",
        "            gemini_model = gr.Textbox(value=LLM.gemini_model, label=\"Modelo Gemini (si aplica)\")\n",
        "        llm_feedback = gr.Textbox(label=\"Resultado\", lines=2)\n",
        "        btn_save_llm = gr.Button(\"Guardar configuraciÃ³n LLM\")\n",
        "        btn_refresh_llm = gr.Button(\"Actualizar estado\")\n",
        "        btn_save_llm.click(\n",
        "            ui_llm_config,\n",
        "            inputs=[provider, openai_model, gemini_model, temperature, max_tokens],\n",
        "            outputs=[llm_feedback, llm_status_box],\n",
        "        )\n",
        "        btn_refresh_llm.click(lambda: (\"\", ui_llm_status()), outputs=[llm_feedback, llm_status_box])\n",
        "\n",
        "    with gr.Tab(\"5) Preguntar\"):\n",
        "        gr.Markdown(\"Realiza pruebas con el mismo motor que usa Telegram.\")\n",
        "        gr.Markdown(\"**GuÃ­a rÃ¡pida:** `top_k` controla cuÃ¡ntos fragmentos del Ã­ndice se mandan al modelo y `MÃ¡x. tokens` limita la longitud de la respuesta generada.\")\n",
        "        question = gr.Textbox(label=\"Pregunta\")\n",
        "        with gr.Row():\n",
        "            top_k = gr.Slider(value=4, minimum=1, maximum=10, step=1, label=\"top_k\")\n",
        "            temp_slider = gr.Slider(value=LLM.temperature, minimum=0.0, maximum=1.2, step=0.05, label=\"Temperatura (solo esta consulta)\")\n",
        "            max_tokens_slider = gr.Slider(value=LLM.max_tokens, minimum=100, maximum=1000, step=50, label=\"MÃ¡x. tokens (solo esta consulta)\")\n",
        "        answer_box = gr.Markdown(\"Respuesta\")\n",
        "        sources_box = gr.Textbox(label=\"Fuentes\", lines=4, interactive=False)\n",
        "        history_state = gr.State([])\n",
        "        ask_button = gr.Button(\"Consultar\")\n",
        "        ask_button.click(\n",
        "            ui_ask,\n",
        "            inputs=[question, top_k, temp_slider, max_tokens_slider, history_state],\n",
        "            outputs=[answer_box, sources_box, history_state],\n",
        "        )\n",
        "        btn_reset_chat = gr.Button(\"Nueva conversaciÃ³n\")\n",
        "        btn_reset_chat.click(lambda: (\"\", \"\", []), outputs=[answer_box, sources_box, history_state])\n",
        "\n",
        "        with gr.Row():\n",
        "            btn_list_sources = gr.Button(\"Ver archivos indexados\")\n",
        "            catalog_box = gr.Textbox(label=\"Archivos disponibles\", lines=4, interactive=False)\n",
        "        btn_list_sources.click(ui_available_sources, outputs=catalog_box)\n",
        "\n",
        "    with gr.Tab(\"6) Telegram (Admin)\"):\n",
        "        gr.Markdown(\n",
        "            \"Cuando un usuario escribe 'asesor' o 'ayuda', el bot pausa las respuestas automÃ¡ticas, avisa al admin y espera intervenciÃ³n humana.\"\n",
        "        )\n",
        "        with gr.Row():\n",
        "            token_box = gr.Textbox(label=\"BOT TOKEN\", type=\"password\", value=os.getenv(\"TELEGRAM_BOT_TOKEN\", \"\"))\n",
        "            token_result = gr.Textbox(label=\"Bot\", interactive=False)\n",
        "            btn_set_token = gr.Button(\"Guardar token / Ver bot\")\n",
        "            btn_use_saved = gr.Button(\"Usar token existente\")\n",
        "        btn_set_token.click(ui_set_token, inputs=token_box, outputs=token_result)\n",
        "        btn_use_saved.click(lambda: ui_set_token(\"\"), outputs=token_result)\n",
        "\n",
        "        with gr.Row():\n",
        "            auto_checkbox = gr.Checkbox(label=\"Auto-responder (bot responde solo)\", value=True)\n",
        "            auto_state = gr.Textbox(label=\"Estado\", interactive=False)\n",
        "            auto_checkbox.change(ui_auto_toggle, inputs=auto_checkbox, outputs=auto_state)\n",
        "        with gr.Row():\n",
        "            poll_checkbox = gr.Checkbox(label=\"Polling (escuchar mensajes)\", value=False)\n",
        "            poll_state = gr.Textbox(label=\"Estado\", interactive=False)\n",
        "            poll_checkbox.change(ui_poll_toggle, inputs=poll_checkbox, outputs=poll_state)\n",
        "            btn_poll_once = gr.Button(\"Leer ahora\")\n",
        "            poll_once_state = gr.Textbox(label=\"Resultado\", interactive=False)\n",
        "            btn_poll_once.click(ui_poll_once, outputs=poll_once_state)\n",
        "\n",
        "        with gr.Row():\n",
        "            btn_list_chats = gr.Button(\"Listar chats conocidos\")\n",
        "            chats_box = gr.Textbox(label=\"Chats\", lines=8)\n",
        "            btn_alerts = gr.Button(\"Chats pidiendo ayuda\")\n",
        "            alerts_box = gr.Textbox(label=\"Alertas\", lines=6)\n",
        "        btn_list_chats.click(ui_list_chats, outputs=chats_box)\n",
        "        btn_alerts.click(ui_list_alerts, outputs=alerts_box)\n",
        "        with gr.Row():\n",
        "            status_button = gr.Button(\"Estado del bot\")\n",
        "            status_box = gr.Textbox(label=\"Status\", lines=7)\n",
        "        status_button.click(lambda: (ui_bot_status_box()), outputs=status_box)\n",
        "\n",
        "        with gr.Row():\n",
        "            chat_id_box = gr.Textbox(label=\"chat_id seleccionado\")\n",
        "            history_box = gr.Textbox(label=\"Historial reciente\", lines=10)\n",
        "        with gr.Row():\n",
        "            btn_view = gr.Button(\"Ver historial\")\n",
        "            btn_release = gr.Button(\"Reanudar bot en chat\")\n",
        "            btn_close = gr.Button(\"Cerrar chat\")\n",
        "        btn_view.click(ui_view_chat_history, inputs=chat_id_box, outputs=history_box)\n",
        "        btn_release.click(ui_release_hold_from_ui, inputs=chat_id_box, outputs=history_box)\n",
        "        btn_close.click(ui_close_chat_from_ui, inputs=chat_id_box, outputs=history_box)\n",
        "\n",
        "        message_box = gr.Textbox(label=\"Mensaje del admin\", value=\"Hola, soy soporte. Â¿En quÃ© te ayudo?\")\n",
        "        send_result = gr.Textbox(label=\"Resultado\", interactive=False)\n",
        "        send_button = gr.Button(\"Enviar como admin\")\n",
        "        send_button.click(ui_admin_send, inputs=[chat_id_box, message_box], outputs=send_result)\n",
        "\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}