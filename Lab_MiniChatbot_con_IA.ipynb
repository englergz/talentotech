{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ARBQyjYD4zla",
      "metadata": {
        "id": "ARBQyjYD4zla"
      },
      "source": [
        "# Chatbot con **tus archivos** + Gemini + Telegram\n",
        "\n",
        "By: **Ing. Engler Gonzalez**\n",
        "\n",
        "\n",
        "**Sube todo a la carpeta `/content`**. El cuaderno ya la usa por defecto.\n",
        "Flujo:\n",
        "1) Instalar ‚Üí\n",
        "2) Subir archivos ‚Üí\n",
        "3) Crear KB ‚Üí\n",
        "4) Entrenar ‚Üí\n",
        "5) Gemini ‚Üí\n",
        "6) Telegram.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A5WXBZRt4zlk",
      "metadata": {
        "id": "A5WXBZRt4zlk"
      },
      "outputs": [],
      "source": [
        "# ‚¨áÔ∏è 1) Instalaci√≥n\n",
        "%%bash\n",
        "pip -q install pdfplumber pillow pytesseract speechrecognition pydub python-telegram-bot google-generativeai duckduckgo_search scikit-learn > /dev/null\n",
        "if command -v apt >/dev/null 2>&1; then\n",
        "  apt -qq update >/dev/null && apt -qq install -y tesseract-ocr >/dev/null || true\n",
        "fi\n",
        "echo 'Instalaci√≥n lista ‚úÖ'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rtCiFJwx4zlr",
      "metadata": {
        "id": "rtCiFJwx4zlr"
      },
      "source": [
        "## üîë Claves / Tokens\n",
        "- **Gemini (opcional)**: pega tu `GOOGLE_API_KEY`. Si est√° vac√≠o, usar√°s TF‚ÄëIDF.\n",
        "- **Telegram**: crea un bot con **@BotFather** y pega el `TELEGRAM_TOKEN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bXiiFTqB4zls",
      "metadata": {
        "id": "bXiiFTqB4zls"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "TELEGRAM_TOKEN = userdata.get('TELEGRAM_TOKEN')\n",
        "\n",
        "if GOOGLE_API_KEY:\n",
        "   os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY.strip()\n",
        "print('Gemini KEY cargada?', bool(os.getenv('GOOGLE_API_KEY')))\n",
        "\n",
        "if TELEGRAM_TOKEN:\n",
        "   os.environ['TELEGRAM_TOKEN'] = TELEGRAM_TOKEN.strip()\n",
        "print('Telegram TOKEN cargado?', bool(os.getenv('TELEGRAM_TOKEN')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DvSEar0K4zlp",
      "metadata": {
        "id": "DvSEar0K4zlp"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, pathlib\n",
        "if os.path.isdir('/content'):\n",
        "    os.chdir('/content')\n",
        "print('üìÇ Carpeta de trabajo:', pathlib.Path('.').resolve())\n",
        "print('üìÑ Archivos actuales:', os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7lmeccDG4zlt",
      "metadata": {
        "id": "7lmeccDG4zlt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List, Dict\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Detectar si estamos en Colab para subir archivos\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    from google.colab import files  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Gemini\n",
        "USE_GEMINI = False\n",
        "GEM_MODEL = None\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    if os.getenv('GOOGLE_API_KEY'):\n",
        "        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
        "        GEM_MODEL = genai.GenerativeModel('gemini-1.5-flash')\n",
        "        USE_GEMINI = True\n",
        "        print('‚úÖ Gemini listo.')\n",
        "    else:\n",
        "        print('‚ÑπÔ∏è Sin GOOGLE_API_KEY')\n",
        "except Exception as e:\n",
        "    print('‚ÑπÔ∏è Gemini no disponible', e)\n",
        "\n",
        "# Estado global\n",
        "KB_CHUNKS: List[Dict] = []\n",
        "_VECTORIZER = None\n",
        "_MATRIX = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ih8WIaer4zlv",
      "metadata": {
        "id": "ih8WIaer4zlv"
      },
      "source": [
        "## üì• 3) Subir y leer archivos (usa **/content**)\n",
        "### A) Subir desde el panel *Files*\n",
        "En la izquierda, entra a **content** ‚Üí icono de **subir** (flecha ‚Üë) ‚Üí elige tus archivos. Quedan en `/content`.\n",
        "\n",
        "### B) O usar la funci√≥n (abre di√°logo): `subir_archivos()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "j9ZbWLoB4zlw",
      "metadata": {
        "id": "j9ZbWLoB4zlw"
      },
      "outputs": [],
      "source": [
        "def subir_archivos() -> List[str]:\n",
        "    \"\"\"Di√°logo para subir archivos a /content en Colab.\"\"\"\n",
        "    if not IN_COLAB:\n",
        "        print('No est√°s en Colab. Copia tus archivos a /content manualmente.')\n",
        "        return []\n",
        "    up = files.upload()\n",
        "    guardados = []\n",
        "    for nombre, datos in up.items():\n",
        "        with open(nombre, 'wb') as f:\n",
        "            f.write(datos)\n",
        "        guardados.append(nombre)\n",
        "    print('‚úÖ Subidos:', guardados)\n",
        "    return guardados\n",
        "\n",
        "def listar_utiles():\n",
        "    import glob\n",
        "    print('üìÑ Aqu√≠ hay:')\n",
        "    print(glob.glob('*.pdf')+glob.glob('*.csv')+glob.glob('*.txt')+glob.glob('*.png')+glob.glob('*.jpg')+glob.glob('*.jpeg')+glob.glob('*.wav')+glob.glob('*.mp3'))\n",
        "\n",
        "def extraer_texto_archivo(path: str) -> str:\n",
        "    import os\n",
        "    texto = ''\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    try:\n",
        "        if ext == '.txt':\n",
        "            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                texto = f.read()\n",
        "        elif ext == '.csv':\n",
        "            tmp = pd.read_csv(path, encoding='utf-8', errors='ignore')\n",
        "            texto = '\\n'.join(tmp.astype(str).agg(' '.join, axis=1).tolist())\n",
        "        elif ext == '.pdf':\n",
        "            import pdfplumber\n",
        "            with pdfplumber.open(path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    texto += (page.extract_text() or '') + '\\n'\n",
        "        elif ext in ('.png', '.jpg', '.jpeg'):\n",
        "            from PIL import Image\n",
        "            import pytesseract\n",
        "            texto = pytesseract.image_to_string(Image.open(path), lang='spa')\n",
        "        elif ext == '.wav':\n",
        "            import speech_recognition as sr\n",
        "            r = sr.Recognizer()\n",
        "            with sr.AudioFile(path) as source:\n",
        "                audio = r.record(source)\n",
        "            texto = r.recognize_google(audio, language='es-ES')\n",
        "        elif ext == '.mp3':\n",
        "            from pydub import AudioSegment\n",
        "            import speech_recognition as sr\n",
        "            wav = path + '.wav'\n",
        "            AudioSegment.from_mp3(path).export(wav, format='wav')\n",
        "            r = sr.Recognizer()\n",
        "            with sr.AudioFile(wav) as source:\n",
        "                audio = r.record(source)\n",
        "            texto = r.recognize_google(audio, language='es-ES')\n",
        "        else:\n",
        "            print('Formato no soportado:', ext); return ''\n",
        "    except Exception as e:\n",
        "        print('Error leyendo', path, ':', e); return ''\n",
        "    print('‚úÖ Extra√≠do', len(texto), 'caracteres de', path)\n",
        "    return texto\n",
        "\n",
        "def trocear_texto(texto: str, max_len: int = 900) -> List[str]:\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "    return [texto[i:i+max_len] for i in range(0, len(texto), max_len)]\n",
        "\n",
        "def agregar_a_kb(path: str, texto: str, max_len: int = 900):\n",
        "    global KB_CHUNKS\n",
        "    for ch in trocear_texto(texto, max_len=max_len):\n",
        "        if ch:\n",
        "            KB_CHUNKS.append({'source': path, 'text': ch})\n",
        "    print('KB ahora tiene', len(KB_CHUNKS), 'fragmentos.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lpRgwlnu4zlz",
      "metadata": {
        "id": "lpRgwlnu4zlz"
      },
      "source": [
        "## üß† 4) Entrenar √≠ndice y responder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "OrUcFetP4zl0",
      "metadata": {
        "id": "OrUcFetP4zl0"
      },
      "outputs": [],
      "source": [
        "def entrenar_indice():\n",
        "    global _VECTORIZER, _MATRIX\n",
        "    if not KB_CHUNKS:\n",
        "        print('Primero agrega contenido a la KB.'); return\n",
        "    corpus = [c['text'] for c in KB_CHUNKS]\n",
        "    _VECTORIZER = TfidfVectorizer(lowercase=True, ngram_range=(1,2))\n",
        "    _MATRIX = _VECTORIZER.fit_transform(corpus)\n",
        "    print('‚úÖ √çndice TF‚ÄëIDF entrenado con', len(corpus), 'fragmentos.')\n",
        "\n",
        "def buscar_contexto(query: str, top_k: int = 4):\n",
        "    if _VECTORIZER is None or _MATRIX is None:\n",
        "        entrenar_indice()\n",
        "    if _VECTORIZER is None or _MATRIX is None:\n",
        "        return []\n",
        "    vec = _VECTORIZER.transform([query])\n",
        "    sims = cosine_similarity(vec, _MATRIX).ravel()\n",
        "    idx = sims.argsort()[::-1][:top_k]\n",
        "    return [KB_CHUNKS[i] | {'score': float(sims[i])} for i in idx]\n",
        "\n",
        "def responder(query: str) -> str:\n",
        "    ctx = buscar_contexto(query)\n",
        "    if not ctx:\n",
        "        return 'No tengo informaci√≥n a√∫n. Sube archivos y entrena el √≠ndice.'\n",
        "    if USE_GEMINI and GEM_MODEL:\n",
        "        context_text = '\\n\\n'.join([c['text'] for c in ctx])\n",
        "        prompt = (\n",
        "            'Responde en espa√±ol, breve y claro, usando SOLO el contexto.\\n'\n",
        "            \"Si no est√° en el contexto, di: 'No tengo esa informaci√≥n a√∫n'.\\n\\n\"\n",
        "            + 'Contexto:\\n' + context_text + '\\n\\n'\n",
        "            + 'Pregunta: ' + query + '\\nRespuesta:'\n",
        "        )\n",
        "        try:\n",
        "            resp = GEM_MODEL.generate_content(prompt)\n",
        "            txt = getattr(resp, 'text', '')\n",
        "            return txt.strip() or 'No tengo esa informaci√≥n a√∫n.'\n",
        "        except Exception as e:\n",
        "            print('Gemini fall√≥; uso TF‚ÄëIDF:', e)\n",
        "    best = max(ctx, key=lambda x: x['score'])\n",
        "    return best['text'][:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W605dszW4zl1",
      "metadata": {
        "id": "W605dszW4zl1"
      },
      "source": [
        "## ‚öôÔ∏è 5) Demo r√°pida\n",
        "1)  Ejecuta `subir_archivos()`\n",
        "2) Procesa y agrega a la KB.\n",
        "3) Entrena.  \n",
        "4) Pregunta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-pv4RBcQ4zl2",
      "metadata": {
        "id": "-pv4RBcQ4zl2"
      },
      "outputs": [],
      "source": [
        "# Subir (abre di√°logo en Colab) y listar\n",
        "archivos = subir_archivos() if IN_COLAB else []\n",
        "listar_utiles()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vEwEOTCJ4zl3",
      "metadata": {
        "id": "vEwEOTCJ4zl3"
      },
      "outputs": [],
      "source": [
        "# Procesar lo subido y cargar a KB\n",
        "for a in archivos:\n",
        "    t = extraer_texto_archivo(a)\n",
        "    if t:\n",
        "        agregar_a_kb(a, t)\n",
        "print('Fragmentos en KB:', len(KB_CHUNKS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "VppfX12i4zl4",
      "metadata": {
        "id": "VppfX12i4zl4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f23561c2-fd3c-4bab-b925-b18491422dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ √çndice TF‚ÄëIDF entrenado con 18 fragmentos.\n",
            "Las actividades son: Introducci√≥n a la representaci√≥n de im√°genes como matrices de p√≠xeles; Introducci√≥n a scikit-image; y Presentaci√≥n de OpenCV y sus caracter√≠sticas principales.\n"
          ]
        }
      ],
      "source": [
        "entrenar_indice()\n",
        "print(responder('¬øCu√°l es la actividad?'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bDGcysWW4zl4",
      "metadata": {
        "id": "bDGcysWW4zl4"
      },
      "source": [
        "## üí¨ 6) Telegram\n",
        "Responde a **cualquier texto** que reciba usando `responder(query)`.\n",
        "Antes de lanzar: asegura **KB > 0** y ejecutaste `entrenar_indice()`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOQUE √öNICO: Mantiene tu estructura, responde saludos y usa Gemini SOLO con tu KB ====\n",
        "# Requisitos opcionales: pip install google-generativeai\n",
        "# Variables esperadas (ya las tienes en tu cuaderno): TELEGRAM_TOKEN, KB_CHUNKS, _VECTORIZER, responder(q)\n",
        "# Si faltan, aqu√≠ ponemos valores seguros para que no truene.\n",
        "\n",
        "import os, re, time, asyncio, nest_asyncio\n",
        "from typing import List, Tuple, Optional\n",
        "from telegram.ext import Application, MessageHandler, filters\n",
        "from telegram import constants\n",
        "\n",
        "# -------- Defaults seguros si algo no existe ----------\n",
        "TELEGRAM_TOKEN = globals().get(\"TELEGRAM_TOKEN\", os.getenv(\"TELEGRAM_TOKEN\", \"\"))\n",
        "if \"KB_CHUNKS\" not in globals(): KB_CHUNKS = []\n",
        "if \"_VECTORIZER\" not in globals(): _VECTORIZER = None\n",
        "\n",
        "if \"responder\" not in globals():\n",
        "    def responder(q: str) -> str:\n",
        "        # Respaldo m√≠nimo si no tienes definida tu funci√≥n\n",
        "        return \"A√∫n no tengo mi √≠ndice entrenado. Carga tus archivos y ejecuta entrenar_indice().\"\n",
        "\n",
        "def _trained_ok() -> bool:\n",
        "    # Considera entrenado si hay al menos algo en la KB, aunque no haya vectorizador\n",
        "    return bool(KB_CHUNKS)\n",
        "\n",
        "# -------- Branding autom√°tico (saludo contextual) ----------\n",
        "def _infer_branding_from_kb() -> str:\n",
        "    # Busca pistas en la KB para construir el saludo\n",
        "    text = \" \".join([(t if isinstance(ch,(list,tuple)) and len(ch)==2 else \"\") for ch in KB_CHUNKS for t in (ch if isinstance(ch,(list,tuple)) else [str(ch)])])\n",
        "    t = text.lower()\n",
        "    base = \"asistente virtual\"\n",
        "    if \"procesamiento de datos\" in t and (\"ia\" in t or \"inteligencia artificial\" in t):\n",
        "        return f\"{base} de procesamiento de datos con IA de bootcamps\"\n",
        "    if \"talento tech\" in t:\n",
        "        return f\"{base} de Talento Tech\"\n",
        "    if \"botcamp\" in t or \"bootcamp\" in t:\n",
        "        return f\"{base} del bootcamp\"\n",
        "    return f\"{base}\"\n",
        "\n",
        "_BRANDING = _infer_branding_from_kb()\n",
        "\n",
        "# -------- Gemini (opcional y solo con tu KB; NO web) ----------\n",
        "HAVE_GEMINI = False\n",
        "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\", \"gemini-1.5-flash\")\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    _GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\").strip()\n",
        "    if _GEMINI_API_KEY:\n",
        "        genai.configure(api_key=_GEMINI_API_KEY)\n",
        "        HAVE_GEMINI = True\n",
        "except Exception:\n",
        "    HAVE_GEMINI = False\n",
        "\n",
        "def _is_low_conf(txt: Optional[str]) -> bool:\n",
        "    if not txt: return True\n",
        "    t = re.sub(r\"\\s+\",\" \",txt.strip().lower())\n",
        "    band = [\n",
        "        \"kb vac√≠a\",\"kb vacia\",\"√≠ndice sin entrenar\",\"indice sin entrenar\",\n",
        "        \"no estoy seguro\",\"no tengo informaci√≥n\",\"no encontr\",\"error\",\"excepci√≥n\",\"exception\",\"traceback\"\n",
        "    ]\n",
        "    return (len(t) < 25) or any(b in t for b in band)\n",
        "\n",
        "def _top_terms(s: str, k=12) -> List[str]:\n",
        "    return re.findall(r\"[a-zA-Z√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë0-9_-]{3,}\", s or \"\")[:k]\n",
        "\n",
        "def _retrieve_from_kb(q: str, k=3) -> List[Tuple[str,str,float]]:\n",
        "    # Ranqueo ligero por superposici√≥n de t√©rminos (no depende de _VECTORIZER)\n",
        "    qset = set(w.lower() for w in _top_terms(q, 20))\n",
        "    scored=[]\n",
        "    for ch in KB_CHUNKS:\n",
        "        if isinstance(ch,(list,tuple)) and len(ch)==2:\n",
        "            title, body = ch\n",
        "        else:\n",
        "            title, body = \"\", str(ch)\n",
        "        tset = set(w.lower() for w in _top_terms(f\"{title} {body}\", 80))\n",
        "        j = len(qset & tset) / max(1, len(qset | tset))\n",
        "        if j>0:\n",
        "            scored.append((title, body, j))\n",
        "    scored.sort(key=lambda x: x[2], reverse=True)\n",
        "    return scored[:k]\n",
        "\n",
        "def _gemini_with_kb(q: str) -> Optional[str]:\n",
        "    if not HAVE_GEMINI: return None\n",
        "    hits = _retrieve_from_kb(q, k=4)\n",
        "    if not hits and not _trained_ok():\n",
        "        return None\n",
        "    ctx = \"\\n\\n\".join([f\"Fragmento {i+1}:\\n{b}\" for i,(_,b,_) in enumerate(hits)]) if hits else \"\"\n",
        "    sys = (\"Eres un asistente en espa√±ol, breve y claro. Responde SOLO con base en el contexto proporcionado \"\n",
        "           \"(fragmentos de KB del usuario). Si falta informaci√≥n, dilo con precisi√≥n.\")\n",
        "    try:\n",
        "        model = genai.GenerativeModel(GEMINI_MODEL, system_instruction=sys)\n",
        "        prompt = f\"Pregunta:\\n{q}\\n\\nContexto del usuario:\\n{ctx}\\n\\nResponde en espa√±ol de forma directa.\"\n",
        "        r = model.generate_content(prompt)\n",
        "        return (r.text or \"\").strip()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# -------- Pipeline: usa tu responder(q) y, si queda corto, Gemini con tu KB ----------\n",
        "def responder_contextual(q: str) -> str:\n",
        "    # Saludo contextual (no mudo)\n",
        "    if re.fullmatch(r\"\\s*(hola|buenas|hey|hi)\\s*[!.]?\\s*\", q, flags=re.I):\n",
        "        return f\"Hola, soy tu {_BRANDING}. ¬øEn qu√© te ayudo?\"\n",
        "\n",
        "    # 1) Tu motor original\n",
        "    try:\n",
        "        base = responder(q)\n",
        "    except Exception as e:\n",
        "        base = f\"(Tu buscador interno fall√≥: {e})\"\n",
        "\n",
        "    if not _is_low_conf(base):\n",
        "        return base\n",
        "\n",
        "    # 2) Gemini con SOLO tu KB\n",
        "    g = _gemini_with_kb(q)\n",
        "    if g and not _is_low_conf(g):\n",
        "        return g\n",
        "\n",
        "    # 3) √öltimo recurso: mensaje √∫til sin web\n",
        "    if not _trained_ok():\n",
        "        return (\"A√∫n no tengo tu KB entrenada. Carga tus archivos y ejecuta 'entrenar_indice()'. \"\n",
        "                \"Mientras tanto, puedo darte una orientaci√≥n general si me das m√°s detalle.\")\n",
        "    return base or \"Necesito un poco m√°s de detalle o material en tu KB para responder mejor.\"\n",
        "\n",
        "# -------- Handlers (manteniendo tu estructura) ----------\n",
        "async def on_message(update, context):\n",
        "    q = (update.message.text or '').strip()\n",
        "    if not q:\n",
        "        await update.message.reply_text('Env√≠ame un texto y responder√© con lo que haya en tus archivos.')\n",
        "        return\n",
        "    # Ejecuta el pipeline en un hilo para no bloquear el loop\n",
        "    ans = await asyncio.to_thread(responder_contextual, q)\n",
        "    await update.message.reply_text(ans)\n",
        "\n",
        "async def main_telegram():\n",
        "    if not TELEGRAM_TOKEN:\n",
        "        print('‚ö†Ô∏è Falta TELEGRAM_TOKEN. P√©galo en la celda de Claves.')\n",
        "        return\n",
        "    if not _trained_ok():\n",
        "        print(\"‚ÑπÔ∏è KB vac√≠a o √≠ndice sin entrenar. Puedes cargar archivos y ejecutar 'entrenar_indice()' cuando gustes.\")\n",
        "\n",
        "    app = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, on_message))\n",
        "    print('ü§ñ Bot escuchando‚Ä¶ (env√≠ale un mensaje en Telegram)')\n",
        "    await app.run_polling()\n",
        "\n",
        "# --- Arranque (igual que tu c√≥digo) ---\n",
        "nest_asyncio.apply()\n",
        "# üëâ Descomenta para lanzar el bot (si tu entorno lo requiere ya est√° listo):\n",
        "await main_telegram()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "caG7gnvymXfC",
        "outputId": "e8f31076-44f5-47db-bef9-f1a563c5c0fc"
      },
      "id": "caG7gnvymXfC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Bot escuchando‚Ä¶ (env√≠ale un mensaje en Telegram)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8TkAOW564zl5",
      "metadata": {
        "id": "8TkAOW564zl5"
      },
      "source": [
        "## üÜò Ayuda r√°pida\n",
        "- **¬øD√≥nde subo?** A **/content** (panel Files ‚Üí *content* ‚Üí subir). Este cuaderno ya usa esa carpeta.\n",
        "- **No encuentra archivo**: ejecuta la celda de ruta (muestra `/content`), vuelve a subir y usa el **nombre exacto**.\n",
        "- **Gemini**: pega `GOOGLE_API_KEY` sin espacios extras y re‚Äëejecuta la celda de claves.\n",
        "- **OCR/audio**: repite instalaci√≥n si falla; usa im√°genes claras o audio `.wav`.\n",
        "- **Telegram**: pega `TELEGRAM_TOKEN`, aseg√∫rate de KB>0 y de haber entrenado el √≠ndice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EwyTk-1v4zl5",
      "metadata": {
        "id": "EwyTk-1v4zl5"
      },
      "outputs": [],
      "source": [
        "import asyncio, nest_asyncio\n",
        "from telegram.ext import Application, MessageHandler, filters\n",
        "\n",
        "async def on_message(update, context):\n",
        "    q = (update.message.text or '').strip()\n",
        "    if not q:\n",
        "        await update.message.reply_text('Env√≠ame un texto y responder√© con lo que haya en tus archivos.')\n",
        "        return\n",
        "    ans = responder(q)\n",
        "    await update.message.reply_text(ans)\n",
        "\n",
        "async def main_telegram():\n",
        "    if not TELEGRAM_TOKEN:\n",
        "        print('‚ö†Ô∏è Falta TELEGRAM_TOKEN. P√©galo en la celda de Claves.')\n",
        "        return\n",
        "    if not KB_CHUNKS or _VECTORIZER is None:\n",
        "        print(\"‚ö†Ô∏è KB vac√≠a o √≠ndice sin entrenar. Carga archivos y ejecuta 'entrenar_indice()' primero.\")\n",
        "    app = Application.builder().token(TELEGRAM_TOKEN).build()\n",
        "    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, on_message))\n",
        "    print('ü§ñ Bot escuchando‚Ä¶')\n",
        "    await app.run_polling()\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# üëâ Descomenta para lanzar el bot:\n",
        "await main_telegram()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}