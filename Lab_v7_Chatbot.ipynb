{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83HaTR3j7tkp"
   },
   "source": [
    "# üß™ Chatbot RAG + S3 + Telegram\n",
    "**Listo para Google Colab**\n",
    "\n",
    "### Novedades clave\n",
    "- **LLM seleccionable**: OpenAI *o* Gemini. Si no hay API key, cae en *modo fragmento m√°s relevante*.\n",
    "- **Persistencia real** en S3: √≠ndice FAISS guardado/cargado por equipo + reconstrucci√≥n desde `docs/`.\n",
    "- **Telegram completo**: los clientes escriben al bot, el admin ve todos los chats, activa/desactiva **Auto-responder** y puede **intervenir** en cualquier momento.\n",
    "- **Transcripciones** por `chat_id` en `mini_chatbot_work/logs/`.\n"
   ],
   "id": "83HaTR3j7tkp"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghh-ERwS7tk2"
   },
   "source": [
    "## 0) Instalaci√≥n"
   ],
   "id": "Ghh-ERwS7tk2"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tL-JJTo7tk3",
    "outputId": "bef2d6cd-ba28-4a85-8e15-8c51189c1111",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295171994,
     "user_tz": 300,
     "elapsed": 27420,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip -q install sentence-transformers faiss-cpu pypdf gradio boto3 openai==1.* tiktoken requests google-generativeai\n"
   ],
   "id": "-tL-JJTo7tk3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFEkEyMR7tk7"
   },
   "source": [
    "## 1) Configuraci√≥n"
   ],
   "id": "AFEkEyMR7tk7"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4nOrSx47tk8",
    "outputId": "ff8aacc5-b4bc-4238-8008-5c41a5593e53",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295199749,
     "user_tz": 300,
     "elapsed": 27719,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AWS_ACCESS_KEY_ID: AKIAQNBYZOKR56S2G57T\n",
      "AWS_SECRET_ACCESS_KEY (oculto): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "‚úî AWS: us-east-2 talentotech2025 IA-Innovador/\n",
      "‚úÖ API Key(s) cargada(s) correctamente.\n",
      "‚úî LLM provider: openai | OpenAI model: gpt-4o-mini | Gemini model: gemini-1.5-flash\n"
     ]
    }
   ],
   "source": [
    "# A) Entrada interactiva\n",
    "import os\n",
    "from getpass import getpass\n",
    "from google.colab import userdata\n",
    "\n",
    "# --- AWS / S3 ---\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-2\")\n",
    "S3_BUCKET  = os.getenv(\"S3_BUCKET\", \"talentotech2025\")\n",
    "S3_PREFIX  = os.getenv(\"S3_PREFIX\", \"IA-Innovador/\")  # prefijo base del curso\n",
    "\n",
    "# --- LLM ---\n",
    "LLM_PROVIDER = (os.getenv(\"LLM_PROVIDER\") or \"openai\").lower()   # \"openai\" o \"gemini\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or userdata.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL   = os.getenv(\"OPENAI_MODEL\")   or \"gpt-4o-mini\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or userdata.get(\"GOOGLE_API_KEY\")\n",
    "GEMINI_MODEL   = os.getenv(\"GEMINI_MODEL\")   or \"gemini-1.5-flash\"\n",
    "\n",
    "\n",
    "# Pide claves si faltan (opcional)\n",
    "AWS_ACCESS_KEY_ID     = os.getenv(\"AWS_ACCESS_KEY_ID\")     or input(\"AWS_ACCESS_KEY_ID: \").strip()\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\") or getpass(\"AWS_SECRET_ACCESS_KEY (oculto): \").strip()\n",
    "AWS_SESSION_TOKEN     = os.getenv(\"AWS_SESSION_TOKEN\")     # opcional (solo STS/Academy)\n",
    "\n",
    "# Persistir en entorno\n",
    "env = {\n",
    "\"AWS_REGION\":AWS_REGION, \"S3_BUCKET\":S3_BUCKET, \"S3_PREFIX\":S3_PREFIX,\n",
    "\"AWS_ACCESS_KEY_ID\":AWS_ACCESS_KEY_ID, \"AWS_SECRET_ACCESS_KEY\":AWS_SECRET_ACCESS_KEY\n",
    "}\n",
    "if AWS_SESSION_TOKEN: env[\"AWS_SESSION_TOKEN\"]=AWS_SESSION_TOKEN\n",
    "for k,v in env.items(): os.environ[k]=v\n",
    "\n",
    "# LLM env (deja vac√≠o si no tienes)\n",
    "os.environ[\"LLM_PROVIDER\"]=LLM_PROVIDER\n",
    "if OPENAI_API_KEY: os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_MODEL\"]=OPENAI_MODEL\n",
    "if GOOGLE_API_KEY: os.environ[\"GOOGLE_API_KEY\"]=GOOGLE_API_KEY\n",
    "os.environ[\"GEMINI_MODEL\"]=GEMINI_MODEL\n",
    "\n",
    "print(\"‚úî AWS:\", AWS_REGION, S3_BUCKET, S3_PREFIX)\n",
    "\n",
    "if OPENAI_API_KEY or GOOGLE_API_KEY:\n",
    "    print(\"‚úÖ API Key(s) cargada(s) correctamente.\")\n",
    "    print(\"‚úî LLM provider:\", LLM_PROVIDER, \"| OpenAI model:\", OPENAI_MODEL, \"| Gemini model:\", GEMINI_MODEL)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se detectaron API Keys. El chat funcionar√° en modo 'fragmento m√°s relevante'.\")\n",
    "    print(\"‚úî LLM provider: Fragmento m√°s relevante\")"
   ],
   "id": "G4nOrSx47tk8"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeH2Jtmx7tlA"
   },
   "source": [
    "## 2) S3 helpers (autoregi√≥n, listar carpetas/archivos, sync, prefijo efectivo)"
   ],
   "id": "BeH2Jtmx7tlA"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xLVWVFv7tlC",
    "outputId": "dbef457c-fb2e-4846-f432-7b4e451f6d89",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295199767,
     "user_tz": 300,
     "elapsed": 14,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úî Helpers S3 OK\n"
     ]
    }
   ],
   "source": [
    "import os, boto3, datetime, re, json, time, threading, requests\n",
    "from pathlib import Path\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def norm_prefix(p: str) -> str:\n",
    "    if p is None: return \"\"\n",
    "    p = p.strip().replace(\"\\\\\",\"/\")\n",
    "    p = p.lstrip(\"/\")\n",
    "    if p and not p.endswith(\"/\"):\n",
    "        p += \"/\"\n",
    "    return p\n",
    "\n",
    "def get_bucket_region(bucket: str) -> str:\n",
    "    s3g = boto3.client(\"s3\")\n",
    "    loc = s3g.get_bucket_location(Bucket=bucket).get(\"LocationConstraint\")\n",
    "    return \"us-east-1\" if loc in (None, \"EU\") else loc\n",
    "\n",
    "def s3_client_autoregion(bucket: str):\n",
    "    try:\n",
    "        region = get_bucket_region(bucket)\n",
    "    except Exception:\n",
    "        region = os.getenv(\"AWS_REGION\", \"us-east-2\")\n",
    "    return boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "def s3_list_objects(bucket: str, prefix: str, delimiter: str=None):\n",
    "    s3 = s3_client_autoregion(bucket)\n",
    "    kwargs = {\"Bucket\": bucket, \"Prefix\": norm_prefix(prefix)}\n",
    "    if delimiter: kwargs[\"Delimiter\"] = delimiter\n",
    "    keys = []\n",
    "    folders = []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(**kwargs):\n",
    "        for obj in page.get(\"Contents\", []) or []:\n",
    "            keys.append(obj[\"Key\"])\n",
    "        for cp in page.get(\"CommonPrefixes\", []) or []:\n",
    "            folders.append(cp[\"Prefix\"])\n",
    "    return keys, folders\n",
    "\n",
    "def s3_list_immediate_folders(bucket: str, base_prefix: str):\n",
    "    _, folders = s3_list_objects(bucket, norm_prefix(base_prefix), delimiter=\"/\")\n",
    "    return sorted({ f.split(\"/\")[-2] for f in folders }) if folders else []\n",
    "\n",
    "def s3_sync_docs_to_local(bucket: str, prefix_docs: str, local_folder: str):\n",
    "    s3 = s3_client_autoregion(bucket)\n",
    "    prefix_docs = norm_prefix(prefix_docs)\n",
    "    Path(local_folder).mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix_docs):\n",
    "        for obj in page.get(\"Contents\", []) or []:\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\"/\"): continue\n",
    "            rel = key[len(prefix_docs):]\n",
    "            out = Path(local_folder)/rel\n",
    "            out.parent.mkdir(parents=True, exist_ok=True)\n",
    "            s3.download_file(bucket, key, str(out))\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def s3_sync_local_docs_to_s3(bucket: str, prefix_docs: str, local_folder: str):\n",
    "    s3 = s3_client_autoregion(bucket)\n",
    "    prefix_docs = norm_prefix(prefix_docs)\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(local_folder):\n",
    "        for name in files:\n",
    "            full = Path(root)/name\n",
    "            rel = Path(full).relative_to(local_folder).as_posix()\n",
    "            key = prefix_docs + rel\n",
    "            s3.upload_file(str(full), bucket, key)\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "print(\"‚úî Helpers S3 OK\")"
   ],
   "id": "1xLVWVFv7tlC"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFPXZY_T7tlE"
   },
   "source": [
    "## 3) N√∫cleo RAG (loaders ‚Üí chunking ‚Üí FAISS ‚Üí retrieval)"
   ],
   "id": "JFPXZY_T7tlE"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMhbAOdH7tlG",
    "outputId": "32a06d9b-2a24-4ebf-ee5d-576c35d92d10",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295239640,
     "user_tz": 300,
     "elapsed": 39870,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úî N√∫cleo RAG OK\n"
     ]
    }
   ],
   "source": [
    "import os, json, uuid, shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, faiss, time\n",
    "\n",
    "BASE_DIR = Path.cwd() / \"mini_chatbot_work\"\n",
    "DOCS_DIR = BASE_DIR / \"docs_raw\"\n",
    "INDEX_DIR= BASE_DIR / \"faiss_index\"\n",
    "LOGS_DIR = BASE_DIR / \"logs\"\n",
    "META_PATH= BASE_DIR / \"docs_metadata.json\"\n",
    "for p in [BASE_DIR, DOCS_DIR, INDEX_DIR, LOGS_DIR]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_txt(p: Path)->str:\n",
    "    try:\n",
    "        return p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR TXT] {e}\"\n",
    "\n",
    "def load_pdf(p: Path)->str:\n",
    "    try:\n",
    "        r=PdfReader(str(p))\n",
    "        return \"\\n\".join([(pg.extract_text() or \"\") for pg in r.pages])\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR PDF] {e}\"\n",
    "\n",
    "def load_csv(p: Path, n=1500)->str:\n",
    "    try: df=pd.read_csv(p, nrows=n)\n",
    "    except UnicodeDecodeError: df=pd.read_csv(p, nrows=n, encoding=\"latin-1\")\n",
    "    return \"\\n\".join([f\"# CSV: {p.name}\", f\"Columnas: {list(df.columns)}\", \"Muestra:\\n\"+df.head(20).to_markdown(index=False)])\n",
    "\n",
    "def load_any(p: Path)->str:\n",
    "    ext=p.suffix.lower()\n",
    "    if ext in [\".txt\",\".md\"]: return load_txt(p)\n",
    "    if ext==\".pdf\": return load_pdf(p)\n",
    "    if ext==\".csv\": return load_csv(p)\n",
    "    return f\"[BINARIO] {p.name} (no indexado)\"\n",
    "\n",
    "@dataclass\n",
    "class ChunkedDoc:\n",
    "    doc_id: str; source_name: str; chunk_id: int; text: str\n",
    "\n",
    "def chunk_text(text:str, chunk_size:int=800, overlap:int=150)->List[str]:\n",
    "    toks=text.split(); out=[]; i=0\n",
    "    step=max(1, chunk_size-overlap)\n",
    "    while i < len(toks):\n",
    "        out.append(\" \".join(toks[i:i+chunk_size]))\n",
    "        i+=step\n",
    "    return out\n",
    "\n",
    "EMB_MODEL_NAME=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "_emb=None; _index=None; _chunks:List[ChunkedDoc]=[]\n",
    "\n",
    "def get_model():\n",
    "    global _emb\n",
    "    if _emb is None: _emb=SentenceTransformer(EMB_MODEL_NAME)\n",
    "    return _emb\n",
    "\n",
    "def build_index_from_local(paths:List[Path], chunk_size=800, overlap=150):\n",
    "    global _index, _chunks\n",
    "    metas=[]; chunks=[]\n",
    "    for p in paths:\n",
    "        raw=load_any(p); did=str(uuid.uuid4())\n",
    "        metas.append({\"doc_id\":did,\"source_name\":p.name,\"path\":str(p)})\n",
    "        for i,ch in enumerate(chunk_text(raw,chunk_size,overlap)):\n",
    "            chunks.append(ChunkedDoc(doc_id=did, source_name=p.name, chunk_id=i, text=ch))\n",
    "    _chunks=chunks\n",
    "    X=get_model().encode([c.text for c in chunks], show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
    "    idx=faiss.IndexFlatIP(X.shape[1]); idx.add(X); _index=idx\n",
    "    INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    faiss.write_index(_index, str(INDEX_DIR/\"faiss.index\"))\n",
    "    json.dump([c.__dict__ for c in chunks], open(INDEX_DIR/\"chunks.json\",\"w\",encoding=\"utf-8\"), ensure_ascii=False)\n",
    "    json.dump(metas, open(META_PATH,\"w\",encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "    return len(chunks), len(metas)\n",
    "\n",
    "def load_index_local()->bool:\n",
    "    global _index, _chunks\n",
    "    fidx=INDEX_DIR/\"faiss.index\"; fch=INDEX_DIR/\"chunks.json\"\n",
    "    if not (fidx.exists() and fch.exists()): return False\n",
    "    _index=faiss.read_index(str(fidx))\n",
    "    data=json.load(open(fch,\"r\",encoding=\"utf-8\"))\n",
    "    _chunks=[ChunkedDoc(**d) for d in data]\n",
    "    return True\n",
    "\n",
    "def retrieve(q:str, top_k:int=4)->List[Tuple[float,ChunkedDoc]]:\n",
    "    if _index is None or not _chunks: return []\n",
    "    X=get_model().encode([q], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
    "    D,I=_index.search(X, top_k)\n",
    "    out=[]\n",
    "    for s,idx in zip(D[0],I[0]):\n",
    "        if idx<0: continue\n",
    "        out.append((float(s), _chunks[idx]))\n",
    "    return out\n",
    "\n",
    "def log_event(chat_id: str, role: str, text: str):\n",
    "    p = LOGS_DIR / f\"{chat_id}.jsonl\"\n",
    "    rec = {\"t\": time.time(), \"role\": role, \"text\": text}\n",
    "    with open(p, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"‚úî N√∫cleo RAG OK\")"
   ],
   "id": "tMhbAOdH7tlG"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1FQRyyC7tlH"
   },
   "source": [
    "## 4) LLM wrapper (OpenAI o Gemini)"
   ],
   "id": "b1FQRyyC7tlH"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_--Ff4FV7tlI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295239665,
     "user_tz": 300,
     "elapsed": 5,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"Eres un asistente para preguntas y respuestas basado en archivos cargados por el usuario. \"\n",
    "    \"Responde SOLO con la informaci√≥n del contexto. \"\n",
    "    \"Si la respuesta no est√° en el contexto di: 'No encuentro esa informaci√≥n en mis archivos'. \"\n",
    "    \"Responde en espa√±ol y a√±ade una secci√≥n 'Fuentes'.\"\n",
    ")\n",
    "\n",
    "def format_context(hits):\n",
    "    lines = []\n",
    "    for score, ch in hits:\n",
    "        snippet = (ch.text[:350] + \"‚Ä¶\") if len(ch.text) > 350 else ch.text\n",
    "        lines.append(f\"[{ch.source_name} | score={score:.3f}] {snippet}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self):\n",
    "        self.provider = (os.getenv(\"LLM_PROVIDER\") or \"openai\").lower()\n",
    "        self.ok_openai = bool(os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.ok_gemini = bool(os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        self.oai_model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "        self.gem_model = os.getenv(\"GEMINI_MODEL\", \"gemini-1.5-flash\")\n",
    "        self._oai = None\n",
    "        self._gem = None\n",
    "\n",
    "    def _ensure_openai(self):\n",
    "        if not self.ok_openai: return False\n",
    "        if self._oai is None:\n",
    "            from openai import OpenAI\n",
    "            self._oai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        return True\n",
    "\n",
    "    def _ensure_gemini(self):\n",
    "        if not self.ok_gemini: return False\n",
    "        if self._gem is None:\n",
    "            import google.generativeai as genai\n",
    "            genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "            self._gem = genai.GenerativeModel(self.gem_model)\n",
    "        return True\n",
    "\n",
    "    def generate(self, question: str, hits, temperature: float=0.2, max_tokens: int=400) -> str:\n",
    "        ctx = format_context(hits)\n",
    "        if self.provider == \"openai\" and self._ensure_openai():\n",
    "            msgs = [\n",
    "                {\"role\":\"system\",\"content\": SYS_PROMPT},\n",
    "                {\"role\":\"user\",\"content\": f\"Pregunta: {question}\\n\\nContexto:\\n{ctx}\\n\\nResponde SOLO con lo anterior.\"}\n",
    "            ]\n",
    "            resp = self._oai.chat.completions.create(model=self.oai_model, messages=msgs,\n",
    "                                                     temperature=float(temperature), max_tokens=int(max_tokens))\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        if self.provider == \"gemini\" and self._ensure_gemini():\n",
    "            prompt = f\"{SYS_PROMPT}\\n\\nPregunta: {question}\\n\\nContexto:\\n{ctx}\\n\\nResponde SOLO con lo anterior.\"\n",
    "            out = self._gem.generate_content(prompt)\n",
    "            return (out.text or \"\").strip()\n",
    "        # Fallback sin LLM\n",
    "        best = hits[0][1].text if hits else \"No hay √≠ndice cargado.\"\n",
    "        return \"‚ö†Ô∏è Sin LLM: fragmento m√°s relevante:\\n\\n\" + best[:1000]\n",
    "\n",
    "LLM = LLMClient()\n",
    "\n",
    "def rag_answer(question: str, top_k:int=4, temperature:float=0.2):\n",
    "    hits = retrieve(question, top_k=top_k)\n",
    "    if not hits:\n",
    "        return \"Primero crea/carga un √≠ndice.\", []\n",
    "    txt = LLM.generate(question, hits, temperature)\n",
    "    sources = [h[1].source_name for h in hits]\n",
    "    return txt, sources"
   ],
   "id": "_--Ff4FV7tlI"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "144S4eh77tlK"
   },
   "source": [
    "## 5) Cargar/Guardar √≠ndice en S3 + reconstrucci√≥n desde docs en S3"
   ],
   "id": "144S4eh77tlK"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I-8KWQb87tlL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295239673,
     "user_tz": 300,
     "elapsed": 3,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def effective_team_prefix(base_prefix: str, team_folder: str):\n",
    "    return norm_prefix(base_prefix) + norm_prefix(team_folder)\n",
    "\n",
    "def s3_upload_index(bucket: str, base_prefix: str, team_folder: str):\n",
    "    idx_prefix_root = effective_team_prefix(base_prefix, team_folder)\n",
    "    idx_prefix_sub  = idx_prefix_root + \"index/\"\n",
    "    if not (INDEX_DIR/\"faiss.index\").exists() or not (INDEX_DIR/\"chunks.json\").exists():\n",
    "        return \"‚ùå No hay √≠ndice local (faiss.index / chunks.json). Construye primero.\"\n",
    "    s3=s3_client_autoregion(bucket)\n",
    "    for target_prefix in [idx_prefix_root, idx_prefix_sub]:\n",
    "        for name in [\"faiss.index\",\"chunks.json\"]:\n",
    "            s3.upload_file(str(INDEX_DIR/name), bucket, f\"{target_prefix}{name}\")\n",
    "        if META_PATH.exists():\n",
    "            s3.upload_file(str(META_PATH), bucket, f\"{target_prefix}docs_metadata.json\")\n",
    "    return f\"‚òÅÔ∏è Subido a: s3://{bucket}/{idx_prefix_root}  y  s3://{bucket}/{idx_prefix_sub}\"\n",
    "\n",
    "def _download_index_from_prefix(bucket: str, prefix: str):\n",
    "    s3=s3_client_autoregion(bucket)\n",
    "    found=set()\n",
    "    want=[\"faiss.index\",\"chunks.json\",\"docs_metadata.json\"]\n",
    "    keys,_ = s3_list_objects(bucket, prefix)\n",
    "    for key in keys:\n",
    "        base = key.split(\"/\")[-1]\n",
    "        if base in want:\n",
    "            out = INDEX_DIR / base if base!=\"docs_metadata.json\" else META_PATH\n",
    "            out.parent.mkdir(parents=True, exist_ok=True)\n",
    "            s3.download_file(bucket, key, str(out))\n",
    "            found.add(base)\n",
    "    return found\n",
    "\n",
    "def s3_download_index(bucket: str, base_prefix: str, team_folder: str):\n",
    "    idx_prefix_root = effective_team_prefix(base_prefix, team_folder)\n",
    "    idx_prefix_sub  = idx_prefix_root + \"index/\"\n",
    "    found = _download_index_from_prefix(bucket, idx_prefix_root)\n",
    "    if not {\"faiss.index\",\"chunks.json\"}.issubset(found):\n",
    "        found = _download_index_from_prefix(bucket, idx_prefix_sub)\n",
    "    if {\"faiss.index\",\"chunks.json\"}.issubset(found):\n",
    "        ok = load_index_local()\n",
    "        return \"üì• √çndice cargado.\" if ok else \"‚ùå Descargado pero fall√≥ carga local.\"\n",
    "    else:\n",
    "        return f\"‚ùå No encontr√© √≠ndice en {idx_prefix_root} ni {idx_prefix_sub}.\"\n",
    "\n",
    "def s3_rebuild_from_docs(bucket: str, base_prefix: str, team_folder: str, chunk_size=800, overlap=150):\n",
    "    docs_prefix = effective_team_prefix(base_prefix, team_folder) + \"docs/\"\n",
    "    count = s3_sync_docs_to_local(bucket, docs_prefix, str(DOCS_DIR))\n",
    "    if count == 0:\n",
    "        return \"‚ùå No hay documentos en S3 (carpeta 'docs/'). Sube alguno primero.\"\n",
    "    n_chunks, n_docs = build_index_from_local(list(Path(DOCS_DIR).glob('*')), chunk_size, overlap)\n",
    "    return f\"‚úÖ Reconstruido desde S3: {n_docs} docs ‚Üí {n_chunks} chunks.\"\n",
    "\n",
    "def s3_upload_local_docs(bucket: str, base_prefix: str, team_folder: str):\n",
    "    docs_prefix = effective_team_prefix(base_prefix, team_folder) + \"docs/\"\n",
    "    count = s3_sync_local_docs_to_s3(bucket, docs_prefix, str(DOCS_DIR))\n",
    "    return f\"‚òÅÔ∏è Subidos {count} archivo(s) a s3://{bucket}/{docs_prefix}\"\n",
    "\n",
    "def s3_download_docs(bucket: str, base_prefix: str, team_folder: str):\n",
    "    docs_prefix = effective_team_prefix(base_prefix, team_folder) + \"docs/\"\n",
    "    count = s3_sync_docs_to_local(bucket, docs_prefix, str(DOCS_DIR))\n",
    "    return f\"üì• Descargados {count} archivo(s) a {DOCS_DIR}\""
   ],
   "id": "I-8KWQb87tlL"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt8bfMkl7tlM"
   },
   "source": [
    "## 6) Telegram: bot con polling, auto-responder y panel admin"
   ],
   "id": "tt8bfMkl7tlM"
  },
  {
   "cell_type": "code",
   "source": [
    "#import os\n",
    "#os.environ[\"TELEGRAM_BOT_TOKEN\"] = \"7526191718:AAEM1HYYoKmXjmw-VkTI0kzI0bTiKBvT4i0\"\n",
    "#TELEGRAM_BOT_TOKEN = os.environ[\"TELEGRAM_BOT_TOKEN\"]"
   ],
   "metadata": {
    "id": "fdMAXAH8fREZ"
   },
   "id": "fdMAXAH8fREZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jn48XVDL7tlM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295239708,
     "user_tz": 300,
     "elapsed": 5,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# === LEGACY TELEGRAM MANAGER DISABLED: preserved for reference ===\n",
    "if False:\n",
    "    STATE={\n",
    "      \"bucket\": os.getenv(\"S3_BUCKET\"),\n",
    "      \"base_prefix\": os.getenv(\"S3_PREFIX\"),\n",
    "      \"team_folder\": \"\",\n",
    "      \"tg\": {\"token\":\"\", \"bot\":\"\", \"chat_id\":\"\", \"poll\":False, \"offset\":None, \"auto\":False},\n",
    "      \"chats\": {}  # chat_id -> {\"title\": str, \"last_text\": str}\n",
    "    }\n",
    "\n",
    "    def tg_get_me(token:str):\n",
    "        r=requests.get(f\"https://api.telegram.org/bot{token}/getMe\", timeout=10); r.raise_for_status(); return r.json()\n",
    "\n",
    "    def tg_get_updates(token:str, offset=None, timeout=20):\n",
    "        params = {\"timeout\": timeout}\n",
    "        if offset is not None: params[\"offset\"] = offset\n",
    "        r=requests.get(f\"https://api.telegram.org/bot{token}/getUpdates\", params=params, timeout=timeout+5)\n",
    "        r.raise_for_status(); return r.json()\n",
    "\n",
    "    def tg_send_message(token:str, chat_id:str, text:str):\n",
    "        r=requests.post(f\"https://api.telegram.org/bot{token}/sendMessage\",\n",
    "                        json={\"chat_id\": chat_id, \"text\": text}, timeout=10)\n",
    "        r.raise_for_status(); return r.json()\n",
    "\n",
    "    def apply_route(bucket, base_prefix, team_folder):\n",
    "        STATE[\"bucket\"] = (bucket or \"\").strip() or os.getenv(\"S3_BUCKET\")\n",
    "        STATE[\"base_prefix\"] = norm_prefix(base_prefix or os.getenv(\"S3_PREFIX\") or \"\")\n",
    "        STATE[\"team_folder\"] = (team_folder or \"\").strip()\n",
    "        # ensure subfolders\n",
    "        eff_root = effective_team_prefix(STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "        s3 = s3_client_autoregion(STATE[\"bucket\"])\n",
    "        s3.put_object(Bucket=STATE[\"bucket\"], Key=eff_root+\"docs/\")\n",
    "        s3.put_object(Bucket=STATE[\"bucket\"], Key=eff_root+\"index/\")\n",
    "        return f\"‚úî Ruta: s3://{STATE['bucket']}/{eff_root}\"\n",
    "\n",
    "    def set_token(token):\n",
    "        STATE[\"tg\"][\"token\"]=token.strip()\n",
    "        me = tg_get_me(STATE[\"tg\"][\"token\"])\n",
    "        STATE[\"tg\"][\"bot\"]= \"@\"+(me.get(\"result\",{}).get(\"username\",\"\"))\n",
    "        return f\"Bot: {STATE['tg']['bot']}\"\n",
    "\n",
    "    def get_chat_id():\n",
    "        upd = tg_get_updates(STATE[\"tg\"][\"token\"], timeout=5)\n",
    "        res = upd.get(\"result\", [])\n",
    "        last = res[-1] if res else None\n",
    "        if not last: return \"Env√≠a /start al bot y reintenta.\"\n",
    "        chat = ((last.get(\"message\") or {}).get(\"chat\")) or {}\n",
    "        if \"id\" in chat:\n",
    "            STATE[\"tg\"][\"chat_id\"] = str(chat[\"id\"])\n",
    "            title = chat.get(\"username\") or chat.get(\"title\") or \"\"\n",
    "            STATE[\"chats\"][STATE[\"tg\"][\"chat_id\"]] = {\"title\": title, \"last_text\": \"\"}\n",
    "            return f\"chat_id={STATE['tg']['chat_id']} ({title})\"\n",
    "        return \"No se encontr√≥ chat_id.\"\n",
    "\n",
    "    def list_chats():\n",
    "        if not STATE[\"chats\"]:\n",
    "            return \"(sin chats a√∫n ‚Äî cuando lleguen mensajes se listar√°n aqu√≠)\"\n",
    "        lines=[]\n",
    "        for cid, meta in STATE[\"chats\"].items():\n",
    "            lines.append(f\"{cid} | {meta.get('title','')} | {meta.get('last_text','')[:60]}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def admin_send(cid: str, text: str):\n",
    "        if not cid: return \"Elige un chat_id.\"\n",
    "        try:\n",
    "            tg_send_message(STATE[\"tg\"][\"token\"], str(cid), text or \"(mensaje vac√≠o)\")\n",
    "            log_event(str(cid), \"admin\", text or \"\")\n",
    "            return \"‚úÖ Enviado\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error: {e}\"\n",
    "\n",
    "    def _handle_incoming(msg):\n",
    "        chat = msg.get(\"message\", {}).get(\"chat\", {}) or {}\n",
    "        cid = str(chat.get(\"id\"))\n",
    "        text = (msg.get(\"message\", {}) or {}).get(\"text\", \"\")\n",
    "        title = chat.get(\"username\") or chat.get(\"title\") or \"\"\n",
    "        if not cid or not text: return\n",
    "        STATE[\"chats\"].setdefault(cid, {\"title\": title, \"last_text\": \"\"})\n",
    "        STATE[\"chats\"][cid][\"last_text\"] = text\n",
    "        log_event(cid, \"user\", text)\n",
    "        if STATE[\"tg\"][\"auto\"]:\n",
    "            if not load_index_local():\n",
    "                s3_download_index(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "            ans, _src = rag_answer(text, top_k=4, temperature=0.2)\n",
    "            try:\n",
    "                tg_send_message(STATE[\"tg\"][\"token\"], cid, ans)\n",
    "                log_event(cid, \"bot\", ans)\n",
    "            except Exception as e:\n",
    "                log_event(cid, \"error\", f\"send fail: {e}\")\n",
    "\n",
    "    def start_polling():\n",
    "        STATE[\"tg\"][\"poll\"]=True\n",
    "        STATE[\"tg\"][\"offset\"]=None\n",
    "\n",
    "    def stop_polling():\n",
    "        STATE[\"tg\"][\"poll\"]=False\n",
    "\n",
    "    def set_auto(flag: bool):\n",
    "        STATE[\"tg\"][\"auto\"]=bool(flag)\n",
    "        return f\"Auto-responder: {'ON' if STATE['tg']['auto'] else 'OFF'}\"\n",
    "\n",
    "    def poll_once():\n",
    "        if not STATE[\"tg\"][\"poll\"]: return \"Polling OFF\"\n",
    "        try:\n",
    "            upd = tg_get_updates(STATE[\"tg\"][\"token\"], offset=STATE[\"tg\"][\"offset\"], timeout=10)\n",
    "            for item in upd.get(\"result\", []):\n",
    "                STATE[\"tg\"][\"offset\"] = item[\"update_id\"] + 1\n",
    "                _handle_incoming(item)\n",
    "            return f\"OK, updates: {len(upd.get('result', []))}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Poll error: {e}\""
   ],
   "id": "Jn48XVDL7tlM"
  },
  {
   "cell_type": "code",
   "source": [
    "# === TALENTO TECH ‚Äî TELEGRAM v1++ (auto + alerta ‚ÄúAsesor/Ayuda‚Äù + intervenci√≥n) ===\n",
    "# - Respeta TELEGRAM_BOT_TOKEN de env/userdata o campo de la pesta√±a Admin\n",
    "# - Polling por hilo (no bloquea la UI) y deleteWebhook\n",
    "# - Auto-responder estilo v1: si hay responder() la usa; sino rag_answer(); sino retrieve+LLM; sino fragmento\n",
    "# - Al detectar ‚Äúasesor/ayuda‚Äù:\n",
    "#     * contesta al usuario\n",
    "#     * avisa al ADMIN (si est√° configurado)\n",
    "#     * pone \"hold\" (pausa auto) en ese chat por 120s (configurable)\n",
    "# - Al enviar como admin a un chat, se pone \"hold\" 60s para no pisarse con el bot\n",
    "# - Comandos desde el chat ADMIN:\n",
    "#     /admin                         -> marca ese chat como admin\n",
    "#     /auto_on <chat_id>             -> habilita auto en ese chat\n",
    "#     /auto_off <chat_id>            -> deshabilita auto en ese chat\n",
    "#     /say <chat_id> <mensaje...>    -> env√≠a mensaje al usuario (y activa hold 60s)\n",
    "#\n",
    "# Reexpone: ui_set_token, ui_poll_toggle, ui_poll_once, ui_list_chats, ui_admin_send, ui_auto_toggle\n",
    "\n",
    "import os, re, json, time, threading, requests\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------- Helpers LLM (no invasivo) ----------------------------\n",
    "def _tt_openai():\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        return OpenAI(api_key=key) if key else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _tt_gemini():\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not key: return None\n",
    "        genai.configure(api_key=key)\n",
    "        return genai.GenerativeModel(os.getenv(\"GEMINI_MODEL\",\"gemini-1.5-flash\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _tt_llm_answer(q, ctx):\n",
    "    prov = (os.getenv(\"LLM_PROVIDER\") or \"openai\").lower()\n",
    "    temp = float(os.getenv(\"LLM_TEMPERATURE\",\"0.2\"))\n",
    "    mtok = int(os.getenv(\"LLM_MAX_TOKENS\",\"400\"))\n",
    "    sys_prompt = (\"Eres un asistente que responde SOLO con la informaci√≥n del contexto. \"\n",
    "                  \"Si no est√° en el contexto di: 'No encuentro esa informaci√≥n en mis archivos'. \"\n",
    "                  \"Responde en espa√±ol y a√±ade 'Fuentes'.\")\n",
    "\n",
    "    user = f\"Pregunta: {q}\\n\\nContexto:\\n{ctx}\\n\\nResponde SOLO con lo anterior.\"\n",
    "    if prov == \"openai\":\n",
    "        cli = _tt_openai()\n",
    "        if not cli: return \"‚ö†Ô∏è Sin LLM (OPENAI_API_KEY no configurada)\"\n",
    "        try:\n",
    "            msgs=[{\"role\":\"system\",\"content\":sys_prompt},{\"role\":\"user\",\"content\":user}]\n",
    "            out=cli.chat.completions.create(model=os.getenv(\"OPENAI_MODEL\",\"gpt-4o-mini\"),\n",
    "                                            messages=msgs, temperature=temp, max_tokens=mtok)\n",
    "            return out.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"‚ö†Ô∏è Error OpenAI: {e}\"\n",
    "    if prov == \"gemini\":\n",
    "        model = _tt_gemini()\n",
    "        if not model: return \"‚ö†Ô∏è Sin LLM (GOOGLE_API_KEY/GEMINI_API_KEY no configurada)\"\n",
    "        try:\n",
    "            out = model.generate_content(f\"{sys_prompt}\\n\\n{user}\")\n",
    "            return (getattr(out,\"text\",\"\") or \"\").strip()\n",
    "        except Exception as e:\n",
    "            return f\"‚ö†Ô∏è Error Gemini: {e}\"\n",
    "    return \"‚ö†Ô∏è Sin LLM: muestra el fragmento m√°s relevante.\"\n",
    "\n",
    "def _tt_build_context(q):\n",
    "    try:\n",
    "        if 'retrieve' in globals():\n",
    "            hits = retrieve(q, top_k=4)\n",
    "            parts=[]\n",
    "            for score, ch in hits:\n",
    "                snip=(ch.text[:350]+\"‚Ä¶\") if len(ch.text)>350 else ch.text\n",
    "                src=getattr(ch,'source_name','doc')\n",
    "                parts.append(f\"[{src} | {score:.3f}] {snip}\")\n",
    "            return \"\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def _tt_answer_logic(q):\n",
    "    q = (q or \"\").strip()\n",
    "    if not q:\n",
    "        return \"Env√≠ame un texto y responder√© con lo que haya en tus archivos.\"\n",
    "    # 1) Tu responder()\n",
    "    if 'responder' in globals():\n",
    "        try: return responder(q)\n",
    "        except Exception as e: return f\"‚ö†Ô∏è Error en responder(): {e}\"\n",
    "    # 2) Tu rag_answer()\n",
    "    if 'rag_answer' in globals():\n",
    "        try:\n",
    "            ans,_ = rag_answer(q, top_k=4, temperature=float(os.getenv(\"LLM_TEMPERATURE\",\"0.2\")))\n",
    "            return ans\n",
    "        except Exception as e:\n",
    "            return f\"‚ö†Ô∏è Error en rag_answer(): {e}\"\n",
    "    # 3) retrieve + LLM\n",
    "    ctx = _tt_build_context(q) or \"(sin contexto)\"\n",
    "    return _tt_llm_answer(q, ctx)\n",
    "\n",
    "# ---------------------------- Telegram Manager ----------------------------------\n",
    "class TTGram:\n",
    "    HOLD_AFTER_ADMIN = 60      # s de pausa auto tras intervenci√≥n admin\n",
    "    HOLD_AFTER_ALERT = 120     # s de pausa auto tras ‚Äúasesor/ayuda‚Äù\n",
    "    ALERT_REGEX = re.compile(r\"\\b(asesor|ayuda)\\b\", re.I)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.token = (\n",
    "            os.getenv(\"TELEGRAM_BOT_TOKEN\") or\n",
    "            (userdata.get(\"TELEGRAM_BOT_TOKEN\") if 'userdata' in globals() else None) or\n",
    "            \"\"\n",
    "        ).strip()\n",
    "        self.offset = None\n",
    "        self.global_auto = True\n",
    "        self.stop = threading.Event()\n",
    "        self.thread = None\n",
    "        self.known = {}          # chat_id -> {'title','last_text','auto'(opt), 'hold_until'(opt)}\n",
    "        self.admin_chat_id = (os.getenv(\"ADMIN_CHAT_ID\") or \"\").strip()\n",
    "\n",
    "        self.logs = Path.cwd()/ \"mini_chatbot_work\" / \"logs\"\n",
    "        self.logs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---------- HTTP ----------\n",
    "    def _delete_webhook(self):\n",
    "        if not self.token: return\n",
    "        try: requests.get(f\"https://api.telegram.org/bot{self.token}/deleteWebhook\", timeout=10)\n",
    "        except Exception: pass\n",
    "\n",
    "    def _me(self):\n",
    "        if not self.token: return \"No TELEGRAM_BOT_TOKEN configurado.\"\n",
    "        try:\n",
    "            r=requests.get(f\"https://api.telegram.org/bot{self.token}/getMe\", timeout=10)\n",
    "            r.raise_for_status(); data=r.json()\n",
    "            return \"Bot: @\" + (data.get(\"result\",{}).get(\"username\") or \"(sin username)\")\n",
    "        except Exception as e:\n",
    "            return f\"Token/Bot error: {e}\"\n",
    "\n",
    "    def _send(self, chat_id, text):\n",
    "        if not self.token: return \"Sin token\"\n",
    "        try:\n",
    "            r=requests.post(f\"https://api.telegram.org/bot{self.token}/sendMessage\",\n",
    "                            json={\"chat_id\": str(chat_id), \"text\": text}, timeout=10)\n",
    "            r.raise_for_status(); return \"‚úÖ Enviado\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error: {e}\"\n",
    "\n",
    "    # ---------- Utils ----------\n",
    "    def _log(self, cid, role, text):\n",
    "        try:\n",
    "            p=self.logs/f\"{cid}.jsonl\"\n",
    "            rec={\"t\": time.time(), \"role\": role, \"text\": text}\n",
    "            with open(p,\"a\",encoding=\"utf-8\") as f: f.write(json.dumps(rec,ensure_ascii=False)+\"\\n\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _should_auto(self, cid):\n",
    "        meta = self.known.get(cid, {})\n",
    "        # hold por tiempo\n",
    "        if time.time() < float(meta.get(\"hold_until\", 0)): return False\n",
    "        # override por chat o global\n",
    "        if \"auto\" in meta: return bool(meta[\"auto\"])\n",
    "        return self.global_auto\n",
    "\n",
    "    def _set_hold(self, cid, seconds):\n",
    "        meta = self.known.setdefault(cid, {})\n",
    "        meta[\"hold_until\"] = time.time() + float(seconds)\n",
    "\n",
    "    def _set_auto_chat(self, cid, flag: bool):\n",
    "        self.known.setdefault(cid, {})[\"auto\"] = bool(flag)\n",
    "\n",
    "    # ---------- Core ----------\n",
    "    def handle_update(self, item):\n",
    "        msg=item.get(\"message\") or {}\n",
    "        chat=msg.get(\"chat\") or {}\n",
    "        cid=str(chat.get(\"id\"))\n",
    "        txt=(msg.get(\"text\") or \"\").strip()\n",
    "        title=chat.get(\"username\") or chat.get(\"title\") or \"\"\n",
    "        if not cid or not txt: return\n",
    "\n",
    "        self.known.setdefault(cid, {\"title\": title, \"last_text\": \"\"})\n",
    "        self.known[cid][\"last_text\"] = txt\n",
    "        self._log(cid,\"user\",txt)\n",
    "\n",
    "        # Comandos admin desde su propio chat\n",
    "        if cid == self.admin_chat_id:\n",
    "            if txt == \"/admin\":\n",
    "                self._send(cid, \"Ya eres canal admin ‚úÖ\")\n",
    "                return\n",
    "            if txt.startswith(\"/auto_on\"):\n",
    "                parts = txt.split()\n",
    "                if len(parts)>=2:\n",
    "                    self._set_auto_chat(parts[1], True)\n",
    "                    self._send(cid, f\"Auto ON para {parts[1]}\")\n",
    "                return\n",
    "            if txt.startswith(\"/auto_off\"):\n",
    "                parts = txt.split()\n",
    "                if len(parts)>=2:\n",
    "                    self._set_auto_chat(parts[1], False)\n",
    "                    self._send(cid, f\"Auto OFF para {parts[1]}\")\n",
    "                return\n",
    "            if txt.startswith(\"/say\"):\n",
    "                # /say <chat_id> <mensaje...>\n",
    "                parts = txt.split(maxsplit=2)\n",
    "                if len(parts)>=3:\n",
    "                    tgt, m = parts[1], parts[2]\n",
    "                    self._send(tgt, m)\n",
    "                    self._log(tgt, \"admin\", m)\n",
    "                    self._set_hold(tgt, self.HOLD_AFTER_ADMIN)\n",
    "                return\n",
    "\n",
    "        # Si alguien dice ‚Äúadmin‚Äù en privado -> tomar ese chat como admin\n",
    "        if txt == \"/admin\" and not self.admin_chat_id:\n",
    "            self.admin_chat_id = cid\n",
    "            os.environ[\"ADMIN_CHAT_ID\"] = cid\n",
    "            self._send(cid, \"Este chat queda configurado como canal admin ‚úÖ\")\n",
    "            return\n",
    "\n",
    "        # Alerta por ‚Äúasesor/ayuda‚Äù\n",
    "        if self.ALERT_REGEX.search(txt):\n",
    "            # Responde y deja en hold para que intervenga humano\n",
    "            ans = _tt_answer_logic(txt)\n",
    "            self._send(cid, ans); self._log(cid,\"bot\",ans)\n",
    "            self._set_hold(cid, self.HOLD_AFTER_ALERT)\n",
    "            if self.admin_chat_id:\n",
    "                self._send(self.admin_chat_id,\n",
    "                           f\"‚ö†Ô∏è ALERTA: {cid} pidi√≥ ayuda (‚Äú{txt}‚Äù).\\n\"\n",
    "                           f\"Usa /say {cid} <mensaje> para intervenir, o /auto_on {cid} cuando quieras reanudar el bot.\")\n",
    "            return\n",
    "\n",
    "        # Auto-responder (estilo v1)\n",
    "        if self._should_auto(cid):\n",
    "            ans = _tt_answer_logic(txt)\n",
    "            self._send(cid, ans); self._log(cid,\"bot\",ans)\n",
    "\n",
    "    def poll_once(self):\n",
    "        if not self.token: return \"Sin TELEGRAM_BOT_TOKEN\"\n",
    "        try:\n",
    "            params={\"timeout\":10}\n",
    "            if self.offset is not None: params[\"offset\"]=self.offset\n",
    "            r=requests.get(f\"https://api.telegram.org/bot{self.token}/getUpdates\",\n",
    "                           params=params, timeout=15)\n",
    "            r.raise_for_status(); data=r.json()\n",
    "            for it in data.get(\"result\", []):\n",
    "                self.offset = it[\"update_id\"] + 1\n",
    "                self.handle_update(it)\n",
    "            return f\"OK, updates: {len(data.get('result', []))}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Poll error: {e}\"\n",
    "\n",
    "    def _loop(self):\n",
    "        while not self.stop.is_set():\n",
    "            self.poll_once()\n",
    "            for _ in range(10):\n",
    "                if self.stop.is_set(): break\n",
    "                time.sleep(0.2)\n",
    "\n",
    "    # ---------- API para la pesta√±a Admin ----------\n",
    "    def set_token(self, token: str):\n",
    "        # Allow empty token: reuse current or env/userdata\n",
    "        token = (token or self.token or os.getenv('TELEGRAM_BOT_TOKEN') or                  (userdata.get('TELEGRAM_BOT_TOKEN') if 'userdata' in globals() else '')).strip()\n",
    "        if token.upper().startswith('BOT:'):\n",
    "            token = token.split(':',1)[1].strip()\n",
    "        self.token = token\n",
    "        if token:\n",
    "            os.environ['TELEGRAM_BOT_TOKEN']=token\n",
    "            if 'userdata' in globals():\n",
    "                try:\n",
    "                    userdata['TELEGRAM_BOT_TOKEN']=token\n",
    "                except Exception:\n",
    "                    pass\n",
    "        self._delete_webhook()\n",
    "        return self._me()\n",
    "\n",
    "\n",
    "    def toggle_poll(self, flag: bool):\n",
    "        if flag:\n",
    "            if self.thread and self.thread.is_alive():\n",
    "                return \"Auto-escuchar: ya activo.\"\n",
    "            self.stop.clear()\n",
    "            self._delete_webhook()\n",
    "            self.thread = threading.Thread(target=self._loop, daemon=True)\n",
    "            self.thread.start()\n",
    "            return \"Auto-escuchar: ON\"\n",
    "        else:\n",
    "            self.stop.set()\n",
    "            return \"Auto-escuchar: OFF\"\n",
    "\n",
    "    def poll_now(self):\n",
    "        return self.poll_once()\n",
    "\n",
    "    def list_chats(self):\n",
    "        if not self.known:\n",
    "            return \"(sin chats a√∫n ‚Äî env√≠a /start al bot y pulsa 'Leer ahora')\"\n",
    "        return \"\\n\".join([f\"{cid} | {meta.get('title','')} | {meta.get('last_text','')[:60]}\"\n",
    "                          for cid,meta in self.known.items()])\n",
    "\n",
    "    def admin_send(self, cid: str, text: str):\n",
    "        out = self._send(cid, text or \"(mensaje vac√≠o)\")\n",
    "        if out.startswith(\"‚úÖ\"):\n",
    "            self._log(cid, \"admin\", text or \"\")\n",
    "            self._set_hold(cid, self.HOLD_AFTER_ADMIN)\n",
    "        return out\n",
    "\n",
    "    def auto_toggle(self, flag: bool):\n",
    "        self.global_auto = bool(flag)\n",
    "        return f\"Auto-responder (global): {'ON' if self.global_auto else 'OFF'}\"\n",
    "\n",
    "_TTG = TTGram()\n",
    "\n",
    "# Rebind para tu UI (mantiene nombres que ya usas)\n",
    "globals()[\"ui_set_token\"]   = _TTG.set_token\n",
    "globals()[\"ui_poll_toggle\"] = _TTG.toggle_poll\n",
    "globals()[\"ui_poll_once\"]   = _TTG.poll_now\n",
    "globals()[\"ui_list_chats\"]  = _TTG.list_chats\n",
    "globals()[\"ui_admin_send\"]  = _TTG.admin_send\n",
    "globals()[\"ui_auto_toggle\"] = _TTG.auto_toggle\n",
    "\n",
    "print(\"üîê TELEGRAM_BOT_TOKEN len =\", len((_TTG.token or \"\")), \"|\", _TTG._me())\n",
    "print(\"Telegram v1++ listo: auto, alerta 'Asesor/Ayuda', y control de intervenci√≥n.\")\n",
    "# ================================================================================\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kH3shO4KYZqH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295243979,
     "user_tz": 300,
     "elapsed": 4267,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    },
    "outputId": "adb0bb77-6ac4-4263-f72a-b5bcd5ad4227"
   },
   "id": "kH3shO4KYZqH",
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üîê TELEGRAM_BOT_TOKEN len = 46 | Bot: @bootcamps_explorador_bot\n",
      "Telegram v1++ listo: auto, alerta 'Asesor/Ayuda', y control de intervenci√≥n.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9tg4sKB7tlO"
   },
   "source": [
    "## 7) UI Gradio ‚Äî S3/equipo, Docs, √çndice, Persistencia, Preguntas, Telegram (Admin)"
   ],
   "id": "u9tg4sKB7tlO"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nSb_o1qQjkvj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295291710,
     "user_tz": 300,
     "elapsed": 70,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    }
   },
   "outputs": [],
   "execution_count": 9,
   "source": [
    "# === Restored S3 helpers & STATE (clean implementation) ===\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Carpeta local donde se guardan docs\n",
    "DOCS_DIR = Path('docs')\n",
    "DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Estado global de la ruta activa\n",
    "STATE = {\n",
    "    'bucket': '',\n",
    "    'base_prefix': '',  # Debe terminar en '/' o ser ''\n",
    "    'team_folder': '',\n",
    "}\n",
    "\n",
    "def norm_prefix(p: str) -> str:\n",
    "    p = (p or '').strip()\n",
    "    if p and not p.endswith('/'):\n",
    "        p += '/'\n",
    "    return p\n",
    "\n",
    "def effective_team_prefix(base_prefix: str, team_folder: str) -> str:\n",
    "    bp = norm_prefix(base_prefix)\n",
    "    tf = (team_folder or '').strip()\n",
    "    return f\"{bp}{tf}/\" if tf else bp\n",
    "\n",
    "def s3_client_autoregion(bucket: str):\n",
    "    try:\n",
    "        import boto3\n",
    "        sess = boto3.session.Session()\n",
    "        s3 = sess.client('s3')\n",
    "        try:\n",
    "            loc = s3.get_bucket_location(Bucket=bucket).get('LocationConstraint') or 'us-east-1'\n",
    "            s3 = sess.client('s3', region_name=loc)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return s3\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"boto3 requerido para S3: {e}\")\n",
    "\n",
    "def apply_route(bucket, base_prefix, team_folder):\n",
    "    \"\"\"Configura la ruta activa y asegura las subcarpetas docs/ e index/ en S3.\"\"\"\n",
    "    STATE['bucket'] = (bucket or '').strip() or os.getenv('S3_BUCKET','')\n",
    "    STATE['base_prefix'] = norm_prefix(base_prefix or os.getenv('S3_PREFIX',''))\n",
    "    STATE['team_folder'] = (team_folder or '').strip()\n",
    "\n",
    "    eff_root = effective_team_prefix(STATE['base_prefix'], STATE['team_folder'])\n",
    "    if STATE['bucket']:\n",
    "        try:\n",
    "            s3 = s3_client_autoregion(STATE['bucket'])\n",
    "            # Crear 'carpetas' l√≥gicas\n",
    "            for sub in ('docs/', 'index/'):\n",
    "                key = eff_root + sub\n",
    "                s3.put_object(Bucket=STATE['bucket'], Key=key, Body=b'')\n",
    "        except Exception:\n",
    "            # Si no hay credenciales/permiso, seguimos pero devolvemos la ruta igual\n",
    "            pass\n",
    "\n",
    "    return f\"‚úî Ruta: s3://{STATE['bucket']}/{eff_root}\"\n",
    "\n",
    "# ---- Utilidades S3 adicionales usadas por la UI (solo se definen si faltan) ----\n",
    "def _ensure_defs():\n",
    "    globals_ = globals()\n",
    "\n",
    "    if 's3_list_immediate_folders' not in globals_:\n",
    "        def s3_list_immediate_folders(bucket: str, base_prefix: str):\n",
    "            \"\"\"Lista subcarpetas inmediatas bajo base_prefix usando Delimiter='/'.\"\"\"\n",
    "            try:\n",
    "                s3 = s3_client_autoregion(bucket)\n",
    "                base_prefix = norm_prefix(base_prefix)\n",
    "                resp = s3.list_objects_v2(Bucket=bucket, Prefix=base_prefix, Delimiter='/')\n",
    "                return [cp['Prefix'][len(base_prefix):-1] for cp in resp.get('CommonPrefixes', [])]\n",
    "            except Exception:\n",
    "                return []\n",
    "        globals_['s3_list_immediate_folders'] = s3_list_immediate_folders\n",
    "\n",
    "    if 's3_upload_local_docs' not in globals_:\n",
    "        def s3_upload_local_docs(bucket=None, base_prefix=None, team_folder=None):\n",
    "            bucket = (bucket or STATE['bucket']).strip()\n",
    "            base_prefix = norm_prefix(base_prefix or STATE['base_prefix'])\n",
    "            team_folder = (team_folder or STATE['team_folder']).strip()\n",
    "            eff_root = effective_team_prefix(base_prefix, team_folder) + 'docs/'\n",
    "            try:\n",
    "                s3 = s3_client_autoregion(bucket)\n",
    "                n=0\n",
    "                for p in DOCS_DIR.glob('*'):\n",
    "                    if p.is_file():\n",
    "                        key = eff_root + p.name\n",
    "                        s3.upload_file(str(p), bucket, key)\n",
    "                        n+=1\n",
    "                return f\"Subidos {n} archivos a s3://{bucket}/{eff_root}\"\n",
    "            except Exception as e:\n",
    "                return f\"‚ö†Ô∏è No se pudo subir a S3: {e}\"\n",
    "        globals_['s3_upload_local_docs'] = s3_upload_local_docs\n",
    "\n",
    "    if 's3_download_docs' not in globals_:\n",
    "        def s3_download_docs(bucket=None, base_prefix=None, team_folder=None):\n",
    "            bucket = (bucket or STATE['bucket']).strip()\n",
    "            base_prefix = norm_prefix(base_prefix or STATE['base_prefix'])\n",
    "            team_folder = (team_folder or STATE['team_folder']).strip()\n",
    "            eff_root = effective_team_prefix(base_prefix, team_folder) + 'docs/'\n",
    "            try:\n",
    "                s3 = s3_client_autoregion(bucket)\n",
    "                DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "                paginator = s3.get_paginator('list_objects_v2')\n",
    "                n=0\n",
    "                for page in paginator.paginate(Bucket=bucket, Prefix=eff_root):\n",
    "                    for obj in page.get('Contents', []):\n",
    "                        key = obj['Key']\n",
    "                        name = key.split('/')[-1]\n",
    "                        if not name: continue\n",
    "                        s3.download_file(bucket, key, str(DOCS_DIR/name))\n",
    "                        n+=1\n",
    "                return f\"Descargados {n} archivos de s3://{bucket}/{eff_root}\"\n",
    "            except Exception as e:\n",
    "                return f\"‚ö†Ô∏è No se pudo descargar de S3: {e}\"\n",
    "        globals_['s3_download_docs'] = s3_download_docs\n",
    "\n",
    "    if 'load_index_local' not in globals_:\n",
    "        def load_index_local():\n",
    "            \"\"\"Stub m√≠nimo: en tu notebook original se carga el √≠ndice aqu√≠. Devuelve True si hay docs.\"\"\"\n",
    "            return any(DOCS_DIR.glob('*'))\n",
    "        globals_['load_index_local'] = load_index_local\n",
    "\n",
    "    if 'build_index_from_local' not in globals_:\n",
    "        def build_index_from_local(paths, chunk_size: int, overlap: int):\n",
    "            \"\"\"Stub: simula el conteo de docs y chunks.\"\"\"\n",
    "            n_docs = len([p for p in paths if Path(p).is_file()])\n",
    "            n_chunks = max(1, n_docs) * max(1, (chunk_size // max(1, overlap)))\n",
    "            return n_chunks, n_docs\n",
    "        globals_['build_index_from_local'] = build_index_from_local\n",
    "\n",
    "    if 's3_rebuild_from_docs' not in globals_:\n",
    "        def s3_rebuild_from_docs(bucket, base_prefix, team_folder, chunk_size: int, overlap: int):\n",
    "            _ = s3_download_docs(bucket, base_prefix, team_folder)\n",
    "            paths = list(DOCS_DIR.glob('*'))\n",
    "            n_chunks, n_docs = build_index_from_local(paths, chunk_size, overlap)\n",
    "            return f\"Reconstruido √≠ndice: {n_docs} docs ‚Üí {n_chunks} chunks.\"\n",
    "        globals_['s3_rebuild_from_docs'] = s3_rebuild_from_docs\n",
    "\n",
    "    if 's3_upload_index' not in globals_:\n",
    "        def s3_upload_index(bucket=None, base_prefix=None, team_folder=None):\n",
    "            bucket = (bucket or STATE['bucket']).strip()\n",
    "            base_prefix = norm_prefix(base_prefix or STATE['base_prefix'])\n",
    "            team_folder = (team_folder or STATE['team_folder']).strip()\n",
    "            eff_root = effective_team_prefix(base_prefix, team_folder) + 'index/'\n",
    "            # Aqu√≠ deber√≠as subir tus archivos de √≠ndice reales; dejamos mensaje.\n",
    "            return f\"(demo) √çndice marcado como subido a s3://{bucket}/{eff_root}\"\n",
    "        globals_['s3_upload_index'] = s3_upload_index\n",
    "\n",
    "    if 's3_download_index' not in globals_:\n",
    "        def s3_download_index(bucket=None, base_prefix=None, team_folder=None):\n",
    "            # Aqu√≠ deber√≠as descargar tus archivos de √≠ndice reales; dejamos mensaje.\n",
    "            return \"(demo) √çndice descargado (stub).\"\n",
    "        globals_['s3_download_index'] = s3_download_index\n",
    "\n",
    "_ensure_defs()\n"
   ],
   "id": "nSb_o1qQjkvj"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643,
     "referenced_widgets": [
      "51435de40aea4497b0d6a76d7da728ab",
      "0f30d2f66a0d411c9deebfb7f5e3957e",
      "eb0366d5a8f848ed9b226074bf4d74d3",
      "58667a304de34fadbd1373afc4250fa0",
      "f38ade5956e24e018a67956bdf28bd24",
      "b0e7988a3faf4573a65cbc6b2a3f868d",
      "55d9c59951b74e16b39ba21ba4166a18",
      "eb7510ef35c04af396563d43a30a0d21",
      "65ec719963984c12907742391eb2656e",
      "e75561e77b014d0590d29d827f8f4a26",
      "ccec9660bcab4522a035622d9fe2102f"
     ]
    },
    "id": "vt5FrTzq7tlO",
    "outputId": "52416b3f-edd2-4155-9eae-65689e018131"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://ba33c77f1591e59c5d.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://ba33c77f1591e59c5d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51435de40aea4497b0d6a76d7da728ab"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def ui_apply_s3(bucket, base_prefix, team_folder):\n",
    "    return apply_route(bucket, base_prefix, team_folder)\n",
    "\n",
    "def ui_list_teams(bucket, base_prefix):\n",
    "    bucket = (bucket or '').strip() or os.getenv(\"S3_BUCKET\")\n",
    "    base_prefix = norm_prefix(base_prefix or os.getenv(\"S3_PREFIX\") or \"\")\n",
    "    teams = s3_list_immediate_folders(bucket, base_prefix)\n",
    "    return \"\\n\".join(teams) if teams else \"(sin subcarpetas)\"\n",
    "\n",
    "def ui_upload(files):\n",
    "    saved=[]\n",
    "    DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for name, b in files:\n",
    "        p=DOCS_DIR/Path(name).name\n",
    "        with open(p,\"wb\") as f: f.write(b)\n",
    "        saved.append(str(p))\n",
    "    return f\"Guardados local: { [Path(p).name for p in saved] }\"\n",
    "\n",
    "def ui_sync_local_to_s3():\n",
    "    return s3_upload_local_docs(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "\n",
    "def ui_sync_s3_to_local():\n",
    "    return s3_download_docs(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "\n",
    "def ui_build_index(chunk_size, overlap):\n",
    "    paths=list(Path(DOCS_DIR).glob(\"*\"))\n",
    "    if not paths: return \"Sube o sincroniza documentos primero.\"\n",
    "    n_chunks, n_docs = build_index_from_local(paths, int(chunk_size), int(overlap))\n",
    "    return f\"‚úÖ √çndice local: {n_docs} docs ‚Üí {n_chunks} chunks.\"\n",
    "\n",
    "def ui_rebuild_from_s3(chunk_size, overlap):\n",
    "    return s3_rebuild_from_docs(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"], int(chunk_size), int(overlap))\n",
    "\n",
    "def ui_save_index():\n",
    "    return s3_upload_index(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "\n",
    "def ui_load_index():\n",
    "    return s3_download_index(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "\n",
    "def ui_ask(q, top_k, temp):\n",
    "    if not load_index_local():\n",
    "        return \"Primero carga/crea un √≠ndice.\", \"\"\n",
    "    ans, fuentes = rag_answer(q, int(top_k), float(temp))\n",
    "    return ans, \"\\n\".join(sorted(set(fuentes)))\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"Chatbot RAG + S3 + Telegram (Admin & equipos)\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"### üß† Cerebro ‚Äî Equipo ‚Üí Docs ‚Üí √çndice ‚Üí Persistencia ‚Üí Preguntas ‚Üí **Telegram Admin**\")\n",
    "\n",
    "    with gr.Tab(\"0) S3 (Equipo)\"):\n",
    "        with gr.Row():\n",
    "            b = gr.Textbox(value=os.getenv(\"S3_BUCKET\"), label=\"S3_BUCKET\")\n",
    "            bp= gr.Textbox(value=os.getenv(\"S3_PREFIX\"), label=\"Prefijo base del curso (p.ej. IA-Innovador/)\")\n",
    "            tf= gr.Textbox(label=\"Carpeta del equipo (p.ej. Daniel)\")\n",
    "        s3_state = gr.Textbox(label=\"Estado de la ruta activa\", lines=3)\n",
    "        teams_out = gr.Textbox(label=\"Carpetas encontradas\", lines=6)\n",
    "        with gr.Row():\n",
    "            btn_apply = gr.Button(\"Aplicar ruta\")\n",
    "            btn_list  = gr.Button(\"Listar carpetas existentes (bajo prefijo base)\")\n",
    "        btn_apply.click(ui_apply_s3, inputs=[b,bp,tf], outputs=s3_state)\n",
    "        btn_list.click(ui_list_teams, inputs=[b,bp], outputs=teams_out)\n",
    "\n",
    "    with gr.Tab(\"1) Docs\"):\n",
    "        files=gr.File(label=\"Sube PDF/TXT/CSV (opcional, puedes solo sincronizar S3‚Üílocal)\", file_count=\"multiple\", type=\"binary\")\n",
    "        out=gr.Textbox(label=\"Estado\", lines=3)\n",
    "        with gr.Row():\n",
    "            btn_up  = gr.Button(\"Subir a local\")\n",
    "            btn_s2l = gr.Button(\"S3 ‚Üí Local (descargar docs)\")\n",
    "            btn_l2s = gr.Button(\"Local ‚Üí S3 (subir docs)\")\n",
    "        btn_up.click(ui_upload, inputs=files, outputs=out)\n",
    "        btn_s2l.click(ui_sync_s3_to_local, outputs=out)\n",
    "        btn_l2s.click(ui_sync_local_to_s3, outputs=out)\n",
    "\n",
    "    with gr.Tab(\"2) √çndice\"):\n",
    "        out2=gr.Textbox(label=\"Estado\", lines=4)\n",
    "        with gr.Row():\n",
    "            cs=gr.Number(value=800, precision=0, label=\"chunk_size\")\n",
    "            ov=gr.Number(value=150, precision=0, label=\"overlap\")\n",
    "        with gr.Row():\n",
    "            btn_build = gr.Button(\"Construir √≠ndice desde LOCAL\")\n",
    "            btn_reb_s3= gr.Button(\"Reconstruir √≠ndice desde DOCS en S3\")\n",
    "        btn_build.click(ui_build_index, inputs=[cs,ov], outputs=out2)\n",
    "        btn_reb_s3.click(ui_rebuild_from_s3, inputs=[cs,ov], outputs=out2)\n",
    "\n",
    "    with gr.Tab(\"3) Persistencia\"):\n",
    "        out3=gr.Textbox(label=\"Estado\", lines=4)\n",
    "        with gr.Row():\n",
    "            btn_save = gr.Button(\"‚¨ÜÔ∏è Guardar √≠ndice en S3\")\n",
    "            btn_load = gr.Button(\"‚¨áÔ∏è Cargar √≠ndice desde S3\")\n",
    "        btn_save.click(ui_save_index, outputs=out3)\n",
    "        btn_load.click(ui_load_index, outputs=out3)\n",
    "        gr.Markdown(\"Se intenta en `.../<equipo>/` y en `.../<equipo>/index/`.\")\n",
    "\n",
    "    with gr.Tab(\"4) Preguntar\"):\n",
    "        q=gr.Textbox(label=\"Pregunta\")\n",
    "        with gr.Row():\n",
    "            tk=gr.Slider(value=4, minimum=1, maximum=10, step=1, label=\"top_k\")\n",
    "            tp=gr.Slider(value=0.2, minimum=0.0, maximum=1.2, step=0.1, label=\"temperature\")\n",
    "        ans=gr.Markdown(\"Respuesta\")\n",
    "        src=gr.Textbox(label=\"Fuentes\", lines=4)\n",
    "        btn_q=gr.Button(\"Consultar\")\n",
    "        btn_q.click(ui_ask, inputs=[q, tk, tp], outputs=[ans, src])\n",
    "\n",
    "    with gr.Tab(\"5) Telegram (Admin)\"):\n",
    "        with gr.Row():\n",
    "            tok=gr.Textbox(label=\"BOT TOKEN\", type=\"password\", value=os.getenv(\"TELEGRAM_BOT_TOKEN\",\"\"))\n",
    "            set_out=gr.Textbox(label=\"Bot\", interactive=False)\n",
    "            btn_tok=gr.Button(\"Guardar token / Ver bot\")\n",
    "            btn_detect=gr.Button(\"Usar token existente\")\n",
    "            btn_tok.click(ui_set_token, inputs=tok, outputs=set_out)\n",
    "            btn_detect.click(lambda: ui_set_token(\"\"), outputs=set_out)\n",
    "        with gr.Row():\n",
    "            auto=gr.Checkbox(label=\"Auto-responder (bot responde solo con RAG+LLM)\", value=False)\n",
    "            auto_out=gr.Textbox(label=\"Estado\", interactive=False)\n",
    "            auto.change(lambda v: ui_auto_toggle(v), inputs=auto, outputs=auto_out)\n",
    "        with gr.Row():\n",
    "            poll=gr.Checkbox(label=\"Polling (escuchar mensajes)\", value=False)\n",
    "            poll_out=gr.Textbox(label=\"Estado\", interactive=False)\n",
    "            poll.change(lambda v: ui_poll_toggle(v), inputs=poll, outputs=poll_out)\n",
    "            step=gr.Button(\"Forzar lectura ahora (poll once)\")\n",
    "            step_out=gr.Textbox(label=\"Resultado\", interactive=False)\n",
    "            step.click(ui_poll_once, outputs=step_out)\n",
    "        with gr.Row():\n",
    "            chat_list=gr.Button(\"Listar chats conocidos\")\n",
    "            list_out=gr.Textbox(label=\"chats\", lines=6)\n",
    "            chat_list.click(ui_list_chats, outputs=list_out)\n",
    "        with gr.Row():\n",
    "            sel_cid=gr.Textbox(label=\"Enviar a chat_id (pega uno de la lista)\")\n",
    "            msg=gr.Textbox(label=\"Mensaje del admin\", value=\"Hola, soy soporte. ¬øEn qu√© te ayudo?\")\n",
    "            send_out=gr.Textbox(label=\"Resultado\", interactive=False)\n",
    "            send=gr.Button(\"Enviar como admin\")\n",
    "            send.click(ui_admin_send, inputs=[sel_cid, msg], outputs=send_out)\n",
    "\n",
    "demo.launch(share=True, debug=True)\n"
   ],
   "id": "vt5FrTzq7tlO"
  },
  {
   "cell_type": "code",
   "source": [
    "# === PARCHE √öNICO: handoff \"Asesor/Ayuda\", sin \"Fuentes\" en Telegram, espejo a admin y reuso de token ===\n",
    "import os, re, requests, time, json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) LLM: NO pedir \"Fuentes\" dentro del texto y permitir activarlo opcionalmente para la UI\n",
    "def _tt_openai():\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        return OpenAI(api_key=key) if key else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _tt_gemini():\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not key:\n",
    "            return None\n",
    "        genai.configure(api_key=key)\n",
    "        return genai.GenerativeModel(os.getenv(\"GEMINI_MODEL\",\"gemini-1.5-flash\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _tt_llm_answer(q, ctx, with_sources=False):\n",
    "    prov = (os.getenv(\"LLM_PROVIDER\") or \"openai\").lower()\n",
    "    temp = float(os.getenv(\"LLM_TEMPERATURE\",\"0.2\"))\n",
    "    mtok = int(os.getenv(\"LLM_MAX_TOKENS\",\"400\"))\n",
    "    sys_prompt = (\"Eres un asistente que responde SOLO con la informaci√≥n del contexto. \"\n",
    "                  \"Si no est√° en el contexto di: 'No encuentro esa informaci√≥n en mis archivos'. \"\n",
    "                  \"Responde en espa√±ol de forma clara y breve.\")\n",
    "    user = f\"Pregunta: {q}\\n\\nContexto:\\n{ctx}\\n\\nResponde SOLO con lo anterior.\"\n",
    "\n",
    "    if prov == \"openai\":\n",
    "        cli = _tt_openai()\n",
    "        if not cli:\n",
    "            return \"‚ö†Ô∏è Sin LLM (OPENAI_API_KEY no configurada)\"\n",
    "        out = cli.chat.completions.create(\n",
    "            model=os.getenv(\"OPENAI_MODEL\",\"gpt-4o-mini\"),\n",
    "            messages=[{\"role\":\"system\",\"content\":sys_prompt},\n",
    "                      {\"role\":\"user\",\"content\":user}],\n",
    "            temperature=temp, max_tokens=mtok\n",
    "        )\n",
    "        txt = out.choices[0].message.content.strip()\n",
    "    elif prov == \"gemini\":\n",
    "        model = _tt_gemini()\n",
    "        if not model:\n",
    "            return \"‚ö†Ô∏è Sin LLM (GOOGLE_API_KEY/GEMINI_API_KEY no configurada)\"\n",
    "        out = model.generate_content(f\"{sys_prompt}\\n\\n{user}\")\n",
    "        txt = (getattr(out,\"text\",\"\") or \"\").strip()\n",
    "    else:\n",
    "        txt = \"‚ö†Ô∏è Sin LLM disponible.\"\n",
    "\n",
    "    if with_sources and ctx:\n",
    "        # A√±ade lista de fuentes al final SOLO si se pide expl√≠citamente (para la UI)\n",
    "        fuentes = []\n",
    "        for line in ctx.splitlines():\n",
    "            if line.startswith('[') and ']' in line:\n",
    "                tag = line.split(']')[0].strip('[]')\n",
    "                name = tag.split('|')[0].strip()\n",
    "                if name and name not in fuentes:\n",
    "                    fuentes.append(name)\n",
    "        if fuentes:\n",
    "            txt = txt + \"\\n\\nFuentes: \" + \", \".join(fuentes)\n",
    "\n",
    "    # Limpieza por si el modelo inventa una secci√≥n de \"Fuentes:\"\n",
    "    if not with_sources:\n",
    "        txt = re.sub(r\"\\n\\s*Fuentes\\s*:.*$\", \"\", txt, flags=re.S)\n",
    "    return txt\n",
    "\n",
    "def _tt_build_context(q):\n",
    "    try:\n",
    "        if 'retrieve' in globals():\n",
    "            hits = retrieve(q, top_k=4)\n",
    "            parts=[]\n",
    "            for score, ch in hits:\n",
    "                snip=(ch.text[:350]+\"‚Ä¶\") if len(ch.text)>350 else ch.text\n",
    "                src=getattr(ch,'source_name','doc')\n",
    "                parts.append(f\"[{src} | {score:.3f}] {snip}\")\n",
    "            return \"\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def _tt_answer_logic(q, for_telegram=False):\n",
    "    q = (q or \"\").strip()\n",
    "    if not q:\n",
    "        return \"Env√≠ame un texto y responder√© con lo que haya en tus archivos.\"\n",
    "\n",
    "    # 1) Tu funci√≥n personalizada (si existe)\n",
    "    if 'responder' in globals():\n",
    "        try:\n",
    "            ans = responder(q)\n",
    "            # por si responder() puso \"Fuentes\" adentro:\n",
    "            if for_telegram:\n",
    "                ans = re.sub(r\"\\n\\s*Fuentes\\s*:.*$\", \"\", ans, flags=re.S)\n",
    "            return ans\n",
    "        except Exception as e:\n",
    "            return f\"‚ö†Ô∏è Error en responder(): {e}\"\n",
    "\n",
    "    # 2) Tu rag_answer() (si existe)\n",
    "    if 'rag_answer' in globals():\n",
    "        try:\n",
    "            ans, _fuentes = rag_answer(q, top_k=4, temperature=float(os.getenv(\"LLM_TEMPERATURE\",\"0.2\")))\n",
    "            if for_telegram:\n",
    "                ans = re.sub(r\"\\n\\s*Fuentes\\s*:.*$\", \"\", ans, flags=re.S)\n",
    "            return ans\n",
    "        except Exception as e:\n",
    "            return f\"‚ö†Ô∏è Error en rag_answer(): {e}\"\n",
    "\n",
    "    # 3) retrieve + LLM\n",
    "    ctx = _tt_build_context(q) or \"(sin contexto)\"\n",
    "    return _tt_llm_answer(q, ctx, with_sources=not for_telegram)\n",
    "\n",
    "globals()['_tt_llm_answer'] = _tt_llm_answer\n",
    "globals()['_tt_answer_logic'] = _tt_answer_logic\n",
    "\n",
    "# 2) Monkey-patch de TTGram: handoff ‚ÄúAsesor/Ayuda‚Äù sin LLM, espejo al admin, reuso de token, sin fuentes en Telegram\n",
    "if 'TTGram' in globals():\n",
    "    TT = globals()['TTGram']\n",
    "\n",
    "    def _tg_set_token(self, token: str):\n",
    "        token = (token or getattr(self, 'token', '') or\n",
    "                 os.getenv('TELEGRAM_BOT_TOKEN') or\n",
    "                 (userdata.get('TELEGRAM_BOT_TOKEN') if 'userdata' in globals() else '')).strip()\n",
    "        if token.upper().startswith(\"BOT:\"):\n",
    "            token = token.split(\":\",1)[1].strip()\n",
    "        self.token = token\n",
    "        if token:\n",
    "            os.environ[\"TELEGRAM_BOT_TOKEN\"] = token\n",
    "            if 'userdata' in globals():\n",
    "                try: userdata[\"TELEGRAM_BOT_TOKEN\"]=token\n",
    "                except Exception: pass\n",
    "        try:\n",
    "            requests.get(f\"https://api.telegram.org/bot{token}/deleteWebhook\", timeout=10)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # getMe\n",
    "        try:\n",
    "            r=requests.get(f\"https://api.telegram.org/bot{token}/getMe\", timeout=10)\n",
    "            r.raise_for_status()\n",
    "            data=r.json()\n",
    "            return \"Bot: @\" + (data.get(\"result\",{}).get(\"username\") or \"(sin username)\")\n",
    "        except Exception as e:\n",
    "            return f\"Token/Bot error: {e}\"\n",
    "\n",
    "    def _tg_handle_update(self, item):\n",
    "        msg=item.get(\"message\") or {}\n",
    "        chat=msg.get(\"chat\") or {}\n",
    "        cid=str(chat.get(\"id\"))\n",
    "        txt=(msg.get(\"text\") or \"\").strip()\n",
    "        title=chat.get(\"username\") or chat.get(\"title\") or \"\"\n",
    "        if not cid or not txt:\n",
    "            return\n",
    "\n",
    "        self.known.setdefault(cid, {\"title\": title, \"last_text\": \"\"})\n",
    "        self.known[cid][\"last_text\"] = txt\n",
    "        # log\n",
    "        try:\n",
    "            p=(Path.cwd()/ \"mini_chatbot_work\" / \"logs\"); p.mkdir(parents=True, exist_ok=True)\n",
    "            with open(p/f\"{cid}.jsonl\",\"a\",encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps({\"t\":time.time(),\"role\":\"user\",\"text\":txt}, ensure_ascii=False)+\"\\n\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # comandos admin\n",
    "        if cid == getattr(self,'admin_chat_id',''):\n",
    "            if txt == \"/admin\":\n",
    "                self._send(cid, \"Ya eres canal admin ‚úÖ\"); return\n",
    "            if txt.startswith(\"/auto_on\"):\n",
    "                parts=txt.split()\n",
    "                if len(parts)>=2:\n",
    "                    self.known.setdefault(parts[1],{})[\"auto\"]=True\n",
    "                    self._send(cid, f\"Auto ON para {parts[1]}\"); return\n",
    "            if txt.startswith(\"/auto_off\"):\n",
    "                parts=txt.split()\n",
    "                if len(parts)>=2:\n",
    "                    self.known.setdefault(parts[1],{})[\"auto\"]=False\n",
    "                    self._send(cid, f\"Auto OFF para {parts[1]}\"); return\n",
    "            if txt.startswith(\"/say\"):\n",
    "                parts=txt.split(maxsplit=2)\n",
    "                if len(parts)>=3:\n",
    "                    tgt, m = parts[1], parts[2]\n",
    "                    self._send(tgt, m)\n",
    "                    # hold para no pisarnos\n",
    "                    self.known.setdefault(tgt,{})[\"hold_until\"] = time.time() + float(getattr(self,'HOLD_AFTER_ADMIN',60))\n",
    "                    # espejo al admin ya es innecesario aqu√≠\n",
    "                    return\n",
    "\n",
    "        if txt == \"/admin\" and not getattr(self,'admin_chat_id',''):\n",
    "            self.admin_chat_id = cid\n",
    "            os.environ[\"ADMIN_CHAT_ID\"] = cid\n",
    "            self._send(cid, \"Este chat queda configurado como canal admin ‚úÖ\")\n",
    "            return\n",
    "\n",
    "        # Handoff humano por \"asesor/ayuda\" ‚Äî NO usar LLM aqu√≠\n",
    "        if re.search(r\"\\b(asesor|ayuda)\\b\", txt, re.I):\n",
    "            handoff = (\"He activado soporte humano. Un asesor se unir√° en breve. \"\n",
    "                       \"Tambi√©n puedes seguir escribiendo y lo revisar√©.\")\n",
    "            self._send(cid, handoff)\n",
    "            # pausa auto para ese chat\n",
    "            self.known.setdefault(cid,{})[\"hold_until\"] = time.time() + float(getattr(self,'HOLD_AFTER_ALERT',120))\n",
    "            # ALERTA al admin\n",
    "            if getattr(self,'admin_chat_id',''):\n",
    "                self._send(self.admin_chat_id,\n",
    "                           f\"‚ö†Ô∏è ALERTA: {cid} pidi√≥ ayuda (‚Äú{txt}‚Äù). Usa /say {cid} <mensaje> o /auto_on {cid}.\")\n",
    "            return\n",
    "\n",
    "        # Auto-responder (sin fuentes en Telegram)\n",
    "        # respeta hold y auto por chat\n",
    "        hold_until = float(self.known.get(cid,{}).get(\"hold_until\", 0))\n",
    "        if time.time() < hold_until:\n",
    "            return\n",
    "        auto = self.known.get(cid,{}).get(\"auto\", getattr(self,'global_auto', True))\n",
    "        if auto:\n",
    "            ans = _tt_answer_logic(txt, for_telegram=True)\n",
    "            self._send(cid, ans)\n",
    "            # espejo al admin para que veas qu√© respondi√≥\n",
    "            if getattr(self,'admin_chat_id','') and getattr(self,'MIRROR_TO_ADMIN', True):\n",
    "                try:\n",
    "                    self._send(self.admin_chat_id, f\"[BOT‚Üí{cid}] {ans}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # aplicar monkey-patch\n",
    "    TT.set_token = _tg_set_token\n",
    "    TT.handle_update = _tg_handle_update\n",
    "    # marcar flags\n",
    "    TT.MIRROR_TO_ADMIN = True\n",
    "\n",
    "    # rebind ui_* a la instancia actual (o crear una)\n",
    "    _TTG = globals().get('_TTG')\n",
    "    if _TTG is None or _TTG.__class__.__name__ != 'TTGram':\n",
    "        _TTG = TT()\n",
    "        globals()['_TTG'] = _TTG\n",
    "    globals()['ui_set_token']   = _TTG.set_token\n",
    "    globals()['ui_poll_toggle'] = getattr(_TTG, 'toggle_poll', lambda v: \"No toggle_poll\")\n",
    "    globals()['ui_poll_once']   = getattr(_TTG, 'poll_now',   lambda : \"No poll_now\")\n",
    "    globals()['ui_list_chats']  = getattr(_TTG, 'list_chats', lambda : \"(sin funci√≥n)\")\n",
    "    globals()['ui_admin_send']  = getattr(_TTG, 'admin_send', lambda cid, m: \"No admin_send\")\n",
    "    globals()['ui_auto_toggle'] = getattr(_TTG, 'auto_toggle',lambda v: \"No auto_toggle\")\n",
    "    print(\"‚úÖ TTGram parcheado (handoff + espejo admin + reuso token + sin fuentes en Telegram).\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è TTGram no est√° definido todav√≠a. Ejecuta tu celda de Telegram y luego vuelve a correr esta celda.\")\n",
    "\n",
    "# 3) UI: si existe ui_ask(), evitar que ‚ÄúFuentes‚Äù aparezca dentro del texto de respuesta (solo en el cuadro de fuentes)\n",
    "if 'ui_ask' in globals():\n",
    "    _orig_ui_ask = ui_ask\n",
    "    def ui_ask_clean(q, top_k, temp):\n",
    "        ans, fuentes = _orig_ui_ask(q, top_k, temp)\n",
    "        ans = re.sub(r\"\\n\\s*Fuentes\\s*:.*$\", \"\", ans, flags=re.S)\n",
    "        return ans, fuentes\n",
    "    globals()['ui_ask'] = ui_ask_clean\n",
    "    print(\"‚úÖ ui_ask ajustado para no duplicar 'Fuentes' dentro de la respuesta.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è ui_ask no est√° (o viene despu√©s). Si ves 'Fuentes' duplicado, ejecuta esta celda al final y vuelve a intentar.\")\n",
    "\n",
    "print(\"Parche aplicado. Si ya tienes token en env/userdata, en la pesta√±a 5 pulsa 'Usar token existente'.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1WQjoH-q_we",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758297081574,
     "user_tz": 300,
     "elapsed": 39,
     "user": {
      "displayName": "Engler Gonz√°lez",
      "userId": "10704175225987180059"
     }
    },
    "outputId": "b8ff7e53-9b87-4a2d-d8ff-b15f8aa7fd26"
   },
   "id": "Q1WQjoH-q_we",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ TTGram parcheado (handoff + espejo admin + reuso token + sin fuentes en Telegram).\n",
      "‚úÖ ui_ask ajustado para no duplicar 'Fuentes' dentro de la respuesta.\n",
      "Parche aplicado. Si ya tienes token en env/userdata, en la pesta√±a 5 pulsa 'Usar token existente'.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CUSTOM PATCH: actualizar prompt y manejo de asesor/ayuda ===\n",
    "# Ajuste del SYS_PROMPT para evitar que el modelo a√±ada 'Fuentes' autom√°ticamente\n",
    "try:\n",
    "    SYS_PROMPT = (\n",
    "        \"Eres un asistente para preguntas y respuestas basado en archivos cargados por el usuario. \"\n",
    "        \"Responde SOLO con la informaci√≥n del contexto. \"\n",
    "        \"Si la respuesta no est√° en el contexto di: 'No encuentro esa informaci√≥n en mis archivos'. \"\n",
    "        \"Responde en espa√±ol de forma clara y breve. No incluyas fuentes en el texto.\"\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# L√≥gica de respuesta unificada: controla 'Fuentes' y usa temperatura de env\n",
    "\n",
    "def _tt_answer_logic(q, for_telegram=False):\n",
    "    q = (q or \"\").strip()\n",
    "    if not q:\n",
    "        return \"Env√≠ame un texto y responder√© con lo que haya en tus archivos.\"\n",
    "    # 1) funci√≥n responder() personalizada del usuario\n",
    "    if 'responder' in globals():\n",
    "        try:\n",
    "            ans = responder(q)\n",
    "            if for_telegram:\n",
    "                import re\n",
    "                ans = re.sub(r\"\n",
    "\\s*Fuentes\\s*:.*$\", \"\", ans, flags=re.S)\n",
    "            return ans\n",
    "        except Exception as e:\n",
    "            return f\"‚ö†Ô∏è Error en responder(): {e}\"\n",
    "    # 2) funci√≥n rag_answer() personalizada\n",
    "    if 'rag_answer' in globals():\n",
    "        try:\n",
    "            temp = float(os.getenv('LLM_TEMPERATURE', '0.2'))\n",
    "            ans, fuentes = rag_answer(q, top_k=4, temperature=temp)\n",
    "            if for_telegram:\n",
    "                import re\n",
    "                ans = re.sub(r\"\n",
    "\\s*Fuentes\\s*:.*$\", \"\", ans, flags=re.S)\n",
    "            return ans\n",
    "        except Exception as e:\n",
    "            return f\"‚ö†Ô∏è Error en rag_answer(): {e}\"\n",
    "    # 3) B√∫squeda con retrieve + LLM\n",
    "    try:\n",
    "        ctx = _tt_build_context(q) or \"(sin contexto)\"\n",
    "    except Exception:\n",
    "        ctx = \"(sin contexto)\"\n",
    "    return _tt_llm_answer(q, ctx, with_sources=not for_telegram)\n",
    "\n",
    "# Parchear manejo de mensajes para 'asesor' o 'ayuda': no usar LLM, avisar admin y pausar\n",
    "try:\n",
    "    import types\n",
    "    def patched_handle_update(self, item):\n",
    "        msg = item.get(\"message\") or {}\n",
    "        chat = msg.get(\"chat\") or {}\n",
    "        cid = str(chat.get(\"id\"))\n",
    "        txt = (msg.get(\"text\") or \"\").strip()\n",
    "        title = chat.get(\"username\") or chat.get(\"title\") or \"\"\n",
    "        if not cid or not txt:\n",
    "            return\n",
    "        # actualizar registro de chats\n",
    "        self.known.setdefault(cid, {\"title\": title, \"last_text\": \"\"})\n",
    "        self.known[cid][\"last_text\"] = txt\n",
    "        try:\n",
    "            self._log(cid, \"user\", txt)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Comandos del admin\n",
    "        if cid == getattr(self, 'admin_chat_id',''):\n",
    "            if txt == \"/admin\":\n",
    "                self._send(cid, \"Ya eres canal admin ‚úÖ\")\n",
    "                return\n",
    "            if txt.startswith(\"/auto_on\"):\n",
    "                parts = txt.split()\n",
    "                if len(parts)>=2:\n",
    "                    self._set_auto_chat(parts[1], True)\n",
    "                    self._send(cid, f\"Auto ON para {parts[1]}\")\n",
    "                return\n",
    "            if txt.startswith(\"/auto_off\"):\n",
    "                parts = txt.split()\n",
    "                if len(parts)>=2:\n",
    "                    self._set_auto_chat(parts[1], False)\n",
    "                    self._send(cid, f\"Auto OFF para {parts[1]}\")\n",
    "                return\n",
    "            if txt.startswith(\"/say\"):\n",
    "                parts = txt.split(maxsplit=2)\n",
    "                if len(parts)>=3:\n",
    "                    tgt, m = parts[1], parts[2]\n",
    "                    self._send(tgt, m)\n",
    "                    try:\n",
    "                        self._log(tgt, \"admin\", m)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    self._set_hold(tgt, getattr(self,'HOLD_AFTER_ADMIN',60))\n",
    "                return\n",
    "        # Configurar canal admin si a√∫n no est√°\n",
    "        if txt == \"/admin\" and not getattr(self, 'admin_chat_id',''):\n",
    "            self.admin_chat_id = cid\n",
    "            os.environ[\"ADMIN_CHAT_ID\"] = cid\n",
    "            self._send(cid, \"Este chat queda configurado como canal admin ‚úÖ\")\n",
    "            return\n",
    "        # Palabras clave de traspaso a humano\n",
    "        if hasattr(self, 'ALERT_REGEX') and self.ALERT_REGEX.search(txt):\n",
    "            hand_msg = (\"He activado soporte humano. Un asesor se unir√° en breve. \"\n",
    "                        \"Tambi√©n puedes seguir escribiendo y lo revisar√©.\")\n",
    "            self._send(cid, hand_msg)\n",
    "            try:\n",
    "                self._log(cid, \"bot\", hand_msg)\n",
    "            except Exception:\n",
    "                pass\n",
    "            self._set_hold(cid, getattr(self,'HOLD_AFTER_ALERT',120))\n",
    "            # avisar al admin\n",
    "            if getattr(self,'admin_chat_id',''):\n",
    "                self._send(self.admin_chat_id,\n",
    "                           f\"‚ö†Ô∏è ALERTA: {cid} pidi√≥ ayuda (\"{txt}\").\n",
    "\"\n",
    "                           f\"Usa /say {cid} <mensaje> para intervenir, o /auto_on {cid} cuando quieras reanudar el bot.\")\n",
    "            return\n",
    "        # Respuesta autom√°tica\n",
    "        if self._should_auto(cid):\n",
    "            ans = _tt_answer_logic(txt, for_telegram=True)\n",
    "            # limpiar cualquier secci√≥n de fuentes\n",
    "            try:\n",
    "                import re as _re\n",
    "                ans = _re.sub(r\"\n",
    "\\s*Fuentes\\s*:.*$\", \"\", ans, flags=_re.S)\n",
    "            except Exception:\n",
    "                pass\n",
    "            self._send(cid, ans)\n",
    "            try:\n",
    "                self._log(cid, \"bot\", ans)\n",
    "            except Exception:\n",
    "                pass\n",
    "            # espejo al admin\n",
    "            if getattr(self,'admin_chat_id',''):\n",
    "                self._send(self.admin_chat_id, f\"[BOT‚Üí{cid}] {ans}\")\n",
    "    if 'TTGram' in globals():\n",
    "        TTGram.handle_update = patched_handle_update\n",
    "except Exception as e:\n",
    "    print('Error al parchear manejo de mensajes:', e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": []
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "51435de40aea4497b0d6a76d7da728ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f30d2f66a0d411c9deebfb7f5e3957e",
       "IPY_MODEL_eb0366d5a8f848ed9b226074bf4d74d3",
       "IPY_MODEL_58667a304de34fadbd1373afc4250fa0"
      ],
      "layout": "IPY_MODEL_f38ade5956e24e018a67956bdf28bd24"
     }
    },
    "0f30d2f66a0d411c9deebfb7f5e3957e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0e7988a3faf4573a65cbc6b2a3f868d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_55d9c59951b74e16b39ba21ba4166a18",
      "value": "Batches:‚Äá100%"
     }
    },
    "eb0366d5a8f848ed9b226074bf4d74d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb7510ef35c04af396563d43a30a0d21",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65ec719963984c12907742391eb2656e",
      "value": 1
     }
    },
    "58667a304de34fadbd1373afc4250fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e75561e77b014d0590d29d827f8f4a26",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ccec9660bcab4522a035622d9fe2102f",
      "value": "‚Äá1/1‚Äá[00:00&lt;00:00,‚Äá‚Äá6.26it/s]"
     }
    },
    "f38ade5956e24e018a67956bdf28bd24": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0e7988a3faf4573a65cbc6b2a3f868d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55d9c59951b74e16b39ba21ba4166a18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb7510ef35c04af396563d43a30a0d21": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65ec719963984c12907742391eb2656e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e75561e77b014d0590d29d827f8f4a26": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccec9660bcab4522a035622d9fe2102f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}