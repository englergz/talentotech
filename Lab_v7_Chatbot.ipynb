{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83HaTR3j7tkp"
   },
   "source": [
    "# 🧪 Chatbot RAG + S3 + Telegram\n",
    "**Listo para Google Colab**\n",
    "\n",
    "### Novedades clave\n",
    "- **LLM seleccionable**: OpenAI *o* Gemini. Si no hay API key, cae en *modo fragmento más relevante*.\n",
    "- **Persistencia real** en S3: índice FAISS guardado/cargado por equipo + reconstrucción desde `docs/`.\n",
    "- **Telegram completo**: los clientes escriben al bot, el admin ve todos los chats, activa/desactiva **Auto-responder** y puede **intervenir** en cualquier momento.\n",
    "- **Transcripciones** por `chat_id` en `mini_chatbot_work/logs/`.\n"
   ],
   "id": "83HaTR3j7tkp"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghh-ERwS7tk2"
   },
   "source": [
    "## 0) Instalación"
   ],
   "id": "Ghh-ERwS7tk2"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tL-JJTo7tk3",
    "outputId": "bef2d6cd-ba28-4a85-8e15-8c51189c1111",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295171994,
     "user_tz": 300,
     "elapsed": 27420,
     "user": {
      "displayName": "Engler González",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip -q install sentence-transformers faiss-cpu pypdf gradio boto3 openai==1.* tiktoken requests google-generativeai\n"
   ],
   "id": "-tL-JJTo7tk3"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFEkEyMR7tk7"
   },
   "source": [
    "## 1) Configuración"
   ],
   "id": "AFEkEyMR7tk7"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4nOrSx47tk8",
    "outputId": "ff8aacc5-b4bc-4238-8008-5c41a5593e53",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295199749,
     "user_tz": 300,
     "elapsed": 27719,
     "user": {
      "displayName": "Engler González",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AWS_ACCESS_KEY_ID: AKIAQNBYZOKR56S2G57T\n",
      "AWS_SECRET_ACCESS_KEY (oculto): ··········\n",
      "✔ AWS: us-east-2 talentotech2025 IA-Innovador/\n",
      "✅ API Key(s) cargada(s) correctamente.\n",
      "✔ LLM provider: openai | OpenAI model: gpt-4o-mini | Gemini model: gemini-1.5-flash\n"
     ]
    }
   ],
   "source": [
    "# A) Entrada interactiva\n",
    "import os\n",
    "from getpass import getpass\n",
    "from google.colab import userdata\n",
    "\n",
    "# --- AWS / S3 ---\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-2\")\n",
    "S3_BUCKET  = os.getenv(\"S3_BUCKET\", \"talentotech2025\")\n",
    "S3_PREFIX  = os.getenv(\"S3_PREFIX\", \"IA-Innovador/\")  # prefijo base del curso\n",
    "\n",
    "# --- LLM ---\n",
    "LLM_PROVIDER = (os.getenv(\"LLM_PROVIDER\") or \"openai\").lower()   # \"openai\" o \"gemini\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or userdata.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL   = os.getenv(\"OPENAI_MODEL\")   or \"gpt-4o-mini\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or userdata.get(\"GOOGLE_API_KEY\")\n",
    "GEMINI_MODEL   = os.getenv(\"GEMINI_MODEL\")   or \"gemini-1.5-flash\"\n",
    "\n",
    "\n",
    "# Pide claves si faltan (opcional)\n",
    "AWS_ACCESS_KEY_ID     = os.getenv(\"AWS_ACCESS_KEY_ID\")     or input(\"AWS_ACCESS_KEY_ID: \").strip()\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\") or getpass(\"AWS_SECRET_ACCESS_KEY (oculto): \").strip()\n",
    "AWS_SESSION_TOKEN     = os.getenv(\"AWS_SESSION_TOKEN\")     # opcional (solo STS/Academy)\n",
    "\n",
    "# Persistir en entorno\n",
    "env = {\n",
    "\"AWS_REGION\":AWS_REGION, \"S3_BUCKET\":S3_BUCKET, \"S3_PREFIX\":S3_PREFIX,\n",
    "\"AWS_ACCESS_KEY_ID\":AWS_ACCESS_KEY_ID, \"AWS_SECRET_ACCESS_KEY\":AWS_SECRET_ACCESS_KEY\n",
    "}\n",
    "if AWS_SESSION_TOKEN: env[\"AWS_SESSION_TOKEN\"]=AWS_SESSION_TOKEN\n",
    "for k,v in env.items(): os.environ[k]=v\n",
    "\n",
    "# LLM env (deja vacío si no tienes)\n",
    "os.environ[\"LLM_PROVIDER\"]=LLM_PROVIDER\n",
    "if OPENAI_API_KEY: os.environ[\"OPENAI_API_KEY\"]=OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_MODEL\"]=OPENAI_MODEL\n",
    "if GOOGLE_API_KEY: os.environ[\"GOOGLE_API_KEY\"]=GOOGLE_API_KEY\n",
    "os.environ[\"GEMINI_MODEL\"]=GEMINI_MODEL\n",
    "\n",
    "print(\"✔ AWS:\", AWS_REGION, S3_BUCKET, S3_PREFIX)\n",
    "\n",
    "if OPENAI_API_KEY or GOOGLE_API_KEY:\n",
    "    print(\"✅ API Key(s) cargada(s) correctamente.\")\n",
    "    print(\"✔ LLM provider:\", LLM_PROVIDER, \"| OpenAI model:\", OPENAI_MODEL, \"| Gemini model:\", GEMINI_MODEL)\n",
    "else:\n",
    "    print(\"⚠️ No se detectaron API Keys. El chat funcionará en modo 'fragmento más relevante'.\")\n",
    "    print(\"✔ LLM provider: Fragmento más relevante\")"
   ],
   "id": "G4nOrSx47tk8"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeH2Jtmx7tlA"
   },
   "source": [
    "## 2) S3 helpers (autoregión, listar carpetas/archivos, sync, prefijo efectivo)"
   ],
   "id": "BeH2Jtmx7tlA"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xLVWVFv7tlC",
    "outputId": "dbef457c-fb2e-4846-f432-7b4e451f6d89",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295199767,
     "user_tz": 300,
     "elapsed": 14,
     "user": {
      "displayName": "Engler González",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, boto3, datetime, re, json, time, threading, requests\n",
    "from pathlib import Path\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "def norm_prefix(p: str) -> str:\n",
    "    if p is None:\n",
    "        return \"\"\n",
    "    p = p.strip().replace(\"\\\\\", \"/\")\n",
    "    p = p.lstrip(\"/\")\n",
    "    if p and not p.endswith(\"/\"):\n",
    "        p += \"/\"\n",
    "    return p\n",
    "\n",
    "\n",
    "def get_bucket_region(bucket: str) -> str:\n",
    "    s3g = boto3.client(\"s3\")\n",
    "    loc = s3g.get_bucket_location(Bucket=bucket).get(\"LocationConstraint\")\n",
    "    return \"us-east-1\" if loc in (None, \"EU\") else loc\n",
    "\n",
    "\n",
    "def s3_client_autoregion(bucket: str):\n",
    "    try:\n",
    "        region = get_bucket_region(bucket)\n",
    "    except Exception:\n",
    "        region = os.getenv(\"AWS_REGION\", \"us-east-2\")\n",
    "    return boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "\n",
    "def s3_list_objects(bucket: str, prefix: str, delimiter: str = None):\n",
    "    s3 = s3_client_autoregion(bucket)\n",
    "    kwargs = {\"Bucket\": bucket, \"Prefix\": norm_prefix(prefix)}\n",
    "    if delimiter:\n",
    "        kwargs[\"Delimiter\"] = delimiter\n",
    "    keys, folders = [], []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(**kwargs):\n",
    "        for obj in page.get(\"Contents\", []) or []:\n",
    "            keys.append(obj[\"Key\"])\n",
    "        for cp in page.get(\"CommonPrefixes\", []) or []:\n",
    "            folders.append(cp[\"Prefix\"])\n",
    "    return keys, folders\n",
    "\n",
    "\n",
    "def s3_list_immediate_folders(bucket: str, base_prefix: str):\n",
    "    _, folders = s3_list_objects(bucket, norm_prefix(base_prefix), delimiter=\"/\")\n",
    "    return sorted({f.split(\"/\")[-2] for f in folders}) if folders else []\n",
    "\n",
    "\n",
    "def s3_sync_docs_to_local(bucket: str, prefix_docs: str, local_folder: str):\n",
    "    s3 = s3_client_autoregion(bucket)\n",
    "    prefix_docs = norm_prefix(prefix_docs)\n",
    "    Path(local_folder).mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix_docs):\n",
    "        for obj in page.get(\"Contents\", []) or []:\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\"/\"):\n",
    "                continue\n",
    "            rel = key[len(prefix_docs):]\n",
    "            out = Path(local_folder) / rel\n",
    "            out.parent.mkdir(parents=True, exist_ok=True)\n",
    "            s3.download_file(bucket, key, str(out))\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def s3_sync_local_docs_to_s3(bucket: str, prefix_docs: str, local_folder: str):\n",
    "    s3 = s3_client_autoregion(bucket)\n",
    "    prefix_docs = norm_prefix(prefix_docs)\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(local_folder):\n",
    "        for name in files:\n",
    "            full = Path(root) / name\n",
    "            rel = Path(full).relative_to(local_folder).as_posix()\n",
    "            key = prefix_docs + rel\n",
    "            s3.upload_file(str(full), bucket, key)\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "print(\"✔ Helpers S3 OK\")\n",
    "\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"Eres un asistente para preguntas y respuestas basado en archivos cargados por el usuario. \"\n",
    "    \"Responde SOLO con la información del contexto. \"\n",
    "    \"Si la respuesta no está en el contexto di: 'No encuentro esa información en mis archivos'. \"\n",
    "    \"Responde en español.\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_context(hits):\n",
    "    lines = []\n",
    "    for score, ch in hits:\n",
    "        snippet = (ch.text[:350] + \"…\") if len(ch.text) > 350 else ch.text\n",
    "        lines.append(f\"[{ch.source_name} | score={score:.3f}] {snippet}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    DEFAULT_PROVIDER = \"openai\"\n",
    "    FALLBACK_PROVIDER = \"fragmento\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._oai = None\n",
    "        self._gem = None\n",
    "        self.provider = self.DEFAULT_PROVIDER\n",
    "        self.temperature = 0.2\n",
    "        self.max_tokens = 400\n",
    "        self.openai_model = \"gpt-4o-mini\"\n",
    "        self.gemini_model = \"gemini-1.5-flash\"\n",
    "        self.effective_provider = self.DEFAULT_PROVIDER\n",
    "        self._cached_provider = None\n",
    "        self._cached_openai_model = None\n",
    "        self._cached_gemini_model = None\n",
    "        self.configure(persist=False)\n",
    "\n",
    "    def configure(self, provider: Optional[str] = None, temperature: Optional[float] = None,\n",
    "                  max_tokens: Optional[int] = None, openai_model: Optional[str] = None,\n",
    "                  gemini_model: Optional[str] = None, persist: bool = True) -> Dict[str, Any]:\n",
    "        if provider is None:\n",
    "            provider = os.getenv(\"LLM_PROVIDER\", self.provider)\n",
    "        provider = (provider or \"\").strip().lower()\n",
    "        if provider not in {\"openai\", \"gemini\"}:\n",
    "            provider = self.FALLBACK_PROVIDER\n",
    "        self.provider = provider\n",
    "\n",
    "        if temperature is None:\n",
    "            temperature = os.getenv(\"LLM_TEMPERATURE\", self.temperature)\n",
    "        self.temperature = float(temperature)\n",
    "\n",
    "        if max_tokens is None:\n",
    "            max_tokens = os.getenv(\"LLM_MAX_TOKENS\", self.max_tokens)\n",
    "        self.max_tokens = int(max_tokens)\n",
    "\n",
    "        if openai_model is None:\n",
    "            openai_model = os.getenv(\"OPENAI_MODEL\", self.openai_model)\n",
    "        self.openai_model = (openai_model or \"gpt-4o-mini\").strip()\n",
    "\n",
    "        if gemini_model is None:\n",
    "            gemini_model = os.getenv(\"GEMINI_MODEL\", self.gemini_model)\n",
    "        self.gemini_model = (gemini_model or \"gemini-1.5-flash\").strip()\n",
    "\n",
    "        if persist:\n",
    "            os.environ[\"LLM_PROVIDER\"] = self.provider\n",
    "            os.environ[\"LLM_TEMPERATURE\"] = str(self.temperature)\n",
    "            os.environ[\"LLM_MAX_TOKENS\"] = str(self.max_tokens)\n",
    "            os.environ[\"OPENAI_MODEL\"] = self.openai_model\n",
    "            os.environ[\"GEMINI_MODEL\"] = self.gemini_model\n",
    "\n",
    "        if self.provider != self._cached_provider:\n",
    "            self._oai = None\n",
    "            self._gem = None\n",
    "        if self.openai_model != self._cached_openai_model:\n",
    "            self._oai = None\n",
    "        if self.gemini_model != self._cached_gemini_model:\n",
    "            self._gem = None\n",
    "\n",
    "        self._cached_provider = self.provider\n",
    "        self._cached_openai_model = self.openai_model\n",
    "        self._cached_gemini_model = self.gemini_model\n",
    "\n",
    "        return self.status()\n",
    "\n",
    "    def status(self) -> Dict[str, Any]:\n",
    "        has_openai = bool(os.getenv(\"OPENAI_API_KEY\"))\n",
    "        has_gemini = bool(os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\"))\n",
    "        available: List[str] = []\n",
    "        if has_openai:\n",
    "            available.append(\"openai\")\n",
    "        if has_gemini:\n",
    "            available.append(\"gemini\")\n",
    "        if not available:\n",
    "            available = [self.FALLBACK_PROVIDER]\n",
    "        effective = self.provider\n",
    "        if effective not in {\"openai\", \"gemini\"}:\n",
    "            effective = self.FALLBACK_PROVIDER\n",
    "        elif effective not in available:\n",
    "            effective = self.FALLBACK_PROVIDER\n",
    "        self.effective_provider = effective\n",
    "        return {\n",
    "            \"provider\": effective,\n",
    "            \"configured_provider\": self.provider,\n",
    "            \"available\": available,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"openai_model\": self.openai_model,\n",
    "            \"gemini_model\": self.gemini_model,\n",
    "            \"has_openai_key\": has_openai,\n",
    "            \"has_gemini_key\": has_gemini,\n",
    "        }\n",
    "\n",
    "    def _ensure_openai(self):\n",
    "        key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not key:\n",
    "            return None\n",
    "        if self._oai is None:\n",
    "            from openai import OpenAI\n",
    "            self._oai = OpenAI(api_key=key)\n",
    "        return self._oai\n",
    "\n",
    "    def _ensure_gemini(self):\n",
    "        key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not key:\n",
    "            return None\n",
    "        if self._gem is None:\n",
    "            import google.generativeai as genai\n",
    "            genai.configure(api_key=key)\n",
    "            self._gem = genai.GenerativeModel(self.gemini_model)\n",
    "        return self._gem\n",
    "\n",
    "    def _fallback_answer(self, hits) -> str:\n",
    "        if hits:\n",
    "            snippet = (hits[0][1].text or \"\").strip()\n",
    "            snippet = snippet[:800]\n",
    "            if snippet:\n",
    "                return \"⚠️ Sin LLM: fragmento más relevante:\\n\\n\" + snippet\n",
    "        return \"No encuentro esa información en mis archivos.\"\n",
    "\n",
    "    def generate(self, question: str, hits: Optional[List] = None,\n",
    "                 temperature: Optional[float] = None, max_tokens: Optional[int] = None) -> str:\n",
    "        hits = hits or []\n",
    "        context = format_context(hits) if hits else \"(sin fragmentos relevantes)\"\n",
    "        temp = float(self.temperature if temperature is None else temperature)\n",
    "        mtok = int(self.max_tokens if max_tokens is None else max_tokens)\n",
    "        provider = self.status()[\"provider\"]\n",
    "\n",
    "        if provider == \"openai\":\n",
    "            cli = self._ensure_openai()\n",
    "            if cli:\n",
    "                try:\n",
    "                    msgs = [\n",
    "                        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": (\n",
    "                                f\"Pregunta: {question}\\n\\nContexto:\\n{context}\\n\\nResponde SOLO con lo anterior.\"\n",
    "                            ),\n",
    "                        },\n",
    "                    ]\n",
    "                    resp = cli.chat.completions.create(\n",
    "                        model=self.openai_model,\n",
    "                        messages=msgs,\n",
    "                        temperature=temp,\n",
    "                        max_tokens=mtok,\n",
    "                    )\n",
    "                    return (resp.choices[0].message.content or \"\").strip()\n",
    "                except Exception as exc:\n",
    "                    return f\"⚠️ Error OpenAI: {exc}\"\n",
    "        elif provider == \"gemini\":\n",
    "            model = self._ensure_gemini()\n",
    "            if model:\n",
    "                try:\n",
    "                    prompt = (\n",
    "                        f\"{SYS_PROMPT}\\n\\nPregunta: {question}\\n\\nContexto:\\n{context}\\n\\nResponde SOLO con lo anterior.\"\n",
    "                    )\n",
    "                    out = model.generate_content(prompt)\n",
    "                    return (getattr(out, \"text\", \"\") or \"\").strip()\n",
    "                except Exception as exc:\n",
    "                    return f\"⚠️ Error Gemini: {exc}\"\n",
    "        return self._fallback_answer(hits)\n",
    "\n",
    "\n",
    "LLM = LLMClient()\n",
    "\n",
    "def collect_sources(hits):\n",
    "    return sorted({getattr(h[1], \"source_name\", \"\") for h in hits if getattr(h[1], \"source_name\", \"\")})\n",
    "\n",
    "\n",
    "def rag_answer(question: str, top_k: int = 4, temperature: Optional[float] = None,\n",
    "               max_tokens: Optional[int] = None, with_sources: bool = True):\n",
    "    hits = retrieve(question, top_k=top_k) if 'retrieve' in globals() else []\n",
    "    if not hits:\n",
    "        return \"Primero crea/carga un índice.\", []\n",
    "    answer = LLM.generate(question, hits=hits, temperature=temperature, max_tokens=max_tokens)\n",
    "    sources = collect_sources(hits)\n",
    "    if with_sources and sources and \"Fuentes\" not in answer:\n",
    "        answer = answer.rstrip() + \"\\n\\nFuentes: \" + \", \".join(sources)\n",
    "    return answer, sources\n"
   ],
   "id": "1xLVWVFv7tlC"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFPXZY_T7tlE"
   },
   "source": [
    "## 3) Núcleo RAG (loaders → chunking → FAISS → retrieval)"
   ],
   "id": "JFPXZY_T7tlE"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMhbAOdH7tlG",
    "outputId": "32a06d9b-2a24-4ebf-ee5d-576c35d92d10",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295239640,
     "user_tz": 300,
     "elapsed": 39870,
     "user": {
      "displayName": "Engler González",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, json, uuid, shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, faiss, time\n",
    "\n",
    "BASE_DIR = Path.cwd() / \"mini_chatbot_work\"\n",
    "DOCS_DIR = BASE_DIR / \"docs_raw\"\n",
    "INDEX_DIR = BASE_DIR / \"faiss_index\"\n",
    "LOGS_DIR = BASE_DIR / \"logs\"\n",
    "META_PATH = BASE_DIR / \"docs_metadata.json\"\n",
    "for path in [BASE_DIR, DOCS_DIR, INDEX_DIR, LOGS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_txt(path: Path) -> str:\n",
    "    try:\n",
    "        return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception as exc:\n",
    "        return f\"[ERROR TXT] {exc}\"\n",
    "\n",
    "\n",
    "def load_pdf(path: Path) -> str:\n",
    "    try:\n",
    "        reader = PdfReader(str(path))\n",
    "        return \"\\n\".join((page.extract_text() or \"\") for page in reader.pages)\n",
    "    except Exception as exc:\n",
    "        return f\"[ERROR PDF] {exc}\"\n",
    "\n",
    "\n",
    "def load_csv(path: Path, n: int = 1500) -> str:\n",
    "    try:\n",
    "        df = pd.read_csv(path, nrows=n)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, nrows=n, encoding=\"latin-1\")\n",
    "    preview = df.head(20).to_markdown(index=False)\n",
    "    return \"\\n\".join([\n",
    "        f\"# CSV: {path.name}\",\n",
    "        f\"Columnas: {list(df.columns)}\",\n",
    "        \"Muestra:\\n\" + preview,\n",
    "    ])\n",
    "\n",
    "\n",
    "def load_any(path: Path) -> str:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext in {\".txt\", \".md\"}:\n",
    "        return load_txt(path)\n",
    "    if ext == \".pdf\":\n",
    "        return load_pdf(path)\n",
    "    if ext == \".csv\":\n",
    "        return load_csv(path)\n",
    "    return f\"[BINARIO] {path.name} (no indexado)\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChunkedDoc:\n",
    "    doc_id: str\n",
    "    source_name: str\n",
    "    chunk_id: int\n",
    "    text: str\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 800, overlap: int = 150) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    if not tokens:\n",
    "        return []\n",
    "    pieces: List[str] = []\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        pieces.append(\" \".join(tokens[i : i + chunk_size]))\n",
    "        i += step\n",
    "    return pieces\n",
    "\n",
    "\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "_emb = None\n",
    "_index = None\n",
    "_chunks: List[ChunkedDoc] = []\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    global _emb\n",
    "    if _emb is None:\n",
    "        _emb = SentenceTransformer(EMB_MODEL_NAME)\n",
    "    return _emb\n",
    "\n",
    "\n",
    "def build_index_from_local(paths: List[Path], chunk_size: int = 800, overlap: int = 150):\n",
    "    global _index, _chunks\n",
    "    metas = []\n",
    "    chunks: List[ChunkedDoc] = []\n",
    "    for path in paths:\n",
    "        path = Path(path)\n",
    "        if not path.is_file():\n",
    "            continue\n",
    "        raw = load_any(path)\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        metas.append({\"doc_id\": doc_id, \"source_name\": path.name, \"path\": str(path)})\n",
    "        for idx, chunk in enumerate(chunk_text(raw, chunk_size, overlap)):\n",
    "            if not chunk.strip():\n",
    "                continue\n",
    "            chunks.append(ChunkedDoc(doc_id=doc_id, source_name=path.name, chunk_id=idx, text=chunk))\n",
    "    if not chunks:\n",
    "        _index = None\n",
    "        _chunks = []\n",
    "        return 0, len(metas)\n",
    "\n",
    "    embeddings = get_model().encode(\n",
    "        [chunk.text for chunk in chunks],\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    ).astype(np.float32)\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    _index = index\n",
    "    _chunks = chunks\n",
    "\n",
    "    INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    faiss.write_index(_index, str(INDEX_DIR / \"faiss.index\"))\n",
    "    json.dump([chunk.__dict__ for chunk in chunks], open(INDEX_DIR / \"chunks.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False)\n",
    "    json.dump(metas, open(META_PATH, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "    return len(chunks), len(metas)\n",
    "\n",
    "\n",
    "def load_index_local() -> bool:\n",
    "    global _index, _chunks\n",
    "    fidx = INDEX_DIR / \"faiss.index\"\n",
    "    fch = INDEX_DIR / \"chunks.json\"\n",
    "    if not (fidx.exists() and fch.exists()):\n",
    "        return False\n",
    "    _index = faiss.read_index(str(fidx))\n",
    "    data = json.load(open(fch, \"r\", encoding=\"utf-8\"))\n",
    "    _chunks = [ChunkedDoc(**entry) for entry in data]\n",
    "    return True\n",
    "\n",
    "\n",
    "def retrieve(question: str, top_k: int = 4) -> List[Tuple[float, ChunkedDoc]]:\n",
    "    if _index is None or not _chunks:\n",
    "        return []\n",
    "    query = get_model().encode([question], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)\n",
    "    distances, indices = _index.search(query, top_k)\n",
    "    results: List[Tuple[float, ChunkedDoc]] = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        results.append((float(score), _chunks[idx]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def log_event(chat_id: str, role: str, text: str):\n",
    "    path = LOGS_DIR / f\"{chat_id}.jsonl\"\n",
    "    record = {\"t\": time.time(), \"role\": role, \"text\": text}\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as handle:\n",
    "        handle.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"✔ Núcleo RAG OK\")\n"
   ],
   "id": "tMhbAOdH7tlG"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1FQRyyC7tlH"
   },
   "source": [
    "## 4) LLM wrapper (OpenAI o Gemini)"
   ],
   "id": "b1FQRyyC7tlH"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_--Ff4FV7tlI"
   },
   "source": [
    "### (Se reemplazó por la celda anterior con la configuración unificada del LLM)."
   ],
   "id": "_--Ff4FV7tlI"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "144S4eh77tlK"
   },
   "source": [
    "## 5) Cargar/Guardar índice en S3 + reconstrucción desde docs en S3"
   ],
   "id": "144S4eh77tlK"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I-8KWQb87tlL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295239673,
     "user_tz": 300,
     "elapsed": 3,
     "user": {
      "displayName": "Engler González",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "def effective_team_prefix(base_prefix: str, team_folder: str):\n",
    "    return norm_prefix(base_prefix) + norm_prefix(team_folder)\n",
    "\n",
    "\n",
    "def s3_upload_index(bucket: str, base_prefix: str, team_folder: str):\n",
    "    idx_prefix_root = effective_team_prefix(base_prefix, team_folder)\n",
    "    idx_prefix_sub = idx_prefix_root + \"index/\"\n",
    "    if not (INDEX_DIR / \"faiss.index\").exists() or not (INDEX_DIR / \"chunks.json\").exists():\n",
    "        return \"❌ No hay índice local (faiss.index / chunks.json). Construye primero.\"\n",
    "    s3 = s3_client_autoregion(bucket)\n",
    "    for target_prefix in [idx_prefix_root, idx_prefix_sub]:\n",
    "        for name in [\"faiss.index\", \"chunks.json\"]:\n",
    "            s3.upload_file(str(INDEX_DIR / name), bucket, f\"{target_prefix}{name}\")\n",
    "        if META_PATH.exists():\n",
    "            s3.upload_file(str(META_PATH), bucket, f\"{target_prefix}docs_metadata.json\")\n",
    "    return f\"☁️ Subido a: s3://{bucket}/{idx_prefix_root}  y  s3://{bucket}/{idx_prefix_sub}\"\n",
    "\n",
    "\n",
    "def _download_index_from_prefix(bucket: str, prefix: str):\n",
    "    s3 = s3_client_autoregion(bucket)\n",
    "    found = set()\n",
    "    wanted = [\"faiss.index\", \"chunks.json\", \"docs_metadata.json\"]\n",
    "    keys, _ = s3_list_objects(bucket, prefix)\n",
    "    for key in keys:\n",
    "        base = key.split(\"/\")[-1]\n",
    "        if base in wanted:\n",
    "            out = INDEX_DIR / base if base != \"docs_metadata.json\" else META_PATH\n",
    "            out.parent.mkdir(parents=True, exist_ok=True)\n",
    "            s3.download_file(bucket, key, str(out))\n",
    "            found.add(base)\n",
    "    return found\n",
    "\n",
    "\n",
    "def s3_download_index(bucket: str, base_prefix: str, team_folder: str):\n",
    "    idx_prefix_root = effective_team_prefix(base_prefix, team_folder)\n",
    "    idx_prefix_sub = idx_prefix_root + \"index/\"\n",
    "    found = _download_index_from_prefix(bucket, idx_prefix_root)\n",
    "    if not {\"faiss.index\", \"chunks.json\"}.issubset(found):\n",
    "        found = _download_index_from_prefix(bucket, idx_prefix_sub)\n",
    "    if {\"faiss.index\", \"chunks.json\"}.issubset(found):\n",
    "        ok = load_index_local()\n",
    "        return \"📥 Índice cargado.\" if ok else \"❌ Descargado pero falló carga local.\"\n",
    "    return f\"❌ No encontré índice en {idx_prefix_root} ni {idx_prefix_sub}.\"\n",
    "\n",
    "\n",
    "def s3_rebuild_from_docs(bucket: str, base_prefix: str, team_folder: str, chunk_size=800, overlap=150):\n",
    "    docs_prefix = effective_team_prefix(base_prefix, team_folder) + \"docs/\"\n",
    "    count = s3_sync_docs_to_local(bucket, docs_prefix, str(DOCS_DIR))\n",
    "    if count == 0:\n",
    "        return \"❌ No hay documentos en S3 (carpeta 'docs/'). Sube alguno primero.\"\n",
    "    n_chunks, n_docs = build_index_from_local(list(Path(DOCS_DIR).glob('*')), chunk_size, overlap)\n",
    "    return f\"✅ Reconstruido desde S3: {n_docs} docs → {n_chunks} chunks.\"\n",
    "\n",
    "\n",
    "def s3_upload_local_docs(bucket: str, base_prefix: str, team_folder: str):\n",
    "    docs_prefix = effective_team_prefix(base_prefix, team_folder) + \"docs/\"\n",
    "    count = s3_sync_local_docs_to_s3(bucket, docs_prefix, str(DOCS_DIR))\n",
    "    return f\"☁️ Subidos {count} archivo(s) a s3://{bucket}/{docs_prefix}\"\n",
    "\n",
    "\n",
    "def s3_download_docs(bucket: str, base_prefix: str, team_folder: str):\n",
    "    docs_prefix = effective_team_prefix(base_prefix, team_folder) + \"docs/\"\n",
    "    count = s3_sync_docs_to_local(bucket, docs_prefix, str(DOCS_DIR))\n",
    "    return f\"📥 Descargados {count} archivo(s) a {DOCS_DIR}\"\n"
   ],
   "id": "I-8KWQb87tlL"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt8bfMkl7tlM"
   },
   "source": [
    "## 6) Telegram: bot con polling, auto-responder y panel admin"
   ],
   "id": "tt8bfMkl7tlM"
  },
  {
   "cell_type": "code",
   "source": [
    "#import os\n",
    "#os.environ[\"TELEGRAM_BOT_TOKEN\"] = \"7526191718:AAEM1HYYoKmXjmw-VkTI0kzI0bTiKBvT4i0\"\n",
    "#TELEGRAM_BOT_TOKEN = os.environ[\"TELEGRAM_BOT_TOKEN\"]"
   ],
   "metadata": {
    "id": "fdMAXAH8fREZ"
   },
   "id": "fdMAXAH8fREZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jn48XVDL7tlM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295239708,
     "user_tz": 300,
     "elapsed": 5,
     "user": {
      "displayName": "Engler González",
      "userId": "10704175225987180059"
     }
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, re, json, time, threading, requests\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "\n",
    "def collect_sources(hits):\n",
    "    return sorted({getattr(hit[1], \"source_name\", \"\") for hit in hits if getattr(hit[1], \"source_name\", \"\")})\n",
    "\n",
    "\n",
    "def _tt_answer_logic(question: str, for_telegram: bool = False) -> str:\n",
    "    question = (question or \"\").strip()\n",
    "    if not question:\n",
    "        return \"Envíame un texto y responderé con lo que haya en tus archivos.\"\n",
    "\n",
    "    fallback_error = None\n",
    "\n",
    "    if 'responder' in globals():\n",
    "        try:\n",
    "            reply = responder(question)\n",
    "            if for_telegram:\n",
    "                reply = re.sub(r\"\\n\\s*Fuentes\\s*:.*$\", \"\", reply, flags=re.S)\n",
    "            return reply\n",
    "        except Exception as exc:\n",
    "            fallback_error = f\"⚠️ Error en responder(): {exc}\"\n",
    "\n",
    "    try:\n",
    "        temp_env = os.getenv(\"LLM_TEMPERATURE\")\n",
    "        temperature = float(temp_env) if temp_env is not None else None\n",
    "    except Exception:\n",
    "        temperature = None\n",
    "    try:\n",
    "        max_tok_env = os.getenv(\"LLM_MAX_TOKENS\")\n",
    "        max_tokens = int(max_tok_env) if max_tok_env is not None else None\n",
    "    except Exception:\n",
    "        max_tokens = None\n",
    "\n",
    "    if 'rag_answer' in globals():\n",
    "        try:\n",
    "            answer, _ = rag_answer(\n",
    "                question,\n",
    "                top_k=4,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                with_sources=not for_telegram,\n",
    "            )\n",
    "            return answer\n",
    "        except Exception as exc:\n",
    "            fallback_error = f\"⚠️ Error en rag_answer(): {exc}\"\n",
    "\n",
    "    hits = []\n",
    "    if 'retrieve' in globals():\n",
    "        try:\n",
    "            hits = retrieve(question, top_k=4)\n",
    "        except Exception:\n",
    "            hits = []\n",
    "    answer = LLM.generate(question, hits=hits, temperature=temperature, max_tokens=max_tokens)\n",
    "    if not for_telegram and hits:\n",
    "        sources = collect_sources(hits)\n",
    "        if sources and \"Fuentes\" not in answer:\n",
    "            answer = answer.rstrip() + \"\\n\\nFuentes: \" + \", \".join(sources)\n",
    "    if fallback_error and \"⚠️\" not in answer:\n",
    "        answer = answer.rstrip() + \"\\n\\n\" + fallback_error\n",
    "    return answer or \"No encuentro esa información en mis archivos.\"\n",
    "\n",
    "\n",
    "class TTGram:\n",
    "    HOLD_AFTER_ADMIN = 60\n",
    "    HOLD_AFTER_ALERT = 120\n",
    "    AUTO_CLOSE_AFTER = 600\n",
    "    ALERT_REGEX = re.compile(r\"\\b(asesor|ayuda)\\b\", re.I)\n",
    "\n",
    "    def __init__(self):\n",
    "        base_token = (\n",
    "            os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "            or (userdata.get(\"TELEGRAM_BOT_TOKEN\") if 'userdata' in globals() else None)\n",
    "            or \"\"\n",
    "        )\n",
    "        self.token = base_token.strip()\n",
    "        self.offset = None\n",
    "        self.global_auto = True\n",
    "        self.stop = threading.Event()\n",
    "        self.thread = None\n",
    "        self.known: Dict[str, Dict[str, Any]] = {}\n",
    "        self.admin_chat_id = (os.getenv(\"ADMIN_CHAT_ID\") or \"\").strip()\n",
    "        self.logs = LOGS_DIR if 'LOGS_DIR' in globals() else Path.cwd() / \"mini_chatbot_work\" / \"logs\"\n",
    "        self.logs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def shutdown(self):\n",
    "        \"\"\"Detiene hilos/polling activos para evitar duplicados al re-ejecutar la celda.\"\"\"\n",
    "        self.stop.set()\n",
    "        thread = getattr(self, \"thread\", None)\n",
    "        if thread and thread.is_alive():\n",
    "            try:\n",
    "                thread.join(timeout=2)\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.thread = None\n",
    "\n",
    "    # --------------- core helpers ---------------\n",
    "    def _known_meta(self, chat_id: str) -> Dict[str, Any]:\n",
    "        return self.known.setdefault(chat_id, {\n",
    "            \"title\": \"\",\n",
    "            \"last_text\": \"\",\n",
    "            \"auto\": None,\n",
    "            \"hold_until\": 0.0,\n",
    "        })\n",
    "\n",
    "    def _update_known(self, chat_id: str, title: str = \"\") -> Dict[str, Any]:\n",
    "        meta = self._known_meta(chat_id)\n",
    "        if title and not meta.get(\"title\"):\n",
    "            meta[\"title\"] = title\n",
    "        return meta\n",
    "\n",
    "    def _log(self, chat_id: str, role: str, text: str):\n",
    "        record = {\"t\": time.time(), \"role\": role, \"text\": text}\n",
    "        try:\n",
    "            with open(self.logs / f\"{chat_id}.jsonl\", \"a\", encoding=\"utf-8\") as handle:\n",
    "                handle.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _send(self, chat_id: str, text: str):\n",
    "        if not self.token:\n",
    "            raise RuntimeError(\"No TELEGRAM_BOT_TOKEN configurado.\")\n",
    "        response = requests.post(\n",
    "            f\"https://api.telegram.org/bot{self.token}/sendMessage\",\n",
    "            json={\"chat_id\": chat_id, \"text\": text},\n",
    "            timeout=15,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    def _send_silent(self, chat_id: str, text: str) -> bool:\n",
    "        try:\n",
    "            self._send(chat_id, text)\n",
    "            return True\n",
    "        except Exception as exc:\n",
    "            self._log(chat_id, \"error\", f\"send fail: {exc}\")\n",
    "            return False\n",
    "\n",
    "    def _mirror_to_admin(self, message: str, origin_chat: Optional[str] = None):\n",
    "        admin = getattr(self, \"admin_chat_id\", \"\")\n",
    "        if not admin or admin == origin_chat:\n",
    "            return\n",
    "        self._send_silent(admin, message)\n",
    "\n",
    "    def _set_hold(self, chat_id: str, seconds: float):\n",
    "        meta = self._known_meta(chat_id)\n",
    "        meta[\"hold_until\"] = time.time() + float(seconds)\n",
    "\n",
    "    def _set_auto_chat(self, chat_id: str, flag: bool):\n",
    "        meta = self._known_meta(chat_id)\n",
    "        meta[\"auto\"] = bool(flag)\n",
    "        meta[\"hold_until\"] = 0.0\n",
    "        meta[\"awaiting_admin\"] = False\n",
    "        meta[\"closed_notified\"] = False\n",
    "\n",
    "    def _should_auto(self, chat_id: str) -> bool:\n",
    "        meta = self._known_meta(chat_id)\n",
    "        if meta.get(\"awaiting_admin\"):\n",
    "            return False\n",
    "        hold_until = meta.get(\"hold_until\", 0)\n",
    "        if hold_until and time.time() < hold_until:\n",
    "            return False\n",
    "        auto = meta.get(\"auto\")\n",
    "        if auto is None:\n",
    "            auto = self.global_auto\n",
    "        return bool(auto)\n",
    "\n",
    "    def _delete_webhook(self):\n",
    "        if not self.token:\n",
    "            return\n",
    "        try:\n",
    "            requests.get(f\"https://api.telegram.org/bot{self.token}/deleteWebhook\", timeout=10)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _me(self):\n",
    "        if not self.token:\n",
    "            return \"No TELEGRAM_BOT_TOKEN configurado.\"\n",
    "        try:\n",
    "            resp = requests.get(f\"https://api.telegram.org/bot{self.token}/getMe\", timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json().get(\"result\", {})\n",
    "            username = data.get(\"username\", \"(sin username)\")\n",
    "            return \"Bot: @\" + username\n",
    "        except Exception as exc:\n",
    "            return f\"Token/Bot error: {exc}\"\n",
    "\n",
    "    # --------------- admin commands ---------------\n",
    "    def _handle_admin_command(self, text: str) -> bool:\n",
    "        cid = getattr(self, \"admin_chat_id\", \"\")\n",
    "        if not cid:\n",
    "            return False\n",
    "        cmd = text.strip()\n",
    "        if cmd == \"/admin\":\n",
    "            self._send_silent(cid, \"Ya eres canal admin ✅\")\n",
    "            return True\n",
    "        if cmd.startswith(\"/auto_on\"):\n",
    "            parts = cmd.split()\n",
    "            if len(parts) >= 2:\n",
    "                self._set_auto_chat(parts[1], True)\n",
    "                self._send_silent(cid, f\"Auto ON para {parts[1]}\")\n",
    "            return True\n",
    "        if cmd.startswith(\"/auto_off\"):\n",
    "            parts = cmd.split()\n",
    "            if len(parts) >= 2:\n",
    "                self._set_auto_chat(parts[1], False)\n",
    "                self._send_silent(cid, f\"Auto OFF para {parts[1]}\")\n",
    "            return True\n",
    "        if cmd.startswith(\"/say\"):\n",
    "            parts = cmd.split(maxsplit=2)\n",
    "            if len(parts) >= 3:\n",
    "                target, message = parts[1], parts[2]\n",
    "                if self._send_silent(target, message):\n",
    "                    self._log(target, \"admin\", message)\n",
    "                self._set_hold(target, self.HOLD_AFTER_ADMIN)\n",
    "                meta = self._known_meta(target)\n",
    "                meta[\"awaiting_admin\"] = False\n",
    "                meta[\"closed_notified\"] = False\n",
    "                self._mirror_to_admin(f\"[ADMIN→{target}] {message}\", origin_chat=cid)\n",
    "            return True\n",
    "        if cmd.startswith(\"/close\"):\n",
    "            parts = cmd.split(maxsplit=1)\n",
    "            if len(parts) >= 2:\n",
    "                self.close_chat(parts[1], notify_user=True)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # --------------- public API ---------------\n",
    "    def set_token(self, token: str):\n",
    "        token = (token or self.token or os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "                 or (userdata.get(\"TELEGRAM_BOT_TOKEN\") if 'userdata' in globals() else \"\")).strip()\n",
    "        if token.upper().startswith(\"BOT:\"):\n",
    "            token = token.split(\":\", 1)[1].strip()\n",
    "        self.token = token\n",
    "        if token:\n",
    "            os.environ[\"TELEGRAM_BOT_TOKEN\"] = token\n",
    "            if 'userdata' in globals():\n",
    "                try:\n",
    "                    userdata[\"TELEGRAM_BOT_TOKEN\"] = token\n",
    "                except Exception:\n",
    "                    pass\n",
    "        self._delete_webhook()\n",
    "        return self._me()\n",
    "\n",
    "    def toggle_poll(self, flag: bool):\n",
    "        if flag:\n",
    "            if self.thread and self.thread.is_alive():\n",
    "                return \"Auto-escuchar: ya activo.\"\n",
    "            self.stop.clear()\n",
    "            self._delete_webhook()\n",
    "            self.thread = threading.Thread(target=self._loop, daemon=True)\n",
    "            self.thread.start()\n",
    "            return \"Auto-escuchar: ON\"\n",
    "        self.stop.set()\n",
    "        thread = getattr(self, \"thread\", None)\n",
    "        if thread and thread.is_alive():\n",
    "            try:\n",
    "                thread.join(timeout=2)\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.thread = None\n",
    "        return \"Auto-escuchar: OFF\"\n",
    "\n",
    "    def poll_once(self):\n",
    "        if not self.token:\n",
    "            return \"Sin TELEGRAM_BOT_TOKEN\"\n",
    "        try:\n",
    "            params = {\"timeout\": 10}\n",
    "            if self.offset is not None:\n",
    "                params[\"offset\"] = self.offset\n",
    "            resp = requests.get(\n",
    "                f\"https://api.telegram.org/bot{self.token}/getUpdates\",\n",
    "                params=params,\n",
    "                timeout=15,\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            for item in data.get(\"result\", []):\n",
    "                self.offset = item[\"update_id\"] + 1\n",
    "                self.handle_update(item)\n",
    "            return f\"OK, updates: {len(data.get('result', []))}\"\n",
    "        except Exception as exc:\n",
    "            return f\"❌ Poll error: {exc}\"\n",
    "\n",
    "    def poll_now(self):\n",
    "        return self.poll_once()\n",
    "\n",
    "    def _loop(self):\n",
    "        while not self.stop.is_set():\n",
    "            self.poll_once()\n",
    "            self._check_auto_close()\n",
    "            for _ in range(10):\n",
    "                if self.stop.is_set():\n",
    "                    break\n",
    "                time.sleep(0.3)\n",
    "\n",
    "    def _check_auto_close(self):\n",
    "        now = time.time()\n",
    "        for cid, meta in list(self.known.items()):\n",
    "            alert_ts = meta.get(\"alert_ts\")\n",
    "            awaiting = meta.get(\"awaiting_admin\", False)\n",
    "            closed = meta.get(\"closed_notified\", False)\n",
    "            if awaiting and alert_ts and not closed and now - alert_ts > self.AUTO_CLOSE_AFTER:\n",
    "                message = (\n",
    "                    \"Cierro esta conversación por inactividad. Si necesitas más ayuda, \"\n",
    "                    \"escríbeme de nuevo y retomaré la consulta.\"\n",
    "                )\n",
    "                if self._send_silent(cid, message):\n",
    "                    self._log(cid, \"bot\", message)\n",
    "                meta[\"awaiting_admin\"] = False\n",
    "                meta[\"closed_notified\"] = True\n",
    "                meta[\"hold_until\"] = 0.0\n",
    "                meta[\"auto\"] = self.global_auto\n",
    "                if self.admin_chat_id:\n",
    "                    self._send_silent(self.admin_chat_id, f\"ℹ️ Conversación {cid} cerrada por inactividad.\")\n",
    "\n",
    "    def list_chats(self):\n",
    "        if not self.known:\n",
    "            return \"(sin chats aún — envía /start al bot y pulsa 'Leer ahora')\"\n",
    "        now = time.time()\n",
    "        lines = []\n",
    "        for cid, meta in self.known.items():\n",
    "            hold_remaining = max(0, int(meta.get(\"hold_until\", 0) - now))\n",
    "            auto = meta.get(\"auto\")\n",
    "            if auto is None:\n",
    "                auto = self.global_auto\n",
    "            state = \"esperando asesor\" if meta.get(\"awaiting_admin\") else \"activo\"\n",
    "            lines.append(\n",
    "                f\"{cid} | {meta.get('title','')} | auto={'ON' if auto else 'OFF'} | hold={hold_remaining}s | {state} | último: {meta.get('last_text','')[:60]}\"\n",
    "            )\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def admin_send(self, chat_id: str, text: str):\n",
    "        chat_id = (chat_id or \"\").strip()\n",
    "        if not chat_id:\n",
    "            return \"Selecciona un chat.\"\n",
    "        message = text or \"(mensaje vacío)\"\n",
    "        try:\n",
    "            self._send(chat_id, message)\n",
    "            self._log(chat_id, \"admin\", message)\n",
    "            meta = self._known_meta(chat_id)\n",
    "            meta[\"last_text\"] = message\n",
    "            meta[\"awaiting_admin\"] = False\n",
    "            meta[\"closed_notified\"] = False\n",
    "            self._set_hold(chat_id, self.HOLD_AFTER_ADMIN)\n",
    "            self._mirror_to_admin(f\"[ADMIN→{chat_id}] {message}\", origin_chat=self.admin_chat_id)\n",
    "            return \"✅ Mensaje enviado.\"\n",
    "        except Exception as exc:\n",
    "            return f\"❌ Error: {exc}\"\n",
    "\n",
    "    def auto_toggle(self, flag: bool):\n",
    "        self.global_auto = bool(flag)\n",
    "        return f\"Auto-responder (global): {'ON' if self.global_auto else 'OFF'}\"\n",
    "\n",
    "    def view_chat(self, chat_id: str, limit: int = 40):\n",
    "        chat_id = (chat_id or \"\").strip()\n",
    "        if not chat_id:\n",
    "            return \"Indica un chat_id.\"\n",
    "        path = self.logs / f\"{chat_id}.jsonl\"\n",
    "        if not path.exists():\n",
    "            return \"No hay historial para ese chat.\"\n",
    "        try:\n",
    "            raw = path.read_text(encoding=\"utf-8\")\n",
    "        except Exception as exc:\n",
    "            return f\"No se pudo leer el historial: {exc}\"\n",
    "        entries = [json.loads(line) for line in raw.splitlines() if line.strip()]\n",
    "        entries = entries[-limit:]\n",
    "        formatted = []\n",
    "        for entry in entries:\n",
    "            ts = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(entry.get(\"t\", time.time())))\n",
    "            role = entry.get(\"role\", \"\").upper()\n",
    "            text = entry.get(\"text\", \"\")\n",
    "            formatted.append(f\"{ts} | {role}: {text}\")\n",
    "        return \"\\n\".join(formatted)\n",
    "\n",
    "    def release_hold(self, chat_id: str):\n",
    "        chat_id = (chat_id or \"\").strip()\n",
    "        if not chat_id:\n",
    "            return \"Indica un chat_id.\"\n",
    "        meta = self._known_meta(chat_id)\n",
    "        meta[\"hold_until\"] = 0.0\n",
    "        meta[\"awaiting_admin\"] = False\n",
    "        meta[\"closed_notified\"] = False\n",
    "        if meta.get(\"auto\") is None:\n",
    "            meta[\"auto\"] = self.global_auto\n",
    "        return f\"Hold liberado y auto={'ON' if meta.get('auto') else 'OFF'}\"\n",
    "\n",
    "    def close_chat(self, chat_id: str, notify_user: bool = True):\n",
    "        chat_id = (chat_id or \"\").strip()\n",
    "        if not chat_id:\n",
    "            return \"Indica un chat_id.\"\n",
    "        meta = self._known_meta(chat_id)\n",
    "        closing = \"Cierro esta conversación. Si necesitas más ayuda, escríbeme nuevamente.\" if notify_user else \"Soporte cerrado.\"\n",
    "        if notify_user:\n",
    "            self._send_silent(chat_id, closing)\n",
    "            self._log(chat_id, \"bot\", closing)\n",
    "        meta[\"awaiting_admin\"] = False\n",
    "        meta[\"hold_until\"] = time.time() + 2\n",
    "        meta[\"auto\"] = self.global_auto\n",
    "        meta[\"closed_notified\"] = True\n",
    "        if self.admin_chat_id:\n",
    "            self._send_silent(self.admin_chat_id, f\"ℹ️ Conversación {chat_id} marcada como cerrada.\")\n",
    "        return \"✅ Conversación cerrada.\"\n",
    "\n",
    "    def handle_update(self, item):\n",
    "        message = item.get(\"message\") or {}\n",
    "        chat = message.get(\"chat\") or {}\n",
    "        chat_id = str(chat.get(\"id\")) if chat.get(\"id\") is not None else \"\"\n",
    "        text = (message.get(\"text\") or \"\").strip()\n",
    "        title = chat.get(\"username\") or chat.get(\"title\") or \"\"\n",
    "        if not chat_id or not text:\n",
    "            return\n",
    "\n",
    "        meta = self._update_known(chat_id, title)\n",
    "        meta[\"last_text\"] = text\n",
    "        if message.get(\"date\"):\n",
    "            meta[\"last_user_ts\"] = message[\"date\"]\n",
    "        else:\n",
    "            meta[\"last_user_ts\"] = time.time()\n",
    "\n",
    "        self._log(chat_id, \"user\", text)\n",
    "        if chat_id != getattr(self, \"admin_chat_id\", \"\"):\n",
    "            self._mirror_to_admin(f\"[USER→{chat_id}] {text}\", origin_chat=chat_id)\n",
    "\n",
    "        if chat_id == getattr(self, \"admin_chat_id\", \"\"):\n",
    "            if self._handle_admin_command(text):\n",
    "                return\n",
    "\n",
    "        if text == \"/admin\" and not getattr(self, \"admin_chat_id\", \"\"):\n",
    "            self.admin_chat_id = chat_id\n",
    "            os.environ[\"ADMIN_CHAT_ID\"] = chat_id\n",
    "            if 'userdata' in globals():\n",
    "                try:\n",
    "                    userdata[\"ADMIN_CHAT_ID\"] = chat_id\n",
    "                except Exception:\n",
    "                    pass\n",
    "            self._send_silent(chat_id, \"Este chat queda configurado como canal admin ✅\")\n",
    "            return\n",
    "\n",
    "        if self.ALERT_REGEX.search(text):\n",
    "            handoff = (\n",
    "                \"He activado soporte humano. Un asesor se unirá en breve. \"\n",
    "                \"También puedes seguir escribiendo y lo revisaré.\"\n",
    "            )\n",
    "            if self._send_silent(chat_id, handoff):\n",
    "                self._log(chat_id, \"bot\", handoff)\n",
    "            meta[\"awaiting_admin\"] = True\n",
    "            meta[\"alert_ts\"] = time.time()\n",
    "            meta[\"auto\"] = False\n",
    "            meta[\"closed_notified\"] = False\n",
    "            self._set_hold(chat_id, self.HOLD_AFTER_ALERT)\n",
    "            if getattr(self, \"admin_chat_id\", \"\"):\n",
    "                self._send_silent(\n",
    "                    self.admin_chat_id,\n",
    "                    f\"⚠️ ALERTA: {chat_id} pidió ayuda (‘{text}’). Usa /say {chat_id} <mensaje> o /auto_on {chat_id} para reanudar el bot.\",\n",
    "                )\n",
    "            return\n",
    "\n",
    "        if text.lower() == \"/start\":\n",
    "            welcome = (\n",
    "                \"Hola, soy tu asistente de Talento TECH. Escríbeme tu duda y consultaré los archivos de tu equipo.\"\n",
    "            )\n",
    "            if self._send_silent(chat_id, welcome):\n",
    "                self._log(chat_id, \"bot\", welcome)\n",
    "            return\n",
    "\n",
    "        if self._should_auto(chat_id):\n",
    "            reply = _tt_answer_logic(text, for_telegram=True)\n",
    "            if self._send_silent(chat_id, reply):\n",
    "                self._log(chat_id, \"bot\", reply)\n",
    "            self._mirror_to_admin(f\"[BOT→{chat_id}] {reply}\", origin_chat=chat_id)\n",
    "\n",
    "    def status(self):\n",
    "        data = {\n",
    "            \"token_configured\": bool(self.token),\n",
    "            \"admin_chat\": self.admin_chat_id or \"(sin configurar)\",\n",
    "            \"global_auto\": self.global_auto,\n",
    "            \"known_chats\": len(self.known),\n",
    "        }\n",
    "        return json.dumps(data, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "if \"_TTG\" in globals():\n",
    "    try:\n",
    "        _TTG.shutdown()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "_TTG = TTGram()\n",
    "\n",
    "\n",
    "def ui_set_token(token: str):\n",
    "    return _TTG.set_token(token)\n",
    "\n",
    "\n",
    "def ui_poll_toggle(flag: bool):\n",
    "    return _TTG.toggle_poll(bool(flag))\n",
    "\n",
    "\n",
    "def ui_poll_once():\n",
    "    return _TTG.poll_now()\n",
    "\n",
    "\n",
    "def ui_list_chats():\n",
    "    return _TTG.list_chats()\n",
    "\n",
    "\n",
    "def ui_admin_send(chat_id: str, text: str):\n",
    "    return _TTG.admin_send(chat_id, text)\n",
    "\n",
    "\n",
    "def ui_auto_toggle(flag: bool):\n",
    "    return _TTG.auto_toggle(bool(flag))\n",
    "\n",
    "\n",
    "def ui_view_chat(chat_id: str):\n",
    "    return _TTG.view_chat(chat_id)\n",
    "\n",
    "\n",
    "def ui_close_chat(chat_id: str):\n",
    "    return _TTG.close_chat(chat_id, notify_user=True)\n",
    "\n",
    "\n",
    "def ui_release_hold(chat_id: str):\n",
    "    return _TTG.release_hold(chat_id)\n",
    "\n",
    "\n",
    "def ui_bot_status():\n",
    "    return _TTG.status()\n"
   ],
   "id": "Jn48XVDL7tlM"
  },
  {
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def format_llm_status(status: Dict[str, Any]) -> str:\n",
    "    available = \", \".join(status.get(\"available\", []))\n",
    "    lines = [\n",
    "        f\"Proveedor activo: {status.get('provider')}\",\n",
    "        f\"Proveedor configurado: {status.get('configured_provider')}\",\n",
    "        f\"Disponibles: {available or 'fragmento'}\",\n",
    "        f\"Temperatura: {status.get('temperature')}\",\n",
    "        f\"Máx. tokens: {status.get('max_tokens')}\",\n",
    "        f\"Modelo OpenAI: {status.get('openai_model')} ({'llave ✅' if status.get('has_openai_key') else 'sin llave'})\",\n",
    "        f\"Modelo Gemini: {status.get('gemini_model')} ({'llave ✅' if status.get('has_gemini_key') else 'sin llave'})\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def ui_apply_s3(bucket, base_prefix, team_folder):\n",
    "    return apply_route(bucket, base_prefix, team_folder)\n",
    "\n",
    "\n",
    "def ui_list_teams(bucket, base_prefix):\n",
    "    bucket = (bucket or '').strip() or os.getenv(\"S3_BUCKET\")\n",
    "    base_prefix = norm_prefix(base_prefix or os.getenv(\"S3_PREFIX\") or \"\")\n",
    "    teams = s3_list_immediate_folders(bucket, base_prefix)\n",
    "    return \"\\n\".join(teams) if teams else \"(sin subcarpetas)\"\n",
    "\n",
    "\n",
    "def ui_upload(files):\n",
    "    saved = []\n",
    "    DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for name, data in files or []:\n",
    "        path = DOCS_DIR / Path(name).name\n",
    "        with open(path, \"wb\") as handle:\n",
    "            handle.write(data)\n",
    "        saved.append(path.name)\n",
    "    if not saved:\n",
    "        return \"No se cargaron archivos.\"\n",
    "    return f\"Guardados local: {saved}\"\n",
    "\n",
    "\n",
    "def ui_sync_local_to_s3():\n",
    "    return s3_upload_local_docs(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "\n",
    "\n",
    "def ui_sync_s3_to_local():\n",
    "    return s3_download_docs(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "\n",
    "\n",
    "def ui_build_index(chunk_size, overlap):\n",
    "    paths = list(Path(DOCS_DIR).glob(\"*\"))\n",
    "    if not paths:\n",
    "        return \"Sube o sincroniza documentos primero.\"\n",
    "    n_chunks, n_docs = build_index_from_local(paths, int(chunk_size), int(overlap))\n",
    "    return f\"✅ Índice local: {n_docs} docs → {n_chunks} chunks.\"\n",
    "\n",
    "\n",
    "def ui_rebuild_from_s3(chunk_size, overlap):\n",
    "    return s3_rebuild_from_docs(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"], int(chunk_size), int(overlap))\n",
    "\n",
    "\n",
    "def ui_save_index():\n",
    "    return s3_upload_index(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "\n",
    "\n",
    "def ui_load_index():\n",
    "    return s3_download_index(STATE[\"bucket\"], STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "\n",
    "\n",
    "def ui_ask(question, top_k, temp, max_tokens):\n",
    "    question = (question or \"\").strip()\n",
    "    if not question:\n",
    "        return \"Escribe una pregunta.\", \"\"\n",
    "    if not load_index_local():\n",
    "        return \"Primero carga/crea un índice.\", \"\"\n",
    "    answer, sources = rag_answer(\n",
    "        question,\n",
    "        top_k=int(top_k),\n",
    "        temperature=float(temp),\n",
    "        max_tokens=int(max_tokens),\n",
    "        with_sources=True,\n",
    "    )\n",
    "    return answer, \"\\n\".join(sorted(set(sources)))\n",
    "\n",
    "\n",
    "def ui_llm_status():\n",
    "    return format_llm_status(LLM.status())\n",
    "\n",
    "\n",
    "def ui_llm_config(provider, openai_model, gemini_model, temperature, max_tokens):\n",
    "    status = LLM.configure(\n",
    "        provider=provider,\n",
    "        openai_model=openai_model,\n",
    "        gemini_model=gemini_model,\n",
    "        temperature=float(temperature),\n",
    "        max_tokens=int(max_tokens),\n",
    "        persist=True,\n",
    "    )\n",
    "    return \"✅ Configuración guardada.\", format_llm_status(status)\n",
    "\n",
    "\n",
    "def ui_view_chat_history(chat_id):\n",
    "    return ui_view_chat(chat_id)\n",
    "\n",
    "\n",
    "def ui_close_chat_from_ui(chat_id):\n",
    "    return ui_close_chat(chat_id)\n",
    "\n",
    "\n",
    "def ui_release_hold_from_ui(chat_id):\n",
    "    return ui_release_hold(chat_id)\n",
    "\n",
    "\n",
    "def ui_bot_status_box():\n",
    "    return ui_bot_status()\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"Chatbot RAG + S3 + Telegram (Admin & equipos)\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"### 🧠 Flujo: Docs → Índice → Preguntas → Telegram (Admin)\")\n",
    "\n",
    "    with gr.Tab(\"0) S3 (Equipo)\"):\n",
    "        with gr.Row():\n",
    "            bucket = gr.Textbox(value=os.getenv(\"S3_BUCKET\"), label=\"S3_BUCKET\")\n",
    "            base_prefix = gr.Textbox(value=os.getenv(\"S3_PREFIX\"), label=\"Prefijo base (p.ej. IA-Innovador/)\")\n",
    "            team_folder = gr.Textbox(label=\"Carpeta del equipo (p.ej. Daniel)\")\n",
    "        s3_state = gr.Textbox(label=\"Estado de la ruta activa\", lines=3)\n",
    "        teams_out = gr.Textbox(label=\"Carpetas encontradas\", lines=6)\n",
    "        with gr.Row():\n",
    "            btn_apply = gr.Button(\"Aplicar ruta\")\n",
    "            btn_list = gr.Button(\"Listar carpetas existentes\")\n",
    "        btn_apply.click(ui_apply_s3, inputs=[bucket, base_prefix, team_folder], outputs=s3_state)\n",
    "        btn_list.click(ui_list_teams, inputs=[bucket, base_prefix], outputs=teams_out)\n",
    "\n",
    "    with gr.Tab(\"1) Docs\"):\n",
    "        files = gr.File(label=\"Sube PDF/TXT/CSV\", file_count=\"multiple\", type=\"binary\")\n",
    "        out_docs = gr.Textbox(label=\"Estado\", lines=3)\n",
    "        with gr.Row():\n",
    "            btn_upload = gr.Button(\"Subir a local\")\n",
    "            btn_s3_to_local = gr.Button(\"S3 → Local\")\n",
    "            btn_local_to_s3 = gr.Button(\"Local → S3\")\n",
    "        btn_upload.click(ui_upload, inputs=files, outputs=out_docs)\n",
    "        btn_s3_to_local.click(ui_sync_s3_to_local, outputs=out_docs)\n",
    "        btn_local_to_s3.click(ui_sync_local_to_s3, outputs=out_docs)\n",
    "\n",
    "    with gr.Tab(\"2) Índice\"):\n",
    "        out_index = gr.Textbox(label=\"Estado\", lines=4)\n",
    "        with gr.Row():\n",
    "            chunk_size = gr.Number(value=800, precision=0, label=\"chunk_size\")\n",
    "            overlap = gr.Number(value=150, precision=0, label=\"overlap\")\n",
    "        with gr.Row():\n",
    "            btn_build = gr.Button(\"Construir índice desde local\")\n",
    "            btn_rebuild = gr.Button(\"Reconstruir índice desde S3\")\n",
    "        btn_build.click(ui_build_index, inputs=[chunk_size, overlap], outputs=out_index)\n",
    "        btn_rebuild.click(ui_rebuild_from_s3, inputs=[chunk_size, overlap], outputs=out_index)\n",
    "\n",
    "    with gr.Tab(\"3) Persistencia\"):\n",
    "        out_persist = gr.Textbox(label=\"Estado\", lines=4)\n",
    "        with gr.Row():\n",
    "            btn_save = gr.Button(\"⬆️ Guardar índice en S3\")\n",
    "            btn_load = gr.Button(\"⬇️ Cargar índice desde S3\")\n",
    "        btn_save.click(ui_save_index, outputs=out_persist)\n",
    "        btn_load.click(ui_load_index, outputs=out_persist)\n",
    "        gr.Markdown(\"Se intenta en `.../<equipo>/` y en `.../<equipo>/index/`.\")\n",
    "\n",
    "    with gr.Tab(\"4) LLM y parámetros\"):\n",
    "        llm_status_box = gr.Textbox(value=ui_llm_status(), label=\"Estado actual\", lines=7)\n",
    "        with gr.Row():\n",
    "            provider = gr.Dropdown(\n",
    "                choices=[\"openai\", \"gemini\", \"fragmento\"],\n",
    "                value=LLM.status().get(\"configured_provider\", \"openai\"),\n",
    "                label=\"Proveedor\",\n",
    "            )\n",
    "            temperature = gr.Slider(value=LLM.temperature, minimum=0.0, maximum=1.5, step=0.05, label=\"Temperatura\")\n",
    "            max_tokens = gr.Slider(value=LLM.max_tokens, minimum=50, maximum=1200, step=50, label=\"Máx. tokens\")\n",
    "        with gr.Row():\n",
    "            openai_model = gr.Textbox(value=LLM.openai_model, label=\"Modelo OpenAI (si aplica)\")\n",
    "            gemini_model = gr.Textbox(value=LLM.gemini_model, label=\"Modelo Gemini (si aplica)\")\n",
    "        llm_feedback = gr.Textbox(label=\"Resultado\", lines=2)\n",
    "        btn_save_llm = gr.Button(\"Guardar configuración LLM\")\n",
    "        btn_refresh_llm = gr.Button(\"Actualizar estado\")\n",
    "        btn_save_llm.click(\n",
    "            ui_llm_config,\n",
    "            inputs=[provider, openai_model, gemini_model, temperature, max_tokens],\n",
    "            outputs=[llm_feedback, llm_status_box],\n",
    "        )\n",
    "        btn_refresh_llm.click(lambda: (\"\", ui_llm_status()), outputs=[llm_feedback, llm_status_box])\n",
    "\n",
    "    with gr.Tab(\"5) Preguntar\"):\n",
    "        question = gr.Textbox(label=\"Pregunta\")\n",
    "        with gr.Row():\n",
    "            top_k = gr.Slider(value=4, minimum=1, maximum=10, step=1, label=\"top_k\")\n",
    "            temp_slider = gr.Slider(value=LLM.temperature, minimum=0.0, maximum=1.2, step=0.05, label=\"Temperatura (solo esta consulta)\")\n",
    "            max_tokens_slider = gr.Slider(value=LLM.max_tokens, minimum=100, maximum=1000, step=50, label=\"Máx. tokens (solo esta consulta)\")\n",
    "        answer_box = gr.Markdown(\"Respuesta\")\n",
    "        sources_box = gr.Textbox(label=\"Fuentes\", lines=4)\n",
    "        ask_button = gr.Button(\"Consultar\")\n",
    "        ask_button.click(\n",
    "            ui_ask,\n",
    "            inputs=[question, top_k, temp_slider, max_tokens_slider],\n",
    "            outputs=[answer_box, sources_box],\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"6) Telegram (Admin)\"):\n",
    "        gr.Markdown(\n",
    "            \"Cuando un usuario escribe 'asesor' o 'ayuda', el bot pausa las respuestas automáticas, avisa al admin y espera intervención humana.\"\n",
    "        )\n",
    "        with gr.Row():\n",
    "            token_box = gr.Textbox(label=\"BOT TOKEN\", type=\"password\", value=os.getenv(\"TELEGRAM_BOT_TOKEN\", \"\"))\n",
    "            token_result = gr.Textbox(label=\"Bot\", interactive=False)\n",
    "            btn_set_token = gr.Button(\"Guardar token / Ver bot\")\n",
    "            btn_use_saved = gr.Button(\"Usar token existente\")\n",
    "        btn_set_token.click(ui_set_token, inputs=token_box, outputs=token_result)\n",
    "        btn_use_saved.click(lambda: ui_set_token(\"\"), outputs=token_result)\n",
    "\n",
    "        with gr.Row():\n",
    "            auto_checkbox = gr.Checkbox(label=\"Auto-responder (bot responde solo)\", value=True)\n",
    "            auto_state = gr.Textbox(label=\"Estado\", interactive=False)\n",
    "            auto_checkbox.change(ui_auto_toggle, inputs=auto_checkbox, outputs=auto_state)\n",
    "        with gr.Row():\n",
    "            poll_checkbox = gr.Checkbox(label=\"Polling (escuchar mensajes)\", value=False)\n",
    "            poll_state = gr.Textbox(label=\"Estado\", interactive=False)\n",
    "            poll_checkbox.change(ui_poll_toggle, inputs=poll_checkbox, outputs=poll_state)\n",
    "            btn_poll_once = gr.Button(\"Leer ahora\")\n",
    "            poll_once_state = gr.Textbox(label=\"Resultado\", interactive=False)\n",
    "            btn_poll_once.click(ui_poll_once, outputs=poll_once_state)\n",
    "\n",
    "        with gr.Row():\n",
    "            btn_list_chats = gr.Button(\"Listar chats conocidos\")\n",
    "            chats_box = gr.Textbox(label=\"Chats\", lines=8)\n",
    "            btn_list_chats.click(ui_list_chats, outputs=chats_box)\n",
    "            status_button = gr.Button(\"Estado del bot\")\n",
    "            status_box = gr.Textbox(label=\"Status\", lines=6)\n",
    "            status_button.click(lambda: (ui_bot_status_box()), outputs=status_box)\n",
    "\n",
    "        with gr.Row():\n",
    "            chat_id_box = gr.Textbox(label=\"chat_id seleccionado\")\n",
    "            history_box = gr.Textbox(label=\"Historial reciente\", lines=10)\n",
    "        with gr.Row():\n",
    "            btn_view = gr.Button(\"Ver historial\")\n",
    "            btn_release = gr.Button(\"Reanudar bot en chat\")\n",
    "            btn_close = gr.Button(\"Cerrar chat\")\n",
    "        btn_view.click(ui_view_chat_history, inputs=chat_id_box, outputs=history_box)\n",
    "        btn_release.click(ui_release_hold_from_ui, inputs=chat_id_box, outputs=history_box)\n",
    "        btn_close.click(ui_close_chat_from_ui, inputs=chat_id_box, outputs=history_box)\n",
    "\n",
    "        message_box = gr.Textbox(label=\"Mensaje del admin\", value=\"Hola, soy soporte. ¿En qué te ayudo?\")\n",
    "        send_result = gr.Textbox(label=\"Resultado\", interactive=False)\n",
    "        send_button = gr.Button(\"Enviar como admin\")\n",
    "        send_button.click(ui_admin_send, inputs=[chat_id_box, message_box], outputs=send_result)\n",
    "\n",
    "\n",
    "demo.launch(share=True, debug=True)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kH3shO4KYZqH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295243979,
     "user_tz": 300,
     "elapsed": 4267,
     "user": {
      "displayName": "Engler González",
      "userId": "10704175225987180059"
     }
    },
    "outputId": "adb0bb77-6ac4-4263-f72a-b5bcd5ad4227"
   },
   "id": "kH3shO4KYZqH",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9tg4sKB7tlO"
   },
   "source": [
    "## 7) UI Gradio — S3/equipo, Docs, Índice, Persistencia, Preguntas, Telegram (Admin)"
   ],
   "id": "u9tg4sKB7tlO"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nSb_o1qQjkvj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758295291710,
     "user_tz": 300,
     "elapsed": 70,
     "user": {
      "displayName": "Engler González",
      "userId": "10704175225987180059"
     }
    }
   },
   "outputs": [],
   "execution_count": null,
   "source": [
    "STATE = {\n",
    "    \"bucket\": os.getenv(\"S3_BUCKET\", \"\"),\n",
    "    \"base_prefix\": norm_prefix(os.getenv(\"S3_PREFIX\", \"\")),\n",
    "    \"team_folder\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "def apply_route(bucket, base_prefix, team_folder):\n",
    "    STATE[\"bucket\"] = (bucket or '').strip() or STATE[\"bucket\"]\n",
    "    STATE[\"base_prefix\"] = norm_prefix(base_prefix or STATE[\"base_prefix\"]) \\\n",
    "        or norm_prefix(os.getenv(\"S3_PREFIX\", \"\"))\n",
    "    STATE[\"team_folder\"] = (team_folder or '').strip()\n",
    "\n",
    "    eff_root = effective_team_prefix(STATE[\"base_prefix\"], STATE[\"team_folder\"])\n",
    "    bucket_name = STATE[\"bucket\"]\n",
    "    if bucket_name:\n",
    "        try:\n",
    "            s3 = s3_client_autoregion(bucket_name)\n",
    "            for sub in (\"docs/\", \"index/\"):\n",
    "                key = eff_root + sub\n",
    "                s3.put_object(Bucket=bucket_name, Key=key, Body=b'')\n",
    "        except Exception:\n",
    "            pass\n",
    "    return f\"✔ Ruta: s3://{bucket_name}/{eff_root}\"\n"
   ],
   "id": "nSb_o1qQjkvj"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643,
     "referenced_widgets": [
      "51435de40aea4497b0d6a76d7da728ab",
      "0f30d2f66a0d411c9deebfb7f5e3957e",
      "eb0366d5a8f848ed9b226074bf4d74d3",
      "58667a304de34fadbd1373afc4250fa0",
      "f38ade5956e24e018a67956bdf28bd24",
      "b0e7988a3faf4573a65cbc6b2a3f868d",
      "55d9c59951b74e16b39ba21ba4166a18",
      "eb7510ef35c04af396563d43a30a0d21",
      "65ec719963984c12907742391eb2656e",
      "e75561e77b014d0590d29d827f8f4a26",
      "ccec9660bcab4522a035622d9fe2102f"
     ]
    },
    "id": "vt5FrTzq7tlO",
    "outputId": "52416b3f-edd2-4155-9eae-65689e018131"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Celda reemplazada por la versión consolidada.\n"
   ],
   "id": "vt5FrTzq7tlO"
  },
  {
   "cell_type": "code",
   "source": [
    "# Celda reemplazada por la versión consolidada.\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1WQjoH-q_we",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758297081574,
     "user_tz": 300,
     "elapsed": 39,
     "user": {
      "displayName": "Engler González",
      "userId": "10704175225987180059"
     }
    },
    "outputId": "b8ff7e53-9b87-4a2d-d8ff-b15f8aa7fd26"
   },
   "id": "Q1WQjoH-q_we",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda reemplazada por la versión consolidada.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": []
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "51435de40aea4497b0d6a76d7da728ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f30d2f66a0d411c9deebfb7f5e3957e",
       "IPY_MODEL_eb0366d5a8f848ed9b226074bf4d74d3",
       "IPY_MODEL_58667a304de34fadbd1373afc4250fa0"
      ],
      "layout": "IPY_MODEL_f38ade5956e24e018a67956bdf28bd24"
     }
    },
    "0f30d2f66a0d411c9deebfb7f5e3957e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0e7988a3faf4573a65cbc6b2a3f868d",
      "placeholder": "​",
      "style": "IPY_MODEL_55d9c59951b74e16b39ba21ba4166a18",
      "value": "Batches: 100%"
     }
    },
    "eb0366d5a8f848ed9b226074bf4d74d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb7510ef35c04af396563d43a30a0d21",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65ec719963984c12907742391eb2656e",
      "value": 1
     }
    },
    "58667a304de34fadbd1373afc4250fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e75561e77b014d0590d29d827f8f4a26",
      "placeholder": "​",
      "style": "IPY_MODEL_ccec9660bcab4522a035622d9fe2102f",
      "value": " 1/1 [00:00&lt;00:00,  6.26it/s]"
     }
    },
    "f38ade5956e24e018a67956bdf28bd24": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0e7988a3faf4573a65cbc6b2a3f868d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55d9c59951b74e16b39ba21ba4166a18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb7510ef35c04af396563d43a30a0d21": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65ec719963984c12907742391eb2656e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e75561e77b014d0590d29d827f8f4a26": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccec9660bcab4522a035622d9fe2102f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}